{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# built-in\n",
    "import re\n",
    "import codecs\n",
    "import random\n",
    "\n",
    "# common libs\n",
    "import pandas as pd\n",
    "import numpy as np \n",
    "from tqdm import tqdm\n",
    "\n",
    "# plotting libs\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "sns.set_style(style='dark')\n",
    "\n",
    "# nltk\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "\n",
    "\n",
    "# torch\n",
    "import torch\n",
    "from torch.autograd import Variable\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "\n",
    "#sklearn\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import RandomizedSearchCV, GridSearchCV\n",
    "\n",
    "\n",
    "#bayes_opt\n",
    "from bayes_opt import BayesianOptimization\n",
    "\n",
    "\n",
    "# import EarlyStopping\n",
    "from pytorchtools import EarlyStopping"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PART I"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(\"training-v1/offenseval-training-v1.tsv\", delimiter='\\t', engine='c')\n",
    "data_aside = pd.read_csv(\"Test_B_Release/testset-taskb.tsv\", delimiter='\\t', engine='c')\n",
    "data = data[data.subtask_a == 'OFF']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3876"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.subtask_b[data.subtask_b == 'TIN'].count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "524"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.subtask_b[data.subtask_b == 'UNT'].count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data.sample(frac = 1)\n",
    "tweets = np.array(data.tweet)\n",
    "tweets_aside = np.array(data_aside.tweet)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4400\n"
     ]
    }
   ],
   "source": [
    "regexp1 = r'@[A-Za-z0-9]+'\n",
    "regexp2 = r'https?://[A-Za-z0-9./]+'\n",
    "\n",
    "combined_regexp = r'|'.join((regexp1, regexp2)) #getting rid of @USER and potentiel URLs\n",
    "def low_stemmed_token_sentence(string, regexp = combined_regexp, SW = False):\n",
    "# this tokenizer just accepts alphabetic word (remove numeric)\n",
    "        \n",
    "    cleaned = re.sub(regexp, '', string)\n",
    "    tokenizer = RegexpTokenizer('[a-z]+') #splits the string into substrings we strip the # \n",
    "    stemmer = PorterStemmer() #basically it is suffix stripping\n",
    "    \n",
    "    low = cleaned.lower().replace('url', '')\n",
    "    tokens = tokenizer.tokenize(low)\n",
    "    if SW == True:\n",
    "        stopWords = set(stopwords.words('english')) #creates a set of words that will be ignored\n",
    "        filtered_tokens = []\n",
    "        for tok in tokens:\n",
    "            if tok not in stopWords:\n",
    "                filtered_tokens.append(tok)\n",
    "        tokens = filtered_tokens\n",
    "        \n",
    "    stemmed_tokens = [stemmer.stem(i) for i in tokens]\n",
    "    return ((stemmed_tokens))\n",
    "\n",
    "#low_stemmed_token_sentence(tweets[1])\n",
    "\n",
    "cleaned_tweets = [low_stemmed_token_sentence(x, SW=True) for x in tweets]\n",
    "print(len(cleaned_tweets))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "240\n"
     ]
    }
   ],
   "source": [
    "cleaned_tweets_aside = [low_stemmed_token_sentence(x, SW=True) for x in tweets_aside]\n",
    "print(len(cleaned_tweets_aside))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing for PyTorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4640,)\n"
     ]
    }
   ],
   "source": [
    "all_cleaned_tweets = np.concatenate([cleaned_tweets, cleaned_tweets_aside])\n",
    "print(all_cleaned_tweets.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_word2idx(tokenized_corpus):\n",
    "    vocabulary = []\n",
    "    for sentence in tokenized_corpus:\n",
    "        for token in sentence:\n",
    "            if token not in vocabulary:\n",
    "                vocabulary.append(token)\n",
    "    word2idx = {w: idx+1 for (idx, w) in enumerate(vocabulary)}\n",
    "    word2idx['<pad>'] = 0     # we reserve the 0 index for the placeholder token\n",
    "    return word2idx\n",
    "\n",
    "def get_model_inputs(tokenized_corpus, word2idx, labels, max_len):\n",
    "    # we index our sentences\n",
    "    vectorized_sents = [[word2idx[tok] for tok in sent if tok in word2idx] for sent in tokenized_corpus]\n",
    "  \n",
    "    # we create a tensor of a fixed size filled with zeroes for padding\n",
    "\n",
    "    sent_tensor = Variable(torch.zeros((len(vectorized_sents), max_len))).long()\n",
    "    sent_lengths = [len(sent) for sent in vectorized_sents]\n",
    "  \n",
    "    # we fill it with our vectorized sentences \n",
    "  \n",
    "    for idx, (sent, sentlen) in enumerate(zip(vectorized_sents, sent_lengths)):\n",
    "\n",
    "        sent_tensor[idx, :sentlen] = torch.LongTensor(sent)\n",
    "        label_tensor = torch.FloatTensor(labels)\n",
    "  \n",
    "    return sent_tensor, label_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "word2idx = get_word2idx(all_cleaned_tweets)\n",
    "sent_lengths = [len(sent) for sent in all_cleaned_tweets]\n",
    "max_len = np.max(np.array(sent_lengths))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train-Valid-Test & Aside"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4400, 5)\n"
     ]
    }
   ],
   "source": [
    "regrouped = all_cleaned_tweets[:4400]\n",
    "aside = all_cleaned_tweets[4400:]\n",
    "data.tweet = regrouped\n",
    "print(data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4400, 5)\n",
      "(7020, 5)\n"
     ]
    }
   ],
   "source": [
    "labels_string = np.array(data.subtask_b)\n",
    "labels_string[labels_string == 'TIN'] = 1\n",
    "labels_string[labels_string == 'UNT'] = 0\n",
    "data.subtask_b = labels_string\n",
    "train, valid, test = data[:3000], data[3000:3700], data[3700:4400]a\n",
    "\n",
    "train_zeroes = train[train.subtask_b == 0]\n",
    "train_upsampled = pd.concat([train, train_zeroes, train_zeroes, train_zeroes, train_zeroes], axis = 0)\n",
    "\n",
    "data_zeroes =data[data.subtask_b == 0]\n",
    "data_upsampled = pd.concat([data, data_zeroes, data_zeroes, data_zeroes, data_zeroes, data_zeroes], axis = 0)\n",
    "\n",
    "# we go from 3000 to 4710\n",
    "print(train_upsampled.shape)\n",
    "# we go from 4400 to 7020\n",
    "print(data_upsampled.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train: 3000 and 3000\n",
      "train: 4400 and 4400\n",
      "valid: 700 and 700\n",
      "test: 700 and 700\n",
      "aide: 240 and None\n",
      "data_upsampled: 7020 and 7020\n"
     ]
    }
   ],
   "source": [
    "train_sent, train_labels = np.array(train.tweet), np.array(train.subtask_b)\n",
    "train_sent_upsampled, train_labels_upsampled = np.array(train_upsampled.tweet), np.array(train_upsampled.subtask_b)\n",
    "\n",
    "valid_sent, valid_labels = np.array(valid.tweet), np.array(valid.subtask_b)\n",
    "test_sent, test_labels = np.array(test.tweet), np.array(test.subtask_b)\n",
    "data_upsampled_sent, data_upsampled_labels = np.array(data_upsampled.tweet), np.array(data_upsampled.subtask_b)\n",
    "\n",
    "print(f'train: {len(train_sent)} and {len(train_labels)}')\n",
    "print(f'train: {len(train_sent_upsampled)} and {len(train_labels_upsampled)}')\n",
    "\n",
    "print(f'valid: {len(valid_sent)} and {len(valid_labels)}')\n",
    "print(f'test: {len(test_sent)} and {len(test_labels)}')\n",
    "print(f'aide: {len(aside)} and None')\n",
    "print(f'data_upsampled: {len(data_upsampled_sent)} and {len(data_upsampled_labels)}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_sent_tensor, train_label_tensor = \\\n",
    "        get_model_inputs(train_sent, word2idx, list(train_labels), max_len)\n",
    "train_sent_upsampled_tensor, train_label_upsampled_tensor = \\\n",
    "        get_model_inputs(train_sent_upsampled, word2idx, list(train_labels_upsampled), max_len)\n",
    "valid_sent_tensor, valid_label_tensor = \\\n",
    "        get_model_inputs(valid_sent, word2idx, list(valid_labels), max_len)\n",
    "test_sent_tensor, test_label_tensor = \\\n",
    "        get_model_inputs(test_sent, word2idx, list(test_labels), max_len)\n",
    "aside_sent_tensor, _ = \\\n",
    "        get_model_inputs(aside, word2idx, list(test_labels[:240]), max_len)\n",
    "data_upsampled_sent_tensor, data_upsampled_label_tensor = \\\n",
    "        get_model_inputs(data_upsampled_sent, word2idx, list(data_upsampled_labels), max_len)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PART II"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deep Lerning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 400000/400000 [00:06<00:00, 65033.04it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.        0.        0.       ...  0.        0.        0.      ]\n",
      " [ 0.10735  -0.13863   0.057066 ... -0.51945   0.88829   0.5476  ]\n",
      " [-0.14465   0.45569   0.36791  ... -0.85594   0.52813   0.27255 ]\n",
      " ...\n",
      " [ 0.        0.        0.       ...  0.        0.        0.      ]\n",
      " [ 0.24479  -0.15516  -0.71726  ...  0.35571  -0.50608   0.26447 ]\n",
      " [ 0.        0.        0.       ...  0.        0.        0.      ]]\n"
     ]
    }
   ],
   "source": [
    "wvecs = np.zeros((len(word2idx), 100))\n",
    "\n",
    "wvecs = np.zeros((len(word2idx), 100))\n",
    "\n",
    "with codecs.open('glove/glove.6B/glove.6B.100d.txt', 'r','utf-8') as f: \n",
    "  index = 0\n",
    "  for line in tqdm(f.readlines()):\n",
    "    if len(line.strip().split()) > 3:\n",
    "      word = line.strip().split()[0]\n",
    "      if word in word2idx:\n",
    "          (word, vec) = (word, list(map(float,line.strip().split()[1:])))\n",
    "          idx = word2idx[word]\n",
    "          wvecs[idx] = vec\n",
    "          \n",
    "print(wvecs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNN(nn.Module):\n",
    "    \n",
    "    def __init__(self, vocab_size, embedding_dim, out_channels, window_size, output_dim, dropout, embedding_matrix = wvecs ,non_trainable = False):\n",
    "        \n",
    "        super(CNN, self).__init__()\n",
    "        \n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
    "        self.embedding.weight.data.copy_(torch.from_numpy(embedding_matrix))\n",
    "        if non_trainable:\n",
    "            self.embedding.weight.requires_grad = False\n",
    "        \n",
    "        self.conv1 = nn.Conv2d\\\n",
    "        (in_channels=1, out_channels=out_channels, kernel_size=(window_size,embedding_dim))\n",
    "        self.conv2 = nn.Conv2d\\\n",
    "        (in_channels=1, out_channels=out_channels, kernel_size=(window_size + 1,embedding_dim))\n",
    "        #self.conv3 = nn.Conv2d\\\n",
    "        #(in_channels=1, out_channels=out_channels, kernel_size=(window_size + 2,embedding_dim))\n",
    "        self.dropout = nn.Dropout2d(dropout)\n",
    "        self.label = nn.Linear(2*out_channels, output_dim)\n",
    "\n",
    "    def conv_block(self, x, conv_layer):\n",
    "        conv_out = conv_layer(x)# conv_out.size() = (batch_size, out_channels, dim, 1)\n",
    "        activation = F.relu(conv_out.squeeze(3))# activation.size() = (batch_size, out_channels, dim1)\n",
    "        max_out = F.max_pool1d(activation, activation.size()[2]).squeeze(2)# maxpool_out.size() = (batch_size, out_channels)\n",
    "\n",
    "        return max_out\n",
    "        \n",
    "              \n",
    "        \n",
    "        \n",
    "    def forward(self, x):\n",
    "\n",
    "        #print(f\"x size : {x.shape}\")\n",
    "        embedded = self.embedding(x)\n",
    "        #print(f\"emb size : {embedded.shape}\")\n",
    "        embedded = embedded.unsqueeze(1)\n",
    "        #print(f\"before conv size : {embedded.shape}\")\n",
    "        # input.size() = (batch_size, 1, num_seq, embedding_length)\n",
    "        max_out1 = self.conv_block(embedded, self.conv1)\n",
    "        max_out2 = self.conv_block(embedded, self.conv2)\n",
    "        #max_out3 = self.conv_block(embedded, self.conv3)\n",
    "\n",
    "        #all_out = torch.cat((max_out1, max_out2, max_out3), 1)\n",
    "        all_out = torch.cat((max_out1, max_out2), 1)\n",
    "        # all_out.size() = (batch_size, num_kernels*out_channels)\n",
    "        fc_in = self.dropout(all_out)\n",
    "        # fc_in.size()) = (batch_size, num_kernels*out_channels)\n",
    "        logits = self.label(fc_in)\n",
    "\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy(output, target):\n",
    " \n",
    "    output = torch.round(torch.sigmoid(output))\n",
    "    correct = (output == target).float()\n",
    "    acc = correct.sum()/len(correct)\n",
    "    return acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Thomas\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1143: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| Epoch: 01 | Train Loss: 0.699 | Train Acc: 48.48% | Val. Loss: 0.367 | Val. Acc: 88.29% |Macro avg f1 0.46889226100151743 |\n",
      "Validation loss decreased (inf --> -0.468892).  Saving model ...\n",
      "| Epoch: 02 | Train Loss: 0.770 | Train Acc: 60.14% | Val. Loss: 0.413 | Val. Acc: 88.00% |Macro avg f1 0.5210322244306148 |\n",
      "Validation loss decreased (-0.468892 --> -0.521032).  Saving model ...\n",
      "| Epoch: 03 | Train Loss: 0.644 | Train Acc: 63.80% | Val. Loss: 0.558 | Val. Acc: 77.14% |Macro avg f1 0.586679263108172 |\n",
      "Validation loss decreased (-0.521032 --> -0.586679).  Saving model ...\n",
      "| Epoch: 04 | Train Loss: 0.606 | Train Acc: 67.86% | Val. Loss: 0.682 | Val. Acc: 61.71% |Macro avg f1 0.50943475168403 |\n",
      "EarlyStopping counter: 1 out of 20\n",
      "| Epoch: 05 | Train Loss: 0.632 | Train Acc: 66.00% | Val. Loss: 0.661 | Val. Acc: 63.86% |Macro avg f1 0.5243735077923787 |\n",
      "EarlyStopping counter: 2 out of 20\n",
      "| Epoch: 06 | Train Loss: 0.617 | Train Acc: 67.86% | Val. Loss: 0.575 | Val. Acc: 73.29% |Macro avg f1 0.5768888888888889 |\n",
      "EarlyStopping counter: 3 out of 20\n",
      "| Epoch: 07 | Train Loss: 0.587 | Train Acc: 70.73% | Val. Loss: 0.497 | Val. Acc: 81.57% |Macro avg f1 0.6123648320891517 |\n",
      "Validation loss decreased (-0.586679 --> -0.612365).  Saving model ...\n",
      "| Epoch: 08 | Train Loss: 0.568 | Train Acc: 72.34% | Val. Loss: 0.447 | Val. Acc: 85.71% |Macro avg f1 0.6129688605803255 |\n",
      "Validation loss decreased (-0.612365 --> -0.612969).  Saving model ...\n",
      "| Epoch: 09 | Train Loss: 0.568 | Train Acc: 71.32% | Val. Loss: 0.422 | Val. Acc: 86.86% |Macro avg f1 0.6152375489914922 |\n",
      "Validation loss decreased (-0.612969 --> -0.615238).  Saving model ...\n",
      "| Epoch: 10 | Train Loss: 0.566 | Train Acc: 70.82% | Val. Loss: 0.417 | Val. Acc: 86.86% |Macro avg f1 0.6203815047982835 |\n",
      "Validation loss decreased (-0.615238 --> -0.620382).  Saving model ...\n",
      "| Epoch: 11 | Train Loss: 0.557 | Train Acc: 71.55% | Val. Loss: 0.425 | Val. Acc: 85.71% |Macro avg f1 0.6129688605803255 |\n",
      "EarlyStopping counter: 1 out of 20\n",
      "| Epoch: 12 | Train Loss: 0.539 | Train Acc: 73.07% | Val. Loss: 0.446 | Val. Acc: 83.29% |Macro avg f1 0.6102950623099652 |\n",
      "EarlyStopping counter: 2 out of 20\n",
      "| Epoch: 13 | Train Loss: 0.518 | Train Acc: 76.48% | Val. Loss: 0.477 | Val. Acc: 79.57% |Macro avg f1 0.5880505862357043 |\n",
      "EarlyStopping counter: 3 out of 20\n",
      "| Epoch: 14 | Train Loss: 0.502 | Train Acc: 79.45% | Val. Loss: 0.508 | Val. Acc: 77.14% |Macro avg f1 0.5922884268157725 |\n",
      "EarlyStopping counter: 4 out of 20\n",
      "| Epoch: 15 | Train Loss: 0.488 | Train Acc: 81.11% | Val. Loss: 0.521 | Val. Acc: 75.57% |Macro avg f1 0.5849500171637211 |\n",
      "EarlyStopping counter: 5 out of 20\n",
      "| Epoch: 16 | Train Loss: 0.472 | Train Acc: 81.43% | Val. Loss: 0.505 | Val. Acc: 76.57% |Macro avg f1 0.5876436781609196 |\n",
      "EarlyStopping counter: 6 out of 20\n",
      "| Epoch: 17 | Train Loss: 0.452 | Train Acc: 82.27% | Val. Loss: 0.468 | Val. Acc: 79.14% |Macro avg f1 0.5910364145658263 |\n",
      "EarlyStopping counter: 7 out of 20\n",
      "| Epoch: 18 | Train Loss: 0.423 | Train Acc: 84.02% | Val. Loss: 0.432 | Val. Acc: 80.71% |Macro avg f1 0.594335289395624 |\n",
      "EarlyStopping counter: 8 out of 20\n",
      "| Epoch: 19 | Train Loss: 0.398 | Train Acc: 84.98% | Val. Loss: 0.408 | Val. Acc: 83.43% |Macro avg f1 0.6116690578670493 |\n",
      "EarlyStopping counter: 9 out of 20\n",
      "| Epoch: 20 | Train Loss: 0.375 | Train Acc: 85.70% | Val. Loss: 0.401 | Val. Acc: 83.86% |Macro avg f1 0.6118267697215065 |\n",
      "EarlyStopping counter: 10 out of 20\n",
      "| Epoch: 21 | Train Loss: 0.348 | Train Acc: 86.84% | Val. Loss: 0.408 | Val. Acc: 82.71% |Macro avg f1 0.6009817638773077 |\n",
      "EarlyStopping counter: 11 out of 20\n",
      "| Epoch: 22 | Train Loss: 0.316 | Train Acc: 89.59% | Val. Loss: 0.425 | Val. Acc: 81.57% |Macro avg f1 0.6054683915955592 |\n",
      "EarlyStopping counter: 12 out of 20\n",
      "| Epoch: 23 | Train Loss: 0.286 | Train Acc: 91.61% | Val. Loss: 0.443 | Val. Acc: 80.86% |Macro avg f1 0.6058757290038488 |\n",
      "EarlyStopping counter: 13 out of 20\n",
      "| Epoch: 24 | Train Loss: 0.262 | Train Acc: 92.68% | Val. Loss: 0.446 | Val. Acc: 80.71% |Macro avg f1 0.5978295563357805 |\n",
      "EarlyStopping counter: 14 out of 20\n",
      "| Epoch: 25 | Train Loss: 0.234 | Train Acc: 93.98% | Val. Loss: 0.436 | Val. Acc: 82.57% |Macro avg f1 0.6111111111111112 |\n",
      "EarlyStopping counter: 15 out of 20\n",
      "| Epoch: 26 | Train Loss: 0.206 | Train Acc: 94.84% | Val. Loss: 0.427 | Val. Acc: 84.14% |Macro avg f1 0.6146425896811503 |\n",
      "EarlyStopping counter: 16 out of 20\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-59-a1e166467f01>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     36\u001b[0m     \u001b[0moptimizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     37\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 38\u001b[1;33m     \u001b[0mpredictions\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfeature_train\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msqueeze\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     39\u001b[0m     \u001b[0mloss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mloss_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpredictions\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtarget_train\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     40\u001b[0m     \u001b[0macc\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0maccuracy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpredictions\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtarget_train\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m    487\u001b[0m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    488\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 489\u001b[1;33m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    490\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    491\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-55-68d6a639dde1>\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m     30\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     31\u001b[0m         \u001b[1;31m#print(f\"x size : {x.shape}\")\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 32\u001b[1;33m         \u001b[0membedded\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0membedding\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     33\u001b[0m         \u001b[1;31m#print(f\"emb size : {embedded.shape}\")\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     34\u001b[0m         \u001b[0membedded\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0membedded\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0munsqueeze\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m    487\u001b[0m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    488\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 489\u001b[1;33m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    490\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    491\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\torch\\nn\\modules\\sparse.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    116\u001b[0m         return F.embedding(\n\u001b[0;32m    117\u001b[0m             \u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpadding_idx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmax_norm\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 118\u001b[1;33m             self.norm_type, self.scale_grad_by_freq, self.sparse)\n\u001b[0m\u001b[0;32m    119\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    120\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mextra_repr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\torch\\nn\\functional.py\u001b[0m in \u001b[0;36membedding\u001b[1;34m(input, weight, padding_idx, max_norm, norm_type, scale_grad_by_freq, sparse)\u001b[0m\n\u001b[0;32m   1452\u001b[0m         \u001b[1;31m# remove once script supports set_grad_enabled\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1453\u001b[0m         \u001b[0m_no_grad_embedding_renorm_\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mweight\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmax_norm\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnorm_type\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1454\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0membedding\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mweight\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpadding_idx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mscale_grad_by_freq\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msparse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1455\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1456\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "epochs= 100\n",
    "\n",
    "INPUT_DIM = len(word2idx)\n",
    "EMBEDDING_DIM = 100\n",
    "OUTPUT_DIM = 1\n",
    "\n",
    "#the hyperparamerts specific to CNN\n",
    "\n",
    "# we define the number of filters\n",
    "N_OUT_CHANNELS = 100\n",
    "# we define the window size\n",
    "WINDOW_SIZE = 1\n",
    "# we apply the dropout with the probability 0.5\n",
    "DROPOUT = 0.2\n",
    "patience = 20\n",
    "\n",
    "model = CNN(INPUT_DIM, EMBEDDING_DIM, N_OUT_CHANNELS, WINDOW_SIZE, OUTPUT_DIM, DROPOUT)\n",
    "\n",
    "optimizer = optim.Adam(model.parameters(), lr= 0.0093, weight_decay=0.0003) # lr= 0.0093, weight_decay=0.0003\n",
    "loss_fn = nn.BCEWithLogitsLoss()\n",
    "\n",
    "feature_train = train_sent_upsampled_tensor\n",
    "target_train = train_label_upsampled_tensor\n",
    "\n",
    "feature_valid = valid_sent_tensor\n",
    "target_valid = valid_label_tensor\n",
    "\n",
    "early_stopping = EarlyStopping(patience=patience, verbose=True)\n",
    "\n",
    "train_acc, val_acc = [], []\n",
    "train_loss, val_loss = [], []\n",
    "for epoch in range(1, epochs+1):\n",
    "   \n",
    "    model.train()\n",
    "  \n",
    "    optimizer.zero_grad()\n",
    "    \n",
    "    predictions = model(feature_train).squeeze(1)\n",
    "    loss = loss_fn(predictions, target_train)\n",
    "    acc = accuracy(predictions, target_train)\n",
    "    train_acc.append(acc)\n",
    "    train_loss.append(loss)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "  \n",
    "    epoch_loss = loss.item()\n",
    "    epoch_acc = acc\n",
    "  \n",
    "    model.eval()\n",
    "  \n",
    "    with torch.no_grad():\n",
    " \n",
    "        predictions_valid = model(feature_valid).squeeze(1)\n",
    "        loss = loss_fn(predictions_valid, target_valid)\n",
    "        acc = accuracy(predictions_valid, target_valid)\n",
    "        val_acc.append(acc)\n",
    "        val_loss.append(loss)\n",
    "        valid_loss = loss.item()\n",
    "        valid_acc = acc\n",
    "        f1 = classification_report(target_valid, torch.round(torch.sigmoid(predictions_valid)), output_dict=True)['macro avg']['f1-score']\n",
    "    print(f'| Epoch: {epoch:02} | Train Loss: {epoch_loss:.3f} | Train Acc: {epoch_acc*100:.2f}% | Val. Loss: {valid_loss:.3f} | Val. Acc: {valid_acc*100:.2f}% |Macro avg f1 {f1} |')\n",
    "    model.eval()\n",
    "    \n",
    "    feature = test_sent_tensor\n",
    "    target = test_label_tensor\n",
    "    \n",
    "    early_stopping(-f1, model)\n",
    "        \n",
    "    if early_stopping.early_stop:\n",
    "        print(\"Early stopping\")\n",
    "        break\n",
    "\n",
    "with torch.no_grad():\n",
    " \n",
    "    predictions = model(feature).squeeze(1)\n",
    "    loss = loss_fn(predictions, target)\n",
    "    acc = accuracy(predictions, target)\n",
    "    print(f'| Test Loss: {loss:.3f} | Test Acc: {acc*100:.2f}%')\n",
    "#    f_measure(predictions, test_labels)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tests on holdout test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 414,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.94      1.00      0.97        81\n",
      "         1.0       1.00      0.99      1.00       619\n",
      "\n",
      "   micro avg       0.99      0.99      0.99       700\n",
      "   macro avg       0.97      1.00      0.98       700\n",
      "weighted avg       0.99      0.99      0.99       700\n",
      "\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    model.load_state_dict(torch.load('checkpoint.pt'))\n",
    "    model.eval()\n",
    "    predictions = model(feature).squeeze(1)\n",
    "\n",
    "print(classification_report(target, torch.round(torch.sigmoid(predictions))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tests on official test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 420,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions_off = model(aside_sent_tensor).squeeze(1)\n",
    "predictions_to_submit = torch.round(torch.sigmoid(predictions_off))\n",
    "\n",
    "p = predictions_to_submit.detach().numpy()\n",
    "p = ['TIN' if x == 1 else 'UNT' for x in p]\n",
    "df_to_submit = pd.DataFrame(p)\n",
    "\n",
    "df_to_submit.index = data_test.id\n",
    "df_to_submit.to_csv('test_3_b.csv',sep=',', header = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 421,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>id</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>15923</th>\n",
       "      <td>TIN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>60133</th>\n",
       "      <td>UNT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>83681</th>\n",
       "      <td>UNT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>65507</th>\n",
       "      <td>TIN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12588</th>\n",
       "      <td>TIN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         0\n",
       "id        \n",
       "15923  TIN\n",
       "60133  UNT\n",
       "83681  UNT\n",
       "65507  TIN\n",
       "12588  TIN"
      ]
     },
     "execution_count": 421,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_to_submit.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bayesian Optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "def f(lr, wd):\n",
    "    epochs=45\n",
    "\n",
    "    INPUT_DIM = len(word2idx)\n",
    "    EMBEDDING_DIM = 100\n",
    "    OUTPUT_DIM = 1\n",
    "\n",
    "    #the hyperparamerts specific to CNN\n",
    "\n",
    "    # we define the number of filters\n",
    "    N_OUT_CHANNELS = 100\n",
    "    # we define the window size\n",
    "    WINDOW_SIZE = 1\n",
    "    # we apply the dropout with the probability 0.5\n",
    "    dp = 0.2\n",
    "    DROPOUT = dp\n",
    "\n",
    "    model = CNN(INPUT_DIM, EMBEDDING_DIM, N_OUT_CHANNELS, WINDOW_SIZE, OUTPUT_DIM, DROPOUT)\n",
    "\n",
    "    optimizer = optim.Adam(model.parameters(), lr=lr, weight_decay=wd)\n",
    "    loss_fn = nn.BCEWithLogitsLoss()\n",
    "\n",
    "    feature_train = train_sent_tensor\n",
    "    target_train = train_label_tensor\n",
    "\n",
    "    feature_valid = valid_sent_tensor\n",
    "    target_valid = valid_label_tensor\n",
    "\n",
    "\n",
    "    for epoch in range(1, epochs+1):\n",
    "   \n",
    "        model.train()\n",
    "  \n",
    "        optimizer.zero_grad()\n",
    "    \n",
    "        predictions = model(feature_train).squeeze(1)\n",
    "        loss = loss_fn(predictions, target_train)\n",
    "        acc = accuracy(predictions, target_train)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "  \n",
    "        epoch_loss = loss.item()\n",
    "        epoch_acc = acc\n",
    "      \n",
    "        model.eval()\n",
    "  \n",
    "        with torch.no_grad():\n",
    " \n",
    "            predictions_valid = model(feature_valid).squeeze(1)\n",
    "            loss = loss_fn(predictions_valid, target_valid)\n",
    "            acc = accuracy(predictions_valid, target_valid)\n",
    "            valid_loss = loss.item()\n",
    "            valid_acc = acc\n",
    "            f1 = classification_report(target_valid, torch.round(torch.sigmoid(predictions_valid)), output_dict=True)['macro avg']['f1-score']\n",
    "        print(f'| Epoch: {epoch:02} | Train Loss: {epoch_loss:.3f} | Train Acc: {epoch_acc*100:.2f}% | Val. Loss: {valid_loss:.3f} | Val. Acc: {valid_acc*100:.2f}% |Macro avg f1 {f1}% |')\n",
    "        model.eval()\n",
    "    \n",
    "        feature = test_sent_tensor\n",
    "        target = test_label_tensor\n",
    "\n",
    "    with torch.no_grad():\n",
    " \n",
    "        predictions = model(feature).squeeze(1)\n",
    "        loss = loss_fn(predictions, target)\n",
    "        acc = accuracy(predictions, target)\n",
    "        print(f'| Test Loss: {loss:.3f} | Test Acc: {acc*100:.2f}%')\n",
    "    return(f1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialization\n",
      "-----------------------------------------------------\n",
      " Step |   Time |      Value |        lr |        wd | \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Thomas\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1143: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| Epoch: 01 | Train Loss: 0.692 | Train Acc: 51.97% | Val. Loss: 1.015 | Val. Acc: 84.86% |Macro avg f1 0.45904173106646057% |\n",
      "| Epoch: 02 | Train Loss: 0.753 | Train Acc: 88.83% | Val. Loss: 1.030 | Val. Acc: 84.86% |Macro avg f1 0.45904173106646057% |\n",
      "| Epoch: 03 | Train Loss: 0.761 | Train Acc: 88.83% | Val. Loss: 0.755 | Val. Acc: 84.86% |Macro avg f1 0.45904173106646057% |\n",
      "| Epoch: 04 | Train Loss: 0.558 | Train Acc: 88.83% | Val. Loss: 0.510 | Val. Acc: 84.86% |Macro avg f1 0.45904173106646057% |\n",
      "| Epoch: 05 | Train Loss: 0.385 | Train Acc: 88.83% | Val. Loss: 0.410 | Val. Acc: 84.86% |Macro avg f1 0.45904173106646057% |\n",
      "| Epoch: 06 | Train Loss: 0.334 | Train Acc: 88.83% | Val. Loss: 0.421 | Val. Acc: 84.86% |Macro avg f1 0.45904173106646057% |\n",
      "| Epoch: 07 | Train Loss: 0.370 | Train Acc: 88.83% | Val. Loss: 0.446 | Val. Acc: 84.86% |Macro avg f1 0.45904173106646057% |\n",
      "| Epoch: 08 | Train Loss: 0.403 | Train Acc: 88.83% | Val. Loss: 0.438 | Val. Acc: 84.86% |Macro avg f1 0.45904173106646057% |\n",
      "| Epoch: 09 | Train Loss: 0.392 | Train Acc: 88.83% | Val. Loss: 0.415 | Val. Acc: 84.86% |Macro avg f1 0.45904173106646057% |\n",
      "| Epoch: 10 | Train Loss: 0.354 | Train Acc: 88.83% | Val. Loss: 0.422 | Val. Acc: 84.86% |Macro avg f1 0.45904173106646057% |\n",
      "| Epoch: 11 | Train Loss: 0.336 | Train Acc: 88.83% | Val. Loss: 0.451 | Val. Acc: 84.86% |Macro avg f1 0.45904173106646057% |\n",
      "| Epoch: 12 | Train Loss: 0.344 | Train Acc: 88.83% | Val. Loss: 0.453 | Val. Acc: 84.86% |Macro avg f1 0.45904173106646057% |\n",
      "| Epoch: 13 | Train Loss: 0.347 | Train Acc: 88.83% | Val. Loss: 0.432 | Val. Acc: 84.86% |Macro avg f1 0.45904173106646057% |\n",
      "| Epoch: 14 | Train Loss: 0.333 | Train Acc: 88.83% | Val. Loss: 0.412 | Val. Acc: 84.86% |Macro avg f1 0.45904173106646057% |\n",
      "| Epoch: 15 | Train Loss: 0.328 | Train Acc: 88.83% | Val. Loss: 0.402 | Val. Acc: 84.86% |Macro avg f1 0.45904173106646057% |\n",
      "| Epoch: 16 | Train Loss: 0.328 | Train Acc: 88.83% | Val. Loss: 0.400 | Val. Acc: 84.86% |Macro avg f1 0.45904173106646057% |\n",
      "| Epoch: 17 | Train Loss: 0.334 | Train Acc: 88.83% | Val. Loss: 0.400 | Val. Acc: 84.86% |Macro avg f1 0.45904173106646057% |\n",
      "| Epoch: 18 | Train Loss: 0.336 | Train Acc: 88.83% | Val. Loss: 0.400 | Val. Acc: 84.86% |Macro avg f1 0.45904173106646057% |\n",
      "| Epoch: 19 | Train Loss: 0.333 | Train Acc: 88.83% | Val. Loss: 0.401 | Val. Acc: 84.86% |Macro avg f1 0.45904173106646057% |\n",
      "| Epoch: 20 | Train Loss: 0.327 | Train Acc: 88.83% | Val. Loss: 0.407 | Val. Acc: 84.86% |Macro avg f1 0.45904173106646057% |\n",
      "| Epoch: 21 | Train Loss: 0.324 | Train Acc: 88.83% | Val. Loss: 0.416 | Val. Acc: 84.86% |Macro avg f1 0.45904173106646057% |\n",
      "| Epoch: 22 | Train Loss: 0.326 | Train Acc: 88.83% | Val. Loss: 0.418 | Val. Acc: 84.86% |Macro avg f1 0.45904173106646057% |\n",
      "| Epoch: 23 | Train Loss: 0.327 | Train Acc: 88.83% | Val. Loss: 0.415 | Val. Acc: 84.86% |Macro avg f1 0.45904173106646057% |\n",
      "| Epoch: 24 | Train Loss: 0.325 | Train Acc: 88.83% | Val. Loss: 0.409 | Val. Acc: 84.86% |Macro avg f1 0.45904173106646057% |\n",
      "| Epoch: 25 | Train Loss: 0.326 | Train Acc: 88.83% | Val. Loss: 0.405 | Val. Acc: 84.86% |Macro avg f1 0.45904173106646057% |\n",
      "| Epoch: 26 | Train Loss: 0.325 | Train Acc: 88.83% | Val. Loss: 0.403 | Val. Acc: 84.86% |Macro avg f1 0.45904173106646057% |\n",
      "| Epoch: 27 | Train Loss: 0.325 | Train Acc: 88.83% | Val. Loss: 0.402 | Val. Acc: 84.86% |Macro avg f1 0.45904173106646057% |\n",
      "| Epoch: 28 | Train Loss: 0.324 | Train Acc: 88.83% | Val. Loss: 0.403 | Val. Acc: 84.86% |Macro avg f1 0.45904173106646057% |\n",
      "| Epoch: 29 | Train Loss: 0.324 | Train Acc: 88.83% | Val. Loss: 0.406 | Val. Acc: 84.86% |Macro avg f1 0.45904173106646057% |\n",
      "| Epoch: 30 | Train Loss: 0.322 | Train Acc: 88.83% | Val. Loss: 0.408 | Val. Acc: 84.86% |Macro avg f1 0.45904173106646057% |\n",
      "| Epoch: 31 | Train Loss: 0.321 | Train Acc: 88.83% | Val. Loss: 0.406 | Val. Acc: 84.86% |Macro avg f1 0.45904173106646057% |\n",
      "| Epoch: 32 | Train Loss: 0.320 | Train Acc: 88.83% | Val. Loss: 0.403 | Val. Acc: 84.86% |Macro avg f1 0.45904173106646057% |\n",
      "| Epoch: 33 | Train Loss: 0.318 | Train Acc: 88.83% | Val. Loss: 0.400 | Val. Acc: 84.86% |Macro avg f1 0.45904173106646057% |\n",
      "| Epoch: 34 | Train Loss: 0.316 | Train Acc: 88.83% | Val. Loss: 0.398 | Val. Acc: 84.86% |Macro avg f1 0.45904173106646057% |\n",
      "| Epoch: 35 | Train Loss: 0.315 | Train Acc: 88.83% | Val. Loss: 0.398 | Val. Acc: 84.86% |Macro avg f1 0.45904173106646057% |\n",
      "| Epoch: 36 | Train Loss: 0.315 | Train Acc: 88.83% | Val. Loss: 0.399 | Val. Acc: 84.86% |Macro avg f1 0.45904173106646057% |\n",
      "| Epoch: 37 | Train Loss: 0.312 | Train Acc: 88.83% | Val. Loss: 0.400 | Val. Acc: 84.86% |Macro avg f1 0.45904173106646057% |\n",
      "| Epoch: 38 | Train Loss: 0.311 | Train Acc: 88.83% | Val. Loss: 0.399 | Val. Acc: 84.86% |Macro avg f1 0.45904173106646057% |\n",
      "| Epoch: 39 | Train Loss: 0.310 | Train Acc: 88.83% | Val. Loss: 0.396 | Val. Acc: 84.86% |Macro avg f1 0.45904173106646057% |\n",
      "| Epoch: 40 | Train Loss: 0.308 | Train Acc: 88.83% | Val. Loss: 0.395 | Val. Acc: 84.86% |Macro avg f1 0.45904173106646057% |\n",
      "| Epoch: 41 | Train Loss: 0.307 | Train Acc: 88.83% | Val. Loss: 0.395 | Val. Acc: 84.86% |Macro avg f1 0.45904173106646057% |\n",
      "| Epoch: 42 | Train Loss: 0.306 | Train Acc: 88.83% | Val. Loss: 0.396 | Val. Acc: 84.86% |Macro avg f1 0.45904173106646057% |\n",
      "| Epoch: 43 | Train Loss: 0.305 | Train Acc: 88.83% | Val. Loss: 0.396 | Val. Acc: 84.86% |Macro avg f1 0.45904173106646057% |\n",
      "| Epoch: 44 | Train Loss: 0.304 | Train Acc: 88.83% | Val. Loss: 0.396 | Val. Acc: 84.86% |Macro avg f1 0.45904173106646057% |\n",
      "| Epoch: 45 | Train Loss: 0.301 | Train Acc: 88.83% | Val. Loss: 0.395 | Val. Acc: 84.86% |Macro avg f1 0.45904173106646057% |\n",
      "| Test Loss: 0.348 | Test Acc: 88.14%\n",
      "    1 | 04m05s |    0.45904 |    0.0209 |    0.0040 | \n",
      "| Epoch: 01 | Train Loss: 0.638 | Train Acc: 78.17% | Val. Loss: 2.174 | Val. Acc: 84.86% |Macro avg f1 0.45904173106646057% |\n",
      "| Epoch: 02 | Train Loss: 1.600 | Train Acc: 88.83% | Val. Loss: 1.620 | Val. Acc: 84.86% |Macro avg f1 0.45904173106646057% |\n",
      "| Epoch: 03 | Train Loss: 1.185 | Train Acc: 88.83% | Val. Loss: 0.812 | Val. Acc: 84.86% |Macro avg f1 0.45904173106646057% |\n",
      "| Epoch: 04 | Train Loss: 0.595 | Train Acc: 88.83% | Val. Loss: 0.444 | Val. Acc: 84.86% |Macro avg f1 0.45904173106646057% |\n",
      "| Epoch: 05 | Train Loss: 0.347 | Train Acc: 88.83% | Val. Loss: 0.452 | Val. Acc: 84.86% |Macro avg f1 0.45904173106646057% |\n",
      "| Epoch: 06 | Train Loss: 0.403 | Train Acc: 88.83% | Val. Loss: 0.492 | Val. Acc: 84.86% |Macro avg f1 0.45904173106646057% |\n",
      "| Epoch: 07 | Train Loss: 0.452 | Train Acc: 88.83% | Val. Loss: 0.440 | Val. Acc: 84.86% |Macro avg f1 0.45904173106646057% |\n",
      "| Epoch: 08 | Train Loss: 0.382 | Train Acc: 88.83% | Val. Loss: 0.437 | Val. Acc: 84.86% |Macro avg f1 0.45904173106646057% |\n",
      "| Epoch: 09 | Train Loss: 0.339 | Train Acc: 88.83% | Val. Loss: 0.487 | Val. Acc: 84.86% |Macro avg f1 0.45904173106646057% |\n",
      "| Epoch: 10 | Train Loss: 0.361 | Train Acc: 88.83% | Val. Loss: 0.464 | Val. Acc: 84.86% |Macro avg f1 0.45904173106646057% |\n",
      "| Epoch: 11 | Train Loss: 0.346 | Train Acc: 88.83% | Val. Loss: 0.423 | Val. Acc: 84.86% |Macro avg f1 0.45904173106646057% |\n",
      "| Epoch: 12 | Train Loss: 0.329 | Train Acc: 88.83% | Val. Loss: 0.406 | Val. Acc: 84.86% |Macro avg f1 0.45904173106646057% |\n",
      "| Epoch: 13 | Train Loss: 0.337 | Train Acc: 88.83% | Val. Loss: 0.407 | Val. Acc: 84.86% |Macro avg f1 0.45904173106646057% |\n",
      "| Epoch: 14 | Train Loss: 0.345 | Train Acc: 88.83% | Val. Loss: 0.408 | Val. Acc: 84.86% |Macro avg f1 0.45904173106646057% |\n",
      "| Epoch: 15 | Train Loss: 0.346 | Train Acc: 88.83% | Val. Loss: 0.406 | Val. Acc: 84.86% |Macro avg f1 0.45904173106646057% |\n",
      "| Epoch: 16 | Train Loss: 0.336 | Train Acc: 88.83% | Val. Loss: 0.420 | Val. Acc: 84.86% |Macro avg f1 0.45904173106646057% |\n",
      "| Epoch: 17 | Train Loss: 0.333 | Train Acc: 88.83% | Val. Loss: 0.432 | Val. Acc: 84.86% |Macro avg f1 0.45904173106646057% |\n",
      "| Epoch: 18 | Train Loss: 0.338 | Train Acc: 88.83% | Val. Loss: 0.421 | Val. Acc: 84.86% |Macro avg f1 0.45904173106646057% |\n",
      "| Epoch: 19 | Train Loss: 0.333 | Train Acc: 88.83% | Val. Loss: 0.409 | Val. Acc: 84.86% |Macro avg f1 0.45904173106646057% |\n",
      "| Epoch: 20 | Train Loss: 0.335 | Train Acc: 88.83% | Val. Loss: 0.409 | Val. Acc: 84.86% |Macro avg f1 0.45904173106646057% |\n",
      "| Epoch: 21 | Train Loss: 0.336 | Train Acc: 88.83% | Val. Loss: 0.416 | Val. Acc: 84.86% |Macro avg f1 0.45904173106646057% |\n",
      "| Epoch: 22 | Train Loss: 0.333 | Train Acc: 88.83% | Val. Loss: 0.418 | Val. Acc: 84.86% |Macro avg f1 0.45904173106646057% |\n",
      "| Epoch: 23 | Train Loss: 0.334 | Train Acc: 88.83% | Val. Loss: 0.415 | Val. Acc: 84.86% |Macro avg f1 0.45904173106646057% |\n",
      "| Epoch: 24 | Train Loss: 0.332 | Train Acc: 88.83% | Val. Loss: 0.411 | Val. Acc: 84.86% |Macro avg f1 0.45904173106646057% |\n",
      "| Epoch: 25 | Train Loss: 0.331 | Train Acc: 88.83% | Val. Loss: 0.411 | Val. Acc: 84.86% |Macro avg f1 0.45904173106646057% |\n",
      "| Epoch: 26 | Train Loss: 0.331 | Train Acc: 88.83% | Val. Loss: 0.413 | Val. Acc: 84.86% |Macro avg f1 0.45904173106646057% |\n",
      "| Epoch: 27 | Train Loss: 0.328 | Train Acc: 88.83% | Val. Loss: 0.413 | Val. Acc: 84.86% |Macro avg f1 0.45904173106646057% |\n",
      "| Epoch: 28 | Train Loss: 0.329 | Train Acc: 88.83% | Val. Loss: 0.409 | Val. Acc: 84.86% |Macro avg f1 0.45904173106646057% |\n",
      "| Epoch: 29 | Train Loss: 0.326 | Train Acc: 88.83% | Val. Loss: 0.409 | Val. Acc: 84.86% |Macro avg f1 0.45904173106646057% |\n",
      "| Epoch: 30 | Train Loss: 0.325 | Train Acc: 88.83% | Val. Loss: 0.410 | Val. Acc: 84.86% |Macro avg f1 0.45904173106646057% |\n",
      "| Epoch: 31 | Train Loss: 0.325 | Train Acc: 88.83% | Val. Loss: 0.411 | Val. Acc: 84.86% |Macro avg f1 0.45904173106646057% |\n",
      "| Epoch: 32 | Train Loss: 0.322 | Train Acc: 88.83% | Val. Loss: 0.409 | Val. Acc: 84.86% |Macro avg f1 0.45904173106646057% |\n",
      "| Epoch: 33 | Train Loss: 0.318 | Train Acc: 88.83% | Val. Loss: 0.407 | Val. Acc: 84.86% |Macro avg f1 0.45904173106646057% |\n",
      "| Epoch: 34 | Train Loss: 0.318 | Train Acc: 88.83% | Val. Loss: 0.407 | Val. Acc: 84.86% |Macro avg f1 0.45904173106646057% |\n",
      "| Epoch: 35 | Train Loss: 0.316 | Train Acc: 88.83% | Val. Loss: 0.407 | Val. Acc: 84.86% |Macro avg f1 0.45904173106646057% |\n",
      "| Epoch: 36 | Train Loss: 0.314 | Train Acc: 88.83% | Val. Loss: 0.406 | Val. Acc: 84.86% |Macro avg f1 0.45904173106646057% |\n",
      "| Epoch: 37 | Train Loss: 0.312 | Train Acc: 88.83% | Val. Loss: 0.404 | Val. Acc: 84.86% |Macro avg f1 0.45904173106646057% |\n",
      "| Epoch: 38 | Train Loss: 0.310 | Train Acc: 88.83% | Val. Loss: 0.404 | Val. Acc: 84.86% |Macro avg f1 0.45904173106646057% |\n",
      "| Epoch: 39 | Train Loss: 0.307 | Train Acc: 88.83% | Val. Loss: 0.404 | Val. Acc: 84.86% |Macro avg f1 0.45904173106646057% |\n",
      "| Epoch: 40 | Train Loss: 0.304 | Train Acc: 88.83% | Val. Loss: 0.403 | Val. Acc: 84.86% |Macro avg f1 0.45904173106646057% |\n",
      "| Epoch: 41 | Train Loss: 0.301 | Train Acc: 88.83% | Val. Loss: 0.403 | Val. Acc: 84.86% |Macro avg f1 0.45904173106646057% |\n",
      "| Epoch: 42 | Train Loss: 0.300 | Train Acc: 88.83% | Val. Loss: 0.403 | Val. Acc: 84.86% |Macro avg f1 0.45904173106646057% |\n",
      "| Epoch: 43 | Train Loss: 0.296 | Train Acc: 88.83% | Val. Loss: 0.403 | Val. Acc: 84.86% |Macro avg f1 0.45904173106646057% |\n",
      "| Epoch: 44 | Train Loss: 0.293 | Train Acc: 88.83% | Val. Loss: 0.403 | Val. Acc: 84.86% |Macro avg f1 0.45904173106646057% |\n",
      "| Epoch: 45 | Train Loss: 0.290 | Train Acc: 88.83% | Val. Loss: 0.403 | Val. Acc: 84.86% |Macro avg f1 0.45904173106646057% |\n",
      "| Test Loss: 0.349 | Test Acc: 88.14%\n",
      "    2 | 03m42s |    0.45904 |    0.0360 |    0.0054 | \n",
      "| Epoch: 01 | Train Loss: 0.749 | Train Acc: 27.37% | Val. Loss: 0.745 | Val. Acc: 21.00% |Macro avg f1 0.2086417708761364% |\n",
      "| Epoch: 02 | Train Loss: 0.748 | Train Acc: 28.03% | Val. Loss: 0.743 | Val. Acc: 21.43% |Macro avg f1 0.21336101212854292% |\n",
      "| Epoch: 03 | Train Loss: 0.743 | Train Acc: 29.20% | Val. Loss: 0.741 | Val. Acc: 22.00% |Macro avg f1 0.21948390364240847% |\n",
      "| Epoch: 04 | Train Loss: 0.743 | Train Acc: 28.60% | Val. Loss: 0.739 | Val. Acc: 22.14% |Macro avg f1 0.2210708999030167% |\n",
      "| Epoch: 05 | Train Loss: 0.743 | Train Acc: 29.77% | Val. Loss: 0.737 | Val. Acc: 23.00% |Macro avg f1 0.2299229922992299% |\n",
      "| Epoch: 06 | Train Loss: 0.740 | Train Acc: 30.00% | Val. Loss: 0.735 | Val. Acc: 23.00% |Macro avg f1 0.22999842856822156% |\n",
      "| Epoch: 07 | Train Loss: 0.739 | Train Acc: 30.40% | Val. Loss: 0.734 | Val. Acc: 23.14% |Macro avg f1 0.23140347439916403% |\n",
      "| Epoch: 08 | Train Loss: 0.734 | Train Acc: 31.33% | Val. Loss: 0.732 | Val. Acc: 24.00% |Macro avg f1 0.23960273122284295% |\n",
      "| Epoch: 09 | Train Loss: 0.732 | Train Acc: 33.67% | Val. Loss: 0.730 | Val. Acc: 24.43% |Macro avg f1 0.243604958748588% |\n",
      "| Epoch: 10 | Train Loss: 0.730 | Train Acc: 34.40% | Val. Loss: 0.728 | Val. Acc: 25.14% |Macro avg f1 0.25022893772893773% |\n",
      "| Epoch: 11 | Train Loss: 0.729 | Train Acc: 34.03% | Val. Loss: 0.726 | Val. Acc: 25.86% |Macro avg f1 0.2564941643080362% |\n",
      "| Epoch: 12 | Train Loss: 0.729 | Train Acc: 34.97% | Val. Loss: 0.724 | Val. Acc: 26.43% |Macro avg f1 0.26095397414056426% |\n",
      "| Epoch: 13 | Train Loss: 0.725 | Train Acc: 36.23% | Val. Loss: 0.722 | Val. Acc: 28.71% |Macro avg f1 0.28094282332355514% |\n",
      "| Epoch: 14 | Train Loss: 0.722 | Train Acc: 38.53% | Val. Loss: 0.720 | Val. Acc: 30.86% |Macro avg f1 0.2989513238811777% |\n",
      "| Epoch: 15 | Train Loss: 0.720 | Train Acc: 38.60% | Val. Loss: 0.719 | Val. Acc: 32.86% |Macro avg f1 0.3151483359561695% |\n",
      "| Epoch: 16 | Train Loss: 0.719 | Train Acc: 39.13% | Val. Loss: 0.717 | Val. Acc: 33.00% |Macro avg f1 0.31575513913117786% |\n",
      "| Epoch: 17 | Train Loss: 0.718 | Train Acc: 39.83% | Val. Loss: 0.715 | Val. Acc: 34.29% |Macro avg f1 0.3255923058878724% |\n",
      "| Epoch: 18 | Train Loss: 0.717 | Train Acc: 39.77% | Val. Loss: 0.713 | Val. Acc: 36.86% |Macro avg f1 0.3460156415134221% |\n",
      "| Epoch: 19 | Train Loss: 0.713 | Train Acc: 41.93% | Val. Loss: 0.711 | Val. Acc: 38.14% |Macro avg f1 0.3536968921584306% |\n",
      "| Epoch: 20 | Train Loss: 0.711 | Train Acc: 43.33% | Val. Loss: 0.709 | Val. Acc: 39.43% |Macro avg f1 0.36263056624518963% |\n",
      "| Epoch: 21 | Train Loss: 0.710 | Train Acc: 42.57% | Val. Loss: 0.708 | Val. Acc: 40.57% |Macro avg f1 0.37030213387999406% |\n",
      "| Epoch: 22 | Train Loss: 0.707 | Train Acc: 44.83% | Val. Loss: 0.706 | Val. Acc: 42.43% |Macro avg f1 0.3830360052401479% |\n",
      "| Epoch: 23 | Train Loss: 0.705 | Train Acc: 46.03% | Val. Loss: 0.704 | Val. Acc: 43.57% |Macro avg f1 0.39134320538699785% |\n",
      "| Epoch: 24 | Train Loss: 0.703 | Train Acc: 46.87% | Val. Loss: 0.702 | Val. Acc: 45.43% |Macro avg f1 0.40363616899800175% |\n",
      "| Epoch: 25 | Train Loss: 0.701 | Train Acc: 48.63% | Val. Loss: 0.700 | Val. Acc: 46.14% |Macro avg f1 0.4064741324828903% |\n",
      "| Epoch: 26 | Train Loss: 0.698 | Train Acc: 49.97% | Val. Loss: 0.699 | Val. Acc: 47.14% |Macro avg f1 0.40745824754060855% |\n",
      "| Epoch: 27 | Train Loss: 0.698 | Train Acc: 49.57% | Val. Loss: 0.697 | Val. Acc: 48.57% |Macro avg f1 0.41189649378290577% |\n",
      "| Epoch: 28 | Train Loss: 0.695 | Train Acc: 50.47% | Val. Loss: 0.695 | Val. Acc: 49.43% |Macro avg f1 0.4133522727272727% |\n",
      "| Epoch: 29 | Train Loss: 0.694 | Train Acc: 52.27% | Val. Loss: 0.693 | Val. Acc: 51.14% |Macro avg f1 0.4214931951747603% |\n",
      "| Epoch: 30 | Train Loss: 0.692 | Train Acc: 52.93% | Val. Loss: 0.692 | Val. Acc: 52.14% |Macro avg f1 0.4230944280024306% |\n",
      "| Epoch: 31 | Train Loss: 0.690 | Train Acc: 53.70% | Val. Loss: 0.690 | Val. Acc: 54.00% |Macro avg f1 0.431330796944223% |\n",
      "| Epoch: 32 | Train Loss: 0.688 | Train Acc: 55.10% | Val. Loss: 0.688 | Val. Acc: 55.57% |Macro avg f1 0.4374080974987014% |\n",
      "| Epoch: 33 | Train Loss: 0.686 | Train Acc: 56.03% | Val. Loss: 0.687 | Val. Acc: 56.00% |Macro avg f1 0.4342692206769877% |\n",
      "| Epoch: 34 | Train Loss: 0.685 | Train Acc: 57.63% | Val. Loss: 0.685 | Val. Acc: 58.29% |Macro avg f1 0.4459142956280361% |\n",
      "| Epoch: 35 | Train Loss: 0.684 | Train Acc: 56.73% | Val. Loss: 0.683 | Val. Acc: 60.00% |Macro avg f1 0.4472271106899508% |\n",
      "| Epoch: 36 | Train Loss: 0.681 | Train Acc: 59.23% | Val. Loss: 0.682 | Val. Acc: 62.00% |Macro avg f1 0.4514753016591252% |\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| Epoch: 37 | Train Loss: 0.679 | Train Acc: 59.40% | Val. Loss: 0.680 | Val. Acc: 63.43% |Macro avg f1 0.459583348411923% |\n",
      "| Epoch: 38 | Train Loss: 0.677 | Train Acc: 61.33% | Val. Loss: 0.678 | Val. Acc: 64.29% |Macro avg f1 0.45910192374264375% |\n",
      "| Epoch: 39 | Train Loss: 0.677 | Train Acc: 61.30% | Val. Loss: 0.677 | Val. Acc: 65.57% |Macro avg f1 0.4634889438015004% |\n",
      "| Epoch: 40 | Train Loss: 0.671 | Train Acc: 63.57% | Val. Loss: 0.675 | Val. Acc: 66.43% |Macro avg f1 0.46533797465474475% |\n",
      "| Epoch: 41 | Train Loss: 0.671 | Train Acc: 63.57% | Val. Loss: 0.673 | Val. Acc: 67.57% |Macro avg f1 0.468616966133946% |\n",
      "| Epoch: 42 | Train Loss: 0.671 | Train Acc: 63.87% | Val. Loss: 0.672 | Val. Acc: 68.71% |Macro avg f1 0.47172724171321645% |\n",
      "| Epoch: 43 | Train Loss: 0.669 | Train Acc: 66.47% | Val. Loss: 0.670 | Val. Acc: 70.29% |Macro avg f1 0.47701149425287354% |\n",
      "| Epoch: 44 | Train Loss: 0.666 | Train Acc: 67.57% | Val. Loss: 0.668 | Val. Acc: 71.86% |Macro avg f1 0.474608623428874% |\n",
      "| Epoch: 45 | Train Loss: 0.666 | Train Acc: 67.13% | Val. Loss: 0.667 | Val. Acc: 72.71% |Macro avg f1 0.4791808687590325% |\n",
      "| Test Loss: 0.662 | Test Acc: 76.86%\n",
      "    3 | 03m36s |    0.47918 |    0.0000 |    0.0042 | \n",
      "| Epoch: 01 | Train Loss: 0.711 | Train Acc: 43.93% | Val. Loss: 0.733 | Val. Acc: 84.86% |Macro avg f1 0.45904173106646057% |\n",
      "| Epoch: 02 | Train Loss: 0.548 | Train Acc: 88.77% | Val. Loss: 0.853 | Val. Acc: 84.86% |Macro avg f1 0.45904173106646057% |\n",
      "| Epoch: 03 | Train Loss: 0.632 | Train Acc: 88.83% | Val. Loss: 0.722 | Val. Acc: 84.86% |Macro avg f1 0.45904173106646057% |\n",
      "| Epoch: 04 | Train Loss: 0.536 | Train Acc: 88.83% | Val. Loss: 0.545 | Val. Acc: 84.86% |Macro avg f1 0.45904173106646057% |\n",
      "| Epoch: 05 | Train Loss: 0.414 | Train Acc: 88.83% | Val. Loss: 0.431 | Val. Acc: 84.86% |Macro avg f1 0.45904173106646057% |\n",
      "| Epoch: 06 | Train Loss: 0.342 | Train Acc: 88.83% | Val. Loss: 0.402 | Val. Acc: 84.86% |Macro avg f1 0.45904173106646057% |\n",
      "| Epoch: 07 | Train Loss: 0.341 | Train Acc: 88.83% | Val. Loss: 0.412 | Val. Acc: 84.86% |Macro avg f1 0.45904173106646057% |\n",
      "| Epoch: 08 | Train Loss: 0.364 | Train Acc: 88.83% | Val. Loss: 0.415 | Val. Acc: 84.86% |Macro avg f1 0.45904173106646057% |\n",
      "| Epoch: 09 | Train Loss: 0.367 | Train Acc: 88.83% | Val. Loss: 0.406 | Val. Acc: 84.86% |Macro avg f1 0.45904173106646057% |\n",
      "| Epoch: 10 | Train Loss: 0.347 | Train Acc: 88.83% | Val. Loss: 0.410 | Val. Acc: 84.86% |Macro avg f1 0.45904173106646057% |\n",
      "| Epoch: 11 | Train Loss: 0.333 | Train Acc: 88.83% | Val. Loss: 0.431 | Val. Acc: 84.86% |Macro avg f1 0.45904173106646057% |\n",
      "| Epoch: 12 | Train Loss: 0.338 | Train Acc: 88.83% | Val. Loss: 0.436 | Val. Acc: 84.86% |Macro avg f1 0.45904173106646057% |\n",
      "| Epoch: 13 | Train Loss: 0.341 | Train Acc: 88.83% | Val. Loss: 0.420 | Val. Acc: 84.86% |Macro avg f1 0.45904173106646057% |\n",
      "| Epoch: 14 | Train Loss: 0.334 | Train Acc: 88.83% | Val. Loss: 0.405 | Val. Acc: 84.86% |Macro avg f1 0.45904173106646057% |\n",
      "| Epoch: 15 | Train Loss: 0.330 | Train Acc: 88.83% | Val. Loss: 0.402 | Val. Acc: 84.86% |Macro avg f1 0.45904173106646057% |\n",
      "| Epoch: 16 | Train Loss: 0.338 | Train Acc: 88.83% | Val. Loss: 0.403 | Val. Acc: 84.86% |Macro avg f1 0.45904173106646057% |\n",
      "| Epoch: 17 | Train Loss: 0.341 | Train Acc: 88.83% | Val. Loss: 0.401 | Val. Acc: 84.86% |Macro avg f1 0.45904173106646057% |\n",
      "| Epoch: 18 | Train Loss: 0.335 | Train Acc: 88.83% | Val. Loss: 0.406 | Val. Acc: 84.86% |Macro avg f1 0.45904173106646057% |\n",
      "| Epoch: 19 | Train Loss: 0.329 | Train Acc: 88.83% | Val. Loss: 0.417 | Val. Acc: 84.86% |Macro avg f1 0.45904173106646057% |\n",
      "| Epoch: 20 | Train Loss: 0.330 | Train Acc: 88.83% | Val. Loss: 0.421 | Val. Acc: 84.86% |Macro avg f1 0.45904173106646057% |\n",
      "| Epoch: 21 | Train Loss: 0.331 | Train Acc: 88.83% | Val. Loss: 0.413 | Val. Acc: 84.86% |Macro avg f1 0.45904173106646057% |\n",
      "| Epoch: 22 | Train Loss: 0.328 | Train Acc: 88.83% | Val. Loss: 0.405 | Val. Acc: 84.86% |Macro avg f1 0.45904173106646057% |\n",
      "| Epoch: 23 | Train Loss: 0.329 | Train Acc: 88.83% | Val. Loss: 0.402 | Val. Acc: 84.86% |Macro avg f1 0.45904173106646057% |\n",
      "| Epoch: 24 | Train Loss: 0.332 | Train Acc: 88.83% | Val. Loss: 0.402 | Val. Acc: 84.86% |Macro avg f1 0.45904173106646057% |\n",
      "| Epoch: 25 | Train Loss: 0.333 | Train Acc: 88.83% | Val. Loss: 0.404 | Val. Acc: 84.86% |Macro avg f1 0.45904173106646057% |\n",
      "| Epoch: 26 | Train Loss: 0.329 | Train Acc: 88.83% | Val. Loss: 0.409 | Val. Acc: 84.86% |Macro avg f1 0.45904173106646057% |\n",
      "| Epoch: 27 | Train Loss: 0.329 | Train Acc: 88.83% | Val. Loss: 0.414 | Val. Acc: 84.86% |Macro avg f1 0.45904173106646057% |\n",
      "| Epoch: 28 | Train Loss: 0.328 | Train Acc: 88.83% | Val. Loss: 0.413 | Val. Acc: 84.86% |Macro avg f1 0.45904173106646057% |\n",
      "| Epoch: 29 | Train Loss: 0.328 | Train Acc: 88.83% | Val. Loss: 0.407 | Val. Acc: 84.86% |Macro avg f1 0.45904173106646057% |\n",
      "| Epoch: 30 | Train Loss: 0.327 | Train Acc: 88.83% | Val. Loss: 0.403 | Val. Acc: 84.86% |Macro avg f1 0.45904173106646057% |\n",
      "| Epoch: 31 | Train Loss: 0.328 | Train Acc: 88.83% | Val. Loss: 0.402 | Val. Acc: 84.86% |Macro avg f1 0.45904173106646057% |\n",
      "| Epoch: 32 | Train Loss: 0.329 | Train Acc: 88.83% | Val. Loss: 0.403 | Val. Acc: 84.86% |Macro avg f1 0.45904173106646057% |\n",
      "| Epoch: 33 | Train Loss: 0.327 | Train Acc: 88.83% | Val. Loss: 0.406 | Val. Acc: 84.86% |Macro avg f1 0.45904173106646057% |\n",
      "| Epoch: 34 | Train Loss: 0.325 | Train Acc: 88.83% | Val. Loss: 0.409 | Val. Acc: 84.86% |Macro avg f1 0.45904173106646057% |\n",
      "| Epoch: 35 | Train Loss: 0.326 | Train Acc: 88.83% | Val. Loss: 0.409 | Val. Acc: 84.86% |Macro avg f1 0.45904173106646057% |\n",
      "| Epoch: 36 | Train Loss: 0.325 | Train Acc: 88.83% | Val. Loss: 0.406 | Val. Acc: 84.86% |Macro avg f1 0.45904173106646057% |\n",
      "| Epoch: 37 | Train Loss: 0.324 | Train Acc: 88.83% | Val. Loss: 0.403 | Val. Acc: 84.86% |Macro avg f1 0.45904173106646057% |\n",
      "| Epoch: 38 | Train Loss: 0.326 | Train Acc: 88.83% | Val. Loss: 0.403 | Val. Acc: 84.86% |Macro avg f1 0.45904173106646057% |\n",
      "| Epoch: 39 | Train Loss: 0.325 | Train Acc: 88.83% | Val. Loss: 0.404 | Val. Acc: 84.86% |Macro avg f1 0.45904173106646057% |\n",
      "| Epoch: 40 | Train Loss: 0.324 | Train Acc: 88.83% | Val. Loss: 0.406 | Val. Acc: 84.86% |Macro avg f1 0.45904173106646057% |\n",
      "| Epoch: 41 | Train Loss: 0.324 | Train Acc: 88.83% | Val. Loss: 0.407 | Val. Acc: 84.86% |Macro avg f1 0.45904173106646057% |\n",
      "| Epoch: 42 | Train Loss: 0.323 | Train Acc: 88.83% | Val. Loss: 0.406 | Val. Acc: 84.86% |Macro avg f1 0.45904173106646057% |\n",
      "| Epoch: 43 | Train Loss: 0.323 | Train Acc: 88.83% | Val. Loss: 0.404 | Val. Acc: 84.86% |Macro avg f1 0.45904173106646057% |\n",
      "| Epoch: 44 | Train Loss: 0.324 | Train Acc: 88.83% | Val. Loss: 0.403 | Val. Acc: 84.86% |Macro avg f1 0.45904173106646057% |\n",
      "| Epoch: 45 | Train Loss: 0.323 | Train Acc: 88.83% | Val. Loss: 0.403 | Val. Acc: 84.86% |Macro avg f1 0.45904173106646057% |\n",
      "| Test Loss: 0.352 | Test Acc: 88.14%\n",
      "    4 | 03m49s |    0.45904 |    0.0151 |    0.0069 | \n",
      "| Epoch: 01 | Train Loss: 0.758 | Train Acc: 27.37% | Val. Loss: 0.433 | Val. Acc: 84.86% |Macro avg f1 0.45904173106646057% |\n",
      "| Epoch: 02 | Train Loss: 0.357 | Train Acc: 88.77% | Val. Loss: 0.553 | Val. Acc: 84.86% |Macro avg f1 0.45904173106646057% |\n",
      "| Epoch: 03 | Train Loss: 0.423 | Train Acc: 88.83% | Val. Loss: 0.607 | Val. Acc: 84.86% |Macro avg f1 0.45904173106646057% |\n",
      "| Epoch: 04 | Train Loss: 0.459 | Train Acc: 88.83% | Val. Loss: 0.588 | Val. Acc: 84.86% |Macro avg f1 0.45904173106646057% |\n",
      "| Epoch: 05 | Train Loss: 0.443 | Train Acc: 88.83% | Val. Loss: 0.532 | Val. Acc: 84.86% |Macro avg f1 0.45904173106646057% |\n",
      "| Epoch: 06 | Train Loss: 0.405 | Train Acc: 88.83% | Val. Loss: 0.472 | Val. Acc: 84.86% |Macro avg f1 0.45904173106646057% |\n",
      "| Epoch: 07 | Train Loss: 0.365 | Train Acc: 88.83% | Val. Loss: 0.426 | Val. Acc: 84.86% |Macro avg f1 0.45904173106646057% |\n",
      "| Epoch: 08 | Train Loss: 0.339 | Train Acc: 88.83% | Val. Loss: 0.402 | Val. Acc: 84.86% |Macro avg f1 0.45904173106646057% |\n",
      "| Epoch: 09 | Train Loss: 0.331 | Train Acc: 88.83% | Val. Loss: 0.396 | Val. Acc: 84.86% |Macro avg f1 0.45904173106646057% |\n",
      "| Epoch: 10 | Train Loss: 0.334 | Train Acc: 88.83% | Val. Loss: 0.397 | Val. Acc: 84.86% |Macro avg f1 0.45904173106646057% |\n",
      "| Epoch: 11 | Train Loss: 0.342 | Train Acc: 88.83% | Val. Loss: 0.400 | Val. Acc: 84.86% |Macro avg f1 0.45904173106646057% |\n",
      "| Epoch: 12 | Train Loss: 0.347 | Train Acc: 88.83% | Val. Loss: 0.400 | Val. Acc: 84.86% |Macro avg f1 0.45904173106646057% |\n",
      "| Epoch: 13 | Train Loss: 0.348 | Train Acc: 88.83% | Val. Loss: 0.398 | Val. Acc: 84.86% |Macro avg f1 0.45904173106646057% |\n",
      "| Epoch: 14 | Train Loss: 0.343 | Train Acc: 88.83% | Val. Loss: 0.396 | Val. Acc: 84.86% |Macro avg f1 0.45904173106646057% |\n",
      "| Epoch: 15 | Train Loss: 0.334 | Train Acc: 88.83% | Val. Loss: 0.395 | Val. Acc: 84.86% |Macro avg f1 0.45904173106646057% |\n",
      "| Epoch: 16 | Train Loss: 0.326 | Train Acc: 88.83% | Val. Loss: 0.397 | Val. Acc: 84.86% |Macro avg f1 0.45904173106646057% |\n",
      "| Epoch: 17 | Train Loss: 0.322 | Train Acc: 88.83% | Val. Loss: 0.403 | Val. Acc: 84.86% |Macro avg f1 0.45904173106646057% |\n",
      "| Epoch: 18 | Train Loss: 0.320 | Train Acc: 88.83% | Val. Loss: 0.410 | Val. Acc: 84.86% |Macro avg f1 0.45904173106646057% |\n",
      "| Epoch: 19 | Train Loss: 0.322 | Train Acc: 88.83% | Val. Loss: 0.416 | Val. Acc: 84.86% |Macro avg f1 0.45904173106646057% |\n",
      "| Epoch: 20 | Train Loss: 0.322 | Train Acc: 88.83% | Val. Loss: 0.417 | Val. Acc: 84.86% |Macro avg f1 0.45904173106646057% |\n",
      "| Epoch: 21 | Train Loss: 0.321 | Train Acc: 88.83% | Val. Loss: 0.415 | Val. Acc: 84.86% |Macro avg f1 0.45904173106646057% |\n",
      "| Epoch: 22 | Train Loss: 0.320 | Train Acc: 88.83% | Val. Loss: 0.410 | Val. Acc: 84.86% |Macro avg f1 0.45904173106646057% |\n",
      "| Epoch: 23 | Train Loss: 0.318 | Train Acc: 88.83% | Val. Loss: 0.405 | Val. Acc: 84.86% |Macro avg f1 0.45904173106646057% |\n",
      "| Epoch: 24 | Train Loss: 0.317 | Train Acc: 88.83% | Val. Loss: 0.400 | Val. Acc: 84.86% |Macro avg f1 0.45904173106646057% |\n",
      "| Epoch: 25 | Train Loss: 0.316 | Train Acc: 88.83% | Val. Loss: 0.397 | Val. Acc: 84.86% |Macro avg f1 0.45904173106646057% |\n",
      "| Epoch: 26 | Train Loss: 0.315 | Train Acc: 88.83% | Val. Loss: 0.395 | Val. Acc: 84.86% |Macro avg f1 0.45904173106646057% |\n",
      "| Epoch: 27 | Train Loss: 0.316 | Train Acc: 88.83% | Val. Loss: 0.393 | Val. Acc: 84.86% |Macro avg f1 0.45904173106646057% |\n",
      "| Epoch: 28 | Train Loss: 0.316 | Train Acc: 88.83% | Val. Loss: 0.393 | Val. Acc: 84.86% |Macro avg f1 0.45904173106646057% |\n",
      "| Epoch: 29 | Train Loss: 0.316 | Train Acc: 88.83% | Val. Loss: 0.393 | Val. Acc: 84.86% |Macro avg f1 0.45904173106646057% |\n",
      "| Epoch: 30 | Train Loss: 0.316 | Train Acc: 88.83% | Val. Loss: 0.393 | Val. Acc: 84.86% |Macro avg f1 0.45904173106646057% |\n",
      "| Epoch: 31 | Train Loss: 0.314 | Train Acc: 88.83% | Val. Loss: 0.394 | Val. Acc: 84.86% |Macro avg f1 0.45904173106646057% |\n",
      "| Epoch: 32 | Train Loss: 0.313 | Train Acc: 88.83% | Val. Loss: 0.395 | Val. Acc: 84.86% |Macro avg f1 0.45904173106646057% |\n",
      "| Epoch: 33 | Train Loss: 0.312 | Train Acc: 88.83% | Val. Loss: 0.397 | Val. Acc: 84.86% |Macro avg f1 0.45904173106646057% |\n",
      "| Epoch: 34 | Train Loss: 0.312 | Train Acc: 88.83% | Val. Loss: 0.398 | Val. Acc: 84.86% |Macro avg f1 0.45904173106646057% |\n",
      "| Epoch: 35 | Train Loss: 0.311 | Train Acc: 88.83% | Val. Loss: 0.399 | Val. Acc: 84.86% |Macro avg f1 0.45904173106646057% |\n",
      "| Epoch: 36 | Train Loss: 0.310 | Train Acc: 88.83% | Val. Loss: 0.399 | Val. Acc: 84.86% |Macro avg f1 0.45904173106646057% |\n",
      "| Epoch: 37 | Train Loss: 0.310 | Train Acc: 88.83% | Val. Loss: 0.399 | Val. Acc: 84.86% |Macro avg f1 0.45904173106646057% |\n",
      "| Epoch: 38 | Train Loss: 0.308 | Train Acc: 88.83% | Val. Loss: 0.398 | Val. Acc: 84.86% |Macro avg f1 0.45904173106646057% |\n",
      "| Epoch: 39 | Train Loss: 0.306 | Train Acc: 88.83% | Val. Loss: 0.396 | Val. Acc: 84.86% |Macro avg f1 0.45904173106646057% |\n",
      "| Epoch: 40 | Train Loss: 0.307 | Train Acc: 88.83% | Val. Loss: 0.395 | Val. Acc: 84.86% |Macro avg f1 0.45904173106646057% |\n",
      "| Epoch: 41 | Train Loss: 0.307 | Train Acc: 88.83% | Val. Loss: 0.394 | Val. Acc: 84.86% |Macro avg f1 0.45904173106646057% |\n",
      "| Epoch: 42 | Train Loss: 0.306 | Train Acc: 88.83% | Val. Loss: 0.393 | Val. Acc: 84.86% |Macro avg f1 0.45904173106646057% |\n",
      "| Epoch: 43 | Train Loss: 0.305 | Train Acc: 88.83% | Val. Loss: 0.392 | Val. Acc: 84.86% |Macro avg f1 0.45904173106646057% |\n",
      "| Epoch: 44 | Train Loss: 0.304 | Train Acc: 88.83% | Val. Loss: 0.392 | Val. Acc: 84.86% |Macro avg f1 0.45904173106646057% |\n",
      "| Epoch: 45 | Train Loss: 0.303 | Train Acc: 88.83% | Val. Loss: 0.392 | Val. Acc: 84.86% |Macro avg f1 0.45904173106646057% |\n",
      "| Test Loss: 0.352 | Test Acc: 88.14%\n",
      "    5 | 03m24s |    0.45904 |    0.0073 |    0.0020 | \n",
      "| Epoch: 01 | Train Loss: 0.784 | Train Acc: 18.33% | Val. Loss: 0.436 | Val. Acc: 84.86% |Macro avg f1 0.45904173106646057% |\n",
      "| Epoch: 02 | Train Loss: 0.388 | Train Acc: 88.60% | Val. Loss: 0.457 | Val. Acc: 84.86% |Macro avg f1 0.45904173106646057% |\n",
      "| Epoch: 03 | Train Loss: 0.362 | Train Acc: 88.73% | Val. Loss: 0.524 | Val. Acc: 84.86% |Macro avg f1 0.45904173106646057% |\n",
      "| Epoch: 04 | Train Loss: 0.399 | Train Acc: 88.80% | Val. Loss: 0.560 | Val. Acc: 84.86% |Macro avg f1 0.45904173106646057% |\n",
      "| Epoch: 05 | Train Loss: 0.422 | Train Acc: 88.83% | Val. Loss: 0.560 | Val. Acc: 84.86% |Macro avg f1 0.45904173106646057% |\n",
      "| Epoch: 06 | Train Loss: 0.422 | Train Acc: 88.83% | Val. Loss: 0.536 | Val. Acc: 84.86% |Macro avg f1 0.45904173106646057% |\n",
      "| Epoch: 07 | Train Loss: 0.407 | Train Acc: 88.83% | Val. Loss: 0.500 | Val. Acc: 84.86% |Macro avg f1 0.45904173106646057% |\n",
      "| Epoch: 08 | Train Loss: 0.382 | Train Acc: 88.83% | Val. Loss: 0.463 | Val. Acc: 84.86% |Macro avg f1 0.45904173106646057% |\n",
      "| Epoch: 09 | Train Loss: 0.359 | Train Acc: 88.83% | Val. Loss: 0.432 | Val. Acc: 84.86% |Macro avg f1 0.45904173106646057% |\n",
      "| Epoch: 10 | Train Loss: 0.343 | Train Acc: 88.83% | Val. Loss: 0.412 | Val. Acc: 84.86% |Macro avg f1 0.45904173106646057% |\n",
      "| Epoch: 11 | Train Loss: 0.337 | Train Acc: 88.83% | Val. Loss: 0.403 | Val. Acc: 84.86% |Macro avg f1 0.45904173106646057% |\n",
      "| Epoch: 12 | Train Loss: 0.338 | Train Acc: 88.80% | Val. Loss: 0.402 | Val. Acc: 84.86% |Macro avg f1 0.45904173106646057% |\n",
      "| Epoch: 13 | Train Loss: 0.345 | Train Acc: 88.83% | Val. Loss: 0.405 | Val. Acc: 84.86% |Macro avg f1 0.45904173106646057% |\n",
      "| Epoch: 14 | Train Loss: 0.352 | Train Acc: 88.83% | Val. Loss: 0.407 | Val. Acc: 84.86% |Macro avg f1 0.45904173106646057% |\n",
      "| Epoch: 15 | Train Loss: 0.358 | Train Acc: 88.83% | Val. Loss: 0.407 | Val. Acc: 84.86% |Macro avg f1 0.45904173106646057% |\n",
      "| Epoch: 16 | Train Loss: 0.357 | Train Acc: 88.83% | Val. Loss: 0.405 | Val. Acc: 84.86% |Macro avg f1 0.45904173106646057% |\n",
      "| Epoch: 17 | Train Loss: 0.353 | Train Acc: 88.83% | Val. Loss: 0.402 | Val. Acc: 84.86% |Macro avg f1 0.45904173106646057% |\n",
      "| Epoch: 18 | Train Loss: 0.345 | Train Acc: 88.83% | Val. Loss: 0.400 | Val. Acc: 84.86% |Macro avg f1 0.45904173106646057% |\n",
      "| Epoch: 19 | Train Loss: 0.339 | Train Acc: 88.83% | Val. Loss: 0.400 | Val. Acc: 84.86% |Macro avg f1 0.45904173106646057% |\n",
      "| Epoch: 20 | Train Loss: 0.334 | Train Acc: 88.83% | Val. Loss: 0.403 | Val. Acc: 84.86% |Macro avg f1 0.45904173106646057% |\n",
      "| Epoch: 21 | Train Loss: 0.330 | Train Acc: 88.83% | Val. Loss: 0.409 | Val. Acc: 84.86% |Macro avg f1 0.45904173106646057% |\n",
      "| Epoch: 22 | Train Loss: 0.329 | Train Acc: 88.83% | Val. Loss: 0.414 | Val. Acc: 84.86% |Macro avg f1 0.45904173106646057% |\n",
      "| Epoch: 23 | Train Loss: 0.331 | Train Acc: 88.83% | Val. Loss: 0.419 | Val. Acc: 84.86% |Macro avg f1 0.45904173106646057% |\n",
      "| Epoch: 24 | Train Loss: 0.333 | Train Acc: 88.83% | Val. Loss: 0.420 | Val. Acc: 84.86% |Macro avg f1 0.45904173106646057% |\n",
      "| Epoch: 25 | Train Loss: 0.332 | Train Acc: 88.83% | Val. Loss: 0.419 | Val. Acc: 84.86% |Macro avg f1 0.45904173106646057% |\n",
      "| Epoch: 26 | Train Loss: 0.331 | Train Acc: 88.83% | Val. Loss: 0.416 | Val. Acc: 84.86% |Macro avg f1 0.45904173106646057% |\n",
      "| Epoch: 27 | Train Loss: 0.329 | Train Acc: 88.83% | Val. Loss: 0.411 | Val. Acc: 84.86% |Macro avg f1 0.45904173106646057% |\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| Epoch: 28 | Train Loss: 0.327 | Train Acc: 88.83% | Val. Loss: 0.407 | Val. Acc: 84.86% |Macro avg f1 0.45904173106646057% |\n",
      "| Epoch: 29 | Train Loss: 0.327 | Train Acc: 88.83% | Val. Loss: 0.403 | Val. Acc: 84.86% |Macro avg f1 0.45904173106646057% |\n",
      "| Epoch: 30 | Train Loss: 0.326 | Train Acc: 88.83% | Val. Loss: 0.401 | Val. Acc: 84.86% |Macro avg f1 0.45904173106646057% |\n",
      "| Epoch: 31 | Train Loss: 0.327 | Train Acc: 88.83% | Val. Loss: 0.399 | Val. Acc: 84.86% |Macro avg f1 0.45904173106646057% |\n",
      "| Epoch: 32 | Train Loss: 0.328 | Train Acc: 88.83% | Val. Loss: 0.399 | Val. Acc: 84.86% |Macro avg f1 0.45904173106646057% |\n",
      "| Epoch: 33 | Train Loss: 0.329 | Train Acc: 88.83% | Val. Loss: 0.398 | Val. Acc: 84.86% |Macro avg f1 0.45904173106646057% |\n",
      "| Epoch: 34 | Train Loss: 0.329 | Train Acc: 88.83% | Val. Loss: 0.399 | Val. Acc: 84.86% |Macro avg f1 0.45904173106646057% |\n",
      "| Epoch: 35 | Train Loss: 0.329 | Train Acc: 88.83% | Val. Loss: 0.399 | Val. Acc: 84.86% |Macro avg f1 0.45904173106646057% |\n",
      "| Epoch: 36 | Train Loss: 0.329 | Train Acc: 88.83% | Val. Loss: 0.400 | Val. Acc: 84.86% |Macro avg f1 0.45904173106646057% |\n",
      "| Epoch: 37 | Train Loss: 0.327 | Train Acc: 88.83% | Val. Loss: 0.401 | Val. Acc: 84.86% |Macro avg f1 0.45904173106646057% |\n",
      "| Epoch: 38 | Train Loss: 0.325 | Train Acc: 88.83% | Val. Loss: 0.403 | Val. Acc: 84.86% |Macro avg f1 0.45904173106646057% |\n",
      "| Epoch: 39 | Train Loss: 0.325 | Train Acc: 88.83% | Val. Loss: 0.405 | Val. Acc: 84.86% |Macro avg f1 0.45904173106646057% |\n",
      "| Epoch: 40 | Train Loss: 0.324 | Train Acc: 88.83% | Val. Loss: 0.406 | Val. Acc: 84.86% |Macro avg f1 0.45904173106646057% |\n",
      "| Epoch: 41 | Train Loss: 0.324 | Train Acc: 88.83% | Val. Loss: 0.406 | Val. Acc: 84.86% |Macro avg f1 0.45904173106646057% |\n",
      "| Epoch: 42 | Train Loss: 0.323 | Train Acc: 88.83% | Val. Loss: 0.406 | Val. Acc: 84.86% |Macro avg f1 0.45904173106646057% |\n",
      "| Epoch: 43 | Train Loss: 0.322 | Train Acc: 88.83% | Val. Loss: 0.404 | Val. Acc: 84.86% |Macro avg f1 0.45904173106646057% |\n",
      "| Epoch: 44 | Train Loss: 0.323 | Train Acc: 88.83% | Val. Loss: 0.403 | Val. Acc: 84.86% |Macro avg f1 0.45904173106646057% |\n",
      "| Epoch: 45 | Train Loss: 0.323 | Train Acc: 88.83% | Val. Loss: 0.401 | Val. Acc: 84.86% |Macro avg f1 0.45904173106646057% |\n",
      "| Test Loss: 0.355 | Test Acc: 88.14%\n",
      "    6 | 03m26s |    0.45904 |    0.0046 |    0.0088 | \n",
      "| Epoch: 01 | Train Loss: 0.690 | Train Acc: 53.50% | Val. Loss: 0.507 | Val. Acc: 84.86% |Macro avg f1 0.45904173106646057% |\n",
      "| Epoch: 02 | Train Loss: 0.397 | Train Acc: 88.77% | Val. Loss: 0.650 | Val. Acc: 84.86% |Macro avg f1 0.45904173106646057% |\n",
      "| Epoch: 03 | Train Loss: 0.487 | Train Acc: 88.83% | Val. Loss: 0.654 | Val. Acc: 84.86% |Macro avg f1 0.45904173106646057% |\n",
      "| Epoch: 04 | Train Loss: 0.491 | Train Acc: 88.80% | Val. Loss: 0.583 | Val. Acc: 84.86% |Macro avg f1 0.45904173106646057% |\n",
      "| Epoch: 05 | Train Loss: 0.438 | Train Acc: 88.80% | Val. Loss: 0.499 | Val. Acc: 84.86% |Macro avg f1 0.45904173106646057% |\n",
      "| Epoch: 06 | Train Loss: 0.380 | Train Acc: 88.83% | Val. Loss: 0.435 | Val. Acc: 84.86% |Macro avg f1 0.45904173106646057% |\n",
      "| Epoch: 07 | Train Loss: 0.343 | Train Acc: 88.83% | Val. Loss: 0.403 | Val. Acc: 84.86% |Macro avg f1 0.45904173106646057% |\n",
      "| Epoch: 08 | Train Loss: 0.330 | Train Acc: 88.83% | Val. Loss: 0.395 | Val. Acc: 84.86% |Macro avg f1 0.45904173106646057% |\n",
      "| Epoch: 09 | Train Loss: 0.334 | Train Acc: 88.83% | Val. Loss: 0.397 | Val. Acc: 84.86% |Macro avg f1 0.45904173106646057% |\n",
      "| Epoch: 10 | Train Loss: 0.342 | Train Acc: 88.83% | Val. Loss: 0.396 | Val. Acc: 84.86% |Macro avg f1 0.45904173106646057% |\n",
      "| Epoch: 11 | Train Loss: 0.340 | Train Acc: 88.83% | Val. Loss: 0.393 | Val. Acc: 84.86% |Macro avg f1 0.45904173106646057% |\n",
      "| Epoch: 12 | Train Loss: 0.329 | Train Acc: 88.83% | Val. Loss: 0.395 | Val. Acc: 84.86% |Macro avg f1 0.45904173106646057% |\n",
      "| Epoch: 13 | Train Loss: 0.320 | Train Acc: 88.83% | Val. Loss: 0.409 | Val. Acc: 84.86% |Macro avg f1 0.45904173106646057% |\n",
      "| Epoch: 14 | Train Loss: 0.320 | Train Acc: 88.83% | Val. Loss: 0.426 | Val. Acc: 84.86% |Macro avg f1 0.45904173106646057% |\n",
      "| Epoch: 15 | Train Loss: 0.325 | Train Acc: 88.83% | Val. Loss: 0.430 | Val. Acc: 84.86% |Macro avg f1 0.45904173106646057% |\n",
      "| Epoch: 16 | Train Loss: 0.326 | Train Acc: 88.83% | Val. Loss: 0.421 | Val. Acc: 84.86% |Macro avg f1 0.45904173106646057% |\n",
      "| Epoch: 17 | Train Loss: 0.321 | Train Acc: 88.83% | Val. Loss: 0.407 | Val. Acc: 84.86% |Macro avg f1 0.45904173106646057% |\n",
      "| Epoch: 18 | Train Loss: 0.314 | Train Acc: 88.83% | Val. Loss: 0.396 | Val. Acc: 84.86% |Macro avg f1 0.45904173106646057% |\n",
      "| Epoch: 19 | Train Loss: 0.310 | Train Acc: 88.83% | Val. Loss: 0.390 | Val. Acc: 84.86% |Macro avg f1 0.45904173106646057% |\n",
      "| Epoch: 20 | Train Loss: 0.312 | Train Acc: 88.83% | Val. Loss: 0.388 | Val. Acc: 84.86% |Macro avg f1 0.45904173106646057% |\n",
      "| Epoch: 21 | Train Loss: 0.312 | Train Acc: 88.83% | Val. Loss: 0.387 | Val. Acc: 84.86% |Macro avg f1 0.45904173106646057% |\n",
      "| Epoch: 22 | Train Loss: 0.310 | Train Acc: 88.83% | Val. Loss: 0.386 | Val. Acc: 84.86% |Macro avg f1 0.45904173106646057% |\n",
      "| Epoch: 23 | Train Loss: 0.307 | Train Acc: 88.83% | Val. Loss: 0.386 | Val. Acc: 84.86% |Macro avg f1 0.45904173106646057% |\n",
      "| Epoch: 24 | Train Loss: 0.301 | Train Acc: 88.83% | Val. Loss: 0.389 | Val. Acc: 84.86% |Macro avg f1 0.45904173106646057% |\n",
      "| Epoch: 25 | Train Loss: 0.298 | Train Acc: 88.83% | Val. Loss: 0.393 | Val. Acc: 84.86% |Macro avg f1 0.45904173106646057% |\n",
      "| Epoch: 26 | Train Loss: 0.295 | Train Acc: 88.83% | Val. Loss: 0.398 | Val. Acc: 84.86% |Macro avg f1 0.45904173106646057% |\n",
      "| Epoch: 27 | Train Loss: 0.293 | Train Acc: 88.83% | Val. Loss: 0.399 | Val. Acc: 84.86% |Macro avg f1 0.45904173106646057% |\n",
      "| Epoch: 28 | Train Loss: 0.291 | Train Acc: 88.83% | Val. Loss: 0.398 | Val. Acc: 84.86% |Macro avg f1 0.45904173106646057% |\n",
      "| Epoch: 29 | Train Loss: 0.287 | Train Acc: 88.83% | Val. Loss: 0.393 | Val. Acc: 84.86% |Macro avg f1 0.45904173106646057% |\n",
      "| Epoch: 30 | Train Loss: 0.282 | Train Acc: 88.83% | Val. Loss: 0.388 | Val. Acc: 84.86% |Macro avg f1 0.45904173106646057% |\n",
      "| Epoch: 31 | Train Loss: 0.277 | Train Acc: 88.83% | Val. Loss: 0.384 | Val. Acc: 84.86% |Macro avg f1 0.45904173106646057% |\n",
      "| Epoch: 32 | Train Loss: 0.272 | Train Acc: 88.83% | Val. Loss: 0.382 | Val. Acc: 84.71% |Macro avg f1 0.4586233565351895% |\n",
      "| Epoch: 33 | Train Loss: 0.270 | Train Acc: 89.00% | Val. Loss: 0.381 | Val. Acc: 84.86% |Macro avg f1 0.4770965468639887% |\n",
      "| Epoch: 34 | Train Loss: 0.264 | Train Acc: 89.10% | Val. Loss: 0.382 | Val. Acc: 85.29% |Macro avg f1 0.5196887636482337% |\n",
      "| Epoch: 35 | Train Loss: 0.258 | Train Acc: 89.37% | Val. Loss: 0.384 | Val. Acc: 85.14% |Macro avg f1 0.5187604114334066% |\n",
      "| Epoch: 36 | Train Loss: 0.253 | Train Acc: 89.37% | Val. Loss: 0.387 | Val. Acc: 85.14% |Macro avg f1 0.5187604114334066% |\n",
      "| Epoch: 37 | Train Loss: 0.245 | Train Acc: 89.23% | Val. Loss: 0.391 | Val. Acc: 85.14% |Macro avg f1 0.5187604114334066% |\n",
      "| Epoch: 38 | Train Loss: 0.239 | Train Acc: 89.50% | Val. Loss: 0.396 | Val. Acc: 85.14% |Macro avg f1 0.5187604114334066% |\n",
      "| Epoch: 39 | Train Loss: 0.232 | Train Acc: 89.53% | Val. Loss: 0.400 | Val. Acc: 85.29% |Macro avg f1 0.5270239243238279% |\n",
      "| Epoch: 40 | Train Loss: 0.224 | Train Acc: 89.90% | Val. Loss: 0.403 | Val. Acc: 85.29% |Macro avg f1 0.5270239243238279% |\n",
      "| Epoch: 41 | Train Loss: 0.218 | Train Acc: 89.93% | Val. Loss: 0.405 | Val. Acc: 85.43% |Macro avg f1 0.53515625% |\n",
      "| Epoch: 42 | Train Loss: 0.208 | Train Acc: 90.10% | Val. Loss: 0.408 | Val. Acc: 85.57% |Macro avg f1 0.5431606562461634% |\n",
      "| Epoch: 43 | Train Loss: 0.201 | Train Acc: 90.43% | Val. Loss: 0.410 | Val. Acc: 85.71% |Macro avg f1 0.551040303753303% |\n",
      "| Epoch: 44 | Train Loss: 0.195 | Train Acc: 90.80% | Val. Loss: 0.414 | Val. Acc: 85.71% |Macro avg f1 0.551040303753303% |\n",
      "| Epoch: 45 | Train Loss: 0.187 | Train Acc: 91.20% | Val. Loss: 0.420 | Val. Acc: 85.86% |Macro avg f1 0.5587982504727161% |\n",
      "| Test Loss: 0.404 | Test Acc: 87.00%\n",
      "    7 | 03m25s |    0.55880 |    0.0093 |    0.0003 | \n",
      "| Epoch: 01 | Train Loss: 0.730 | Train Acc: 36.33% | Val. Loss: 0.860 | Val. Acc: 84.86% |Macro avg f1 0.45904173106646057% |\n",
      "| Epoch: 02 | Train Loss: 0.640 | Train Acc: 88.83% | Val. Loss: 0.965 | Val. Acc: 84.86% |Macro avg f1 0.45904173106646057% |\n",
      "| Epoch: 03 | Train Loss: 0.718 | Train Acc: 88.83% | Val. Loss: 0.776 | Val. Acc: 84.86% |Macro avg f1 0.45904173106646057% |\n",
      "| Epoch: 04 | Train Loss: 0.577 | Train Acc: 88.83% | Val. Loss: 0.561 | Val. Acc: 84.86% |Macro avg f1 0.45904173106646057% |\n",
      "| Epoch: 05 | Train Loss: 0.423 | Train Acc: 88.83% | Val. Loss: 0.431 | Val. Acc: 84.86% |Macro avg f1 0.45904173106646057% |\n",
      "| Epoch: 06 | Train Loss: 0.341 | Train Acc: 88.83% | Val. Loss: 0.403 | Val. Acc: 84.86% |Macro avg f1 0.45904173106646057% |\n",
      "| Epoch: 07 | Train Loss: 0.342 | Train Acc: 88.83% | Val. Loss: 0.416 | Val. Acc: 84.86% |Macro avg f1 0.45904173106646057% |\n",
      "| Epoch: 08 | Train Loss: 0.368 | Train Acc: 88.83% | Val. Loss: 0.420 | Val. Acc: 84.86% |Macro avg f1 0.45904173106646057% |\n",
      "| Epoch: 09 | Train Loss: 0.371 | Train Acc: 88.83% | Val. Loss: 0.409 | Val. Acc: 84.86% |Macro avg f1 0.45904173106646057% |\n",
      "| Epoch: 10 | Train Loss: 0.350 | Train Acc: 88.83% | Val. Loss: 0.411 | Val. Acc: 84.86% |Macro avg f1 0.45904173106646057% |\n",
      "| Epoch: 11 | Train Loss: 0.334 | Train Acc: 88.83% | Val. Loss: 0.436 | Val. Acc: 84.86% |Macro avg f1 0.45904173106646057% |\n",
      "| Epoch: 12 | Train Loss: 0.341 | Train Acc: 88.83% | Val. Loss: 0.440 | Val. Acc: 84.86% |Macro avg f1 0.45904173106646057% |\n",
      "| Epoch: 13 | Train Loss: 0.342 | Train Acc: 88.83% | Val. Loss: 0.420 | Val. Acc: 84.86% |Macro avg f1 0.45904173106646057% |\n",
      "| Epoch: 14 | Train Loss: 0.332 | Train Acc: 88.83% | Val. Loss: 0.406 | Val. Acc: 84.86% |Macro avg f1 0.45904173106646057% |\n",
      "| Epoch: 15 | Train Loss: 0.334 | Train Acc: 88.83% | Val. Loss: 0.405 | Val. Acc: 84.86% |Macro avg f1 0.45904173106646057% |\n",
      "| Epoch: 16 | Train Loss: 0.343 | Train Acc: 88.83% | Val. Loss: 0.404 | Val. Acc: 84.86% |Macro avg f1 0.45904173106646057% |\n",
      "| Epoch: 17 | Train Loss: 0.341 | Train Acc: 88.83% | Val. Loss: 0.405 | Val. Acc: 84.86% |Macro avg f1 0.45904173106646057% |\n",
      "| Epoch: 18 | Train Loss: 0.331 | Train Acc: 88.83% | Val. Loss: 0.417 | Val. Acc: 84.86% |Macro avg f1 0.45904173106646057% |\n",
      "| Epoch: 19 | Train Loss: 0.331 | Train Acc: 88.83% | Val. Loss: 0.425 | Val. Acc: 84.86% |Macro avg f1 0.45904173106646057% |\n",
      "| Epoch: 20 | Train Loss: 0.335 | Train Acc: 88.83% | Val. Loss: 0.417 | Val. Acc: 84.86% |Macro avg f1 0.45904173106646057% |\n",
      "| Epoch: 21 | Train Loss: 0.331 | Train Acc: 88.83% | Val. Loss: 0.405 | Val. Acc: 84.86% |Macro avg f1 0.45904173106646057% |\n",
      "| Epoch: 22 | Train Loss: 0.329 | Train Acc: 88.83% | Val. Loss: 0.402 | Val. Acc: 84.86% |Macro avg f1 0.45904173106646057% |\n",
      "| Epoch: 23 | Train Loss: 0.334 | Train Acc: 88.83% | Val. Loss: 0.402 | Val. Acc: 84.86% |Macro avg f1 0.45904173106646057% |\n",
      "| Epoch: 24 | Train Loss: 0.334 | Train Acc: 88.83% | Val. Loss: 0.403 | Val. Acc: 84.86% |Macro avg f1 0.45904173106646057% |\n",
      "| Epoch: 25 | Train Loss: 0.330 | Train Acc: 88.83% | Val. Loss: 0.411 | Val. Acc: 84.86% |Macro avg f1 0.45904173106646057% |\n",
      "| Epoch: 26 | Train Loss: 0.329 | Train Acc: 88.83% | Val. Loss: 0.416 | Val. Acc: 84.86% |Macro avg f1 0.45904173106646057% |\n",
      "| Epoch: 27 | Train Loss: 0.330 | Train Acc: 88.83% | Val. Loss: 0.412 | Val. Acc: 84.86% |Macro avg f1 0.45904173106646057% |\n",
      "| Epoch: 28 | Train Loss: 0.329 | Train Acc: 88.83% | Val. Loss: 0.405 | Val. Acc: 84.86% |Macro avg f1 0.45904173106646057% |\n",
      "| Epoch: 29 | Train Loss: 0.328 | Train Acc: 88.83% | Val. Loss: 0.402 | Val. Acc: 84.86% |Macro avg f1 0.45904173106646057% |\n",
      "| Epoch: 30 | Train Loss: 0.330 | Train Acc: 88.83% | Val. Loss: 0.401 | Val. Acc: 84.86% |Macro avg f1 0.45904173106646057% |\n",
      "| Epoch: 31 | Train Loss: 0.330 | Train Acc: 88.83% | Val. Loss: 0.403 | Val. Acc: 84.86% |Macro avg f1 0.45904173106646057% |\n",
      "| Epoch: 32 | Train Loss: 0.328 | Train Acc: 88.83% | Val. Loss: 0.408 | Val. Acc: 84.86% |Macro avg f1 0.45904173106646057% |\n",
      "| Epoch: 33 | Train Loss: 0.327 | Train Acc: 88.83% | Val. Loss: 0.410 | Val. Acc: 84.86% |Macro avg f1 0.45904173106646057% |\n",
      "| Epoch: 34 | Train Loss: 0.327 | Train Acc: 88.83% | Val. Loss: 0.409 | Val. Acc: 84.86% |Macro avg f1 0.45904173106646057% |\n",
      "| Epoch: 35 | Train Loss: 0.326 | Train Acc: 88.83% | Val. Loss: 0.405 | Val. Acc: 84.86% |Macro avg f1 0.45904173106646057% |\n",
      "| Epoch: 36 | Train Loss: 0.326 | Train Acc: 88.83% | Val. Loss: 0.403 | Val. Acc: 84.86% |Macro avg f1 0.45904173106646057% |\n",
      "| Epoch: 37 | Train Loss: 0.327 | Train Acc: 88.83% | Val. Loss: 0.403 | Val. Acc: 84.86% |Macro avg f1 0.45904173106646057% |\n",
      "| Epoch: 38 | Train Loss: 0.327 | Train Acc: 88.83% | Val. Loss: 0.404 | Val. Acc: 84.86% |Macro avg f1 0.45904173106646057% |\n",
      "| Epoch: 39 | Train Loss: 0.326 | Train Acc: 88.83% | Val. Loss: 0.407 | Val. Acc: 84.86% |Macro avg f1 0.45904173106646057% |\n",
      "| Epoch: 40 | Train Loss: 0.324 | Train Acc: 88.83% | Val. Loss: 0.408 | Val. Acc: 84.86% |Macro avg f1 0.45904173106646057% |\n",
      "| Epoch: 41 | Train Loss: 0.324 | Train Acc: 88.83% | Val. Loss: 0.407 | Val. Acc: 84.86% |Macro avg f1 0.45904173106646057% |\n",
      "| Epoch: 42 | Train Loss: 0.325 | Train Acc: 88.83% | Val. Loss: 0.404 | Val. Acc: 84.86% |Macro avg f1 0.45904173106646057% |\n",
      "| Epoch: 43 | Train Loss: 0.324 | Train Acc: 88.83% | Val. Loss: 0.403 | Val. Acc: 84.86% |Macro avg f1 0.45904173106646057% |\n",
      "| Epoch: 44 | Train Loss: 0.324 | Train Acc: 88.83% | Val. Loss: 0.403 | Val. Acc: 84.86% |Macro avg f1 0.45904173106646057% |\n",
      "| Epoch: 45 | Train Loss: 0.324 | Train Acc: 88.83% | Val. Loss: 0.404 | Val. Acc: 84.86% |Macro avg f1 0.45904173106646057% |\n",
      "| Test Loss: 0.352 | Test Acc: 88.14%\n",
      "    8 | 03m25s |    0.45904 |    0.0173 |    0.0067 | \n",
      "Bayesian Optimization\n",
      "-----------------------------------------------------\n",
      " Step |   Time |      Value |        lr |        wd | \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Thomas\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1143: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| Epoch: 01 | Train Loss: 0.888 | Train Acc: 12.57% | Val. Loss: 3.428 | Val. Acc: 84.86% |Macro avg f1 0.45904173106646057% |\n",
      "| Epoch: 02 | Train Loss: 2.500 | Train Acc: 88.83% | Val. Loss: 2.894 | Val. Acc: 84.86% |Macro avg f1 0.45904173106646057% |\n",
      "| Epoch: 03 | Train Loss: 2.042 | Train Acc: 88.83% | Val. Loss: 1.400 | Val. Acc: 84.86% |Macro avg f1 0.45904173106646057% |\n",
      "| Epoch: 04 | Train Loss: 0.930 | Train Acc: 88.83% | Val. Loss: 0.638 | Val. Acc: 84.86% |Macro avg f1 0.45904173106646057% |\n",
      "| Epoch: 05 | Train Loss: 0.383 | Train Acc: 88.83% | Val. Loss: 0.624 | Val. Acc: 81.29% |Macro avg f1 0.47003716097115555% |\n",
      "| Epoch: 06 | Train Loss: 0.475 | Train Acc: 84.17% | Val. Loss: 0.629 | Val. Acc: 82.00% |Macro avg f1 0.48008771309330117% |\n",
      "| Epoch: 07 | Train Loss: 0.387 | Train Acc: 87.43% | Val. Loss: 0.688 | Val. Acc: 84.86% |Macro avg f1 0.45904173106646057% |\n",
      "| Epoch: 08 | Train Loss: 0.289 | Train Acc: 88.83% | Val. Loss: 0.894 | Val. Acc: 84.86% |Macro avg f1 0.45904173106646057% |\n",
      "| Epoch: 09 | Train Loss: 0.275 | Train Acc: 88.83% | Val. Loss: 1.108 | Val. Acc: 84.86% |Macro avg f1 0.45904173106646057% |\n",
      "| Epoch: 10 | Train Loss: 0.246 | Train Acc: 88.90% | Val. Loss: 1.315 | Val. Acc: 84.86% |Macro avg f1 0.45904173106646057% |\n",
      "| Epoch: 11 | Train Loss: 0.214 | Train Acc: 89.03% | Val. Loss: 1.523 | Val. Acc: 84.86% |Macro avg f1 0.45904173106646057% |\n",
      "| Epoch: 12 | Train Loss: 0.188 | Train Acc: 89.23% | Val. Loss: 1.737 | Val. Acc: 84.86% |Macro avg f1 0.45904173106646057% |\n",
      "| Epoch: 13 | Train Loss: 0.170 | Train Acc: 89.43% | Val. Loss: 1.954 | Val. Acc: 84.86% |Macro avg f1 0.45904173106646057% |\n",
      "| Epoch: 14 | Train Loss: 0.154 | Train Acc: 89.53% | Val. Loss: 2.174 | Val. Acc: 84.86% |Macro avg f1 0.45904173106646057% |\n",
      "| Epoch: 15 | Train Loss: 0.142 | Train Acc: 89.87% | Val. Loss: 2.392 | Val. Acc: 84.71% |Macro avg f1 0.4586233565351895% |\n",
      "| Epoch: 16 | Train Loss: 0.133 | Train Acc: 90.03% | Val. Loss: 2.605 | Val. Acc: 84.71% |Macro avg f1 0.4677335683170005% |\n",
      "| Epoch: 17 | Train Loss: 0.128 | Train Acc: 90.43% | Val. Loss: 2.810 | Val. Acc: 84.57% |Macro avg f1 0.48432512073340434% |\n",
      "| Epoch: 18 | Train Loss: 0.122 | Train Acc: 91.43% | Val. Loss: 3.010 | Val. Acc: 84.29% |Macro avg f1 0.49869791666666674% |\n",
      "| Epoch: 19 | Train Loss: 0.118 | Train Acc: 92.47% | Val. Loss: 3.206 | Val. Acc: 83.29% |Macro avg f1 0.5204216073781291% |\n",
      "| Epoch: 20 | Train Loss: 0.109 | Train Acc: 94.57% | Val. Loss: 3.393 | Val. Acc: 81.14% |Macro avg f1 0.5489250356368749% |\n",
      "| Epoch: 21 | Train Loss: 0.101 | Train Acc: 95.07% | Val. Loss: 3.565 | Val. Acc: 79.43% |Macro avg f1 0.5691644868441299% |\n",
      "| Epoch: 22 | Train Loss: 0.095 | Train Acc: 95.53% | Val. Loss: 3.735 | Val. Acc: 78.57% |Macro avg f1 0.5765036138358286% |\n",
      "| Epoch: 23 | Train Loss: 0.096 | Train Acc: 95.00% | Val. Loss: 3.895 | Val. Acc: 78.14% |Macro avg f1 0.5731009769649911% |\n",
      "| Epoch: 24 | Train Loss: 0.088 | Train Acc: 95.43% | Val. Loss: 4.034 | Val. Acc: 78.71% |Macro avg f1 0.5707659954483911% |\n",
      "| Epoch: 25 | Train Loss: 0.079 | Train Acc: 95.97% | Val. Loss: 4.152 | Val. Acc: 78.57% |Macro avg f1 0.5588161145565471% |\n",
      "| Epoch: 26 | Train Loss: 0.072 | Train Acc: 96.47% | Val. Loss: 4.249 | Val. Acc: 80.14% |Macro avg f1 0.5671226782337894% |\n",
      "| Epoch: 27 | Train Loss: 0.064 | Train Acc: 97.00% | Val. Loss: 4.324 | Val. Acc: 80.57% |Macro avg f1 0.5623310469114915% |\n",
      "| Epoch: 28 | Train Loss: 0.063 | Train Acc: 97.00% | Val. Loss: 4.381 | Val. Acc: 80.43% |Macro avg f1 0.5653533600134156% |\n",
      "| Epoch: 29 | Train Loss: 0.058 | Train Acc: 97.10% | Val. Loss: 4.426 | Val. Acc: 80.14% |Macro avg f1 0.5748845459828118% |\n",
      "| Epoch: 30 | Train Loss: 0.050 | Train Acc: 97.73% | Val. Loss: 4.471 | Val. Acc: 79.43% |Macro avg f1 0.5653973509933775% |\n",
      "| Epoch: 31 | Train Loss: 0.047 | Train Acc: 97.83% | Val. Loss: 4.516 | Val. Acc: 80.00% |Macro avg f1 0.5774696467991169% |\n",
      "| Epoch: 32 | Train Loss: 0.041 | Train Acc: 98.10% | Val. Loss: 4.564 | Val. Acc: 80.14% |Macro avg f1 0.5748845459828118% |\n",
      "| Epoch: 33 | Train Loss: 0.038 | Train Acc: 98.20% | Val. Loss: 4.610 | Val. Acc: 80.29% |Macro avg f1 0.5682565789473684% |\n",
      "| Epoch: 34 | Train Loss: 0.033 | Train Acc: 98.50% | Val. Loss: 4.649 | Val. Acc: 80.29% |Macro avg f1 0.5682565789473684% |\n",
      "| Epoch: 35 | Train Loss: 0.034 | Train Acc: 98.53% | Val. Loss: 4.705 | Val. Acc: 80.43% |Macro avg f1 0.5693971110542362% |\n",
      "| Epoch: 36 | Train Loss: 0.031 | Train Acc: 98.60% | Val. Loss: 4.699 | Val. Acc: 80.14% |Macro avg f1 0.5710462855606157% |\n",
      "| Epoch: 37 | Train Loss: 0.029 | Train Acc: 98.80% | Val. Loss: 4.694 | Val. Acc: 80.14% |Macro avg f1 0.5859133950420257% |\n",
      "| Epoch: 38 | Train Loss: 0.028 | Train Acc: 98.77% | Val. Loss: 4.696 | Val. Acc: 80.00% |Macro avg f1 0.5882283735861107% |\n",
      "| Epoch: 39 | Train Loss: 0.026 | Train Acc: 98.83% | Val. Loss: 4.696 | Val. Acc: 79.71% |Macro avg f1 0.5892494090811419% |\n",
      "| Epoch: 40 | Train Loss: 0.028 | Train Acc: 98.73% | Val. Loss: 4.701 | Val. Acc: 79.43% |Macro avg f1 0.5868581546330907% |\n",
      "| Epoch: 41 | Train Loss: 0.031 | Train Acc: 98.63% | Val. Loss: 4.788 | Val. Acc: 80.14% |Macro avg f1 0.5894358857509358% |\n",
      "| Epoch: 42 | Train Loss: 0.024 | Train Acc: 98.77% | Val. Loss: 4.861 | Val. Acc: 79.71% |Macro avg f1 0.5858333333333333% |\n",
      "| Epoch: 43 | Train Loss: 0.026 | Train Acc: 98.87% | Val. Loss: 4.901 | Val. Acc: 79.71% |Macro avg f1 0.5823459217801981% |\n",
      "| Epoch: 44 | Train Loss: 0.022 | Train Acc: 98.90% | Val. Loss: 4.935 | Val. Acc: 79.86% |Macro avg f1 0.583528488423611% |\n",
      "| Epoch: 45 | Train Loss: 0.020 | Train Acc: 98.97% | Val. Loss: 4.961 | Val. Acc: 79.86% |Macro avg f1 0.5799553143951484% |\n",
      "| Test Loss: 5.091 | Test Acc: 79.43%\n",
      "    9 | 03m23s |    0.57996 |    0.0500 |    0.0000 | \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Thomas\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1143: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| Epoch: 01 | Train Loss: 0.549 | Train Acc: 88.33% | Val. Loss: 3.527 | Val. Acc: 84.86% |Macro avg f1 0.45904173106646057% |\n",
      "| Epoch: 02 | Train Loss: 2.570 | Train Acc: 88.83% | Val. Loss: 1.769 | Val. Acc: 84.86% |Macro avg f1 0.45904173106646057% |\n",
      "| Epoch: 03 | Train Loss: 1.295 | Train Acc: 88.83% | Val. Loss: 0.554 | Val. Acc: 84.86% |Macro avg f1 0.45904173106646057% |\n",
      "| Epoch: 04 | Train Loss: 0.413 | Train Acc: 88.83% | Val. Loss: 0.482 | Val. Acc: 84.86% |Macro avg f1 0.45904173106646057% |\n",
      "| Epoch: 05 | Train Loss: 0.440 | Train Acc: 88.83% | Val. Loss: 0.505 | Val. Acc: 84.86% |Macro avg f1 0.45904173106646057% |\n",
      "| Epoch: 06 | Train Loss: 0.472 | Train Acc: 88.83% | Val. Loss: 0.428 | Val. Acc: 84.86% |Macro avg f1 0.45904173106646057% |\n",
      "| Epoch: 07 | Train Loss: 0.346 | Train Acc: 88.83% | Val. Loss: 0.559 | Val. Acc: 84.86% |Macro avg f1 0.45904173106646057% |\n",
      "| Epoch: 08 | Train Loss: 0.407 | Train Acc: 88.83% | Val. Loss: 0.447 | Val. Acc: 84.86% |Macro avg f1 0.45904173106646057% |\n",
      "| Epoch: 09 | Train Loss: 0.347 | Train Acc: 88.87% | Val. Loss: 0.410 | Val. Acc: 84.86% |Macro avg f1 0.45904173106646057% |\n",
      "| Epoch: 10 | Train Loss: 0.350 | Train Acc: 88.83% | Val. Loss: 0.412 | Val. Acc: 84.86% |Macro avg f1 0.45904173106646057% |\n",
      "| Epoch: 11 | Train Loss: 0.357 | Train Acc: 88.90% | Val. Loss: 0.429 | Val. Acc: 84.86% |Macro avg f1 0.45904173106646057% |\n",
      "| Epoch: 12 | Train Loss: 0.337 | Train Acc: 88.83% | Val. Loss: 0.438 | Val. Acc: 84.86% |Macro avg f1 0.45904173106646057% |\n",
      "| Epoch: 13 | Train Loss: 0.342 | Train Acc: 88.83% | Val. Loss: 0.411 | Val. Acc: 84.86% |Macro avg f1 0.45904173106646057% |\n",
      "| Epoch: 14 | Train Loss: 0.337 | Train Acc: 88.83% | Val. Loss: 0.408 | Val. Acc: 84.86% |Macro avg f1 0.45904173106646057% |\n",
      "| Epoch: 15 | Train Loss: 0.341 | Train Acc: 88.83% | Val. Loss: 0.414 | Val. Acc: 84.86% |Macro avg f1 0.45904173106646057% |\n",
      "| Epoch: 16 | Train Loss: 0.337 | Train Acc: 88.83% | Val. Loss: 0.429 | Val. Acc: 84.86% |Macro avg f1 0.45904173106646057% |\n",
      "| Epoch: 17 | Train Loss: 0.341 | Train Acc: 88.83% | Val. Loss: 0.413 | Val. Acc: 84.86% |Macro avg f1 0.45904173106646057% |\n",
      "| Epoch: 18 | Train Loss: 0.337 | Train Acc: 88.83% | Val. Loss: 0.409 | Val. Acc: 84.86% |Macro avg f1 0.45904173106646057% |\n",
      "| Epoch: 19 | Train Loss: 0.336 | Train Acc: 88.83% | Val. Loss: 0.421 | Val. Acc: 84.86% |Macro avg f1 0.45904173106646057% |\n",
      "| Epoch: 20 | Train Loss: 0.333 | Train Acc: 88.83% | Val. Loss: 0.419 | Val. Acc: 84.86% |Macro avg f1 0.45904173106646057% |\n",
      "| Epoch: 21 | Train Loss: 0.331 | Train Acc: 88.83% | Val. Loss: 0.410 | Val. Acc: 84.86% |Macro avg f1 0.45904173106646057% |\n",
      "| Epoch: 22 | Train Loss: 0.331 | Train Acc: 88.83% | Val. Loss: 0.409 | Val. Acc: 84.86% |Macro avg f1 0.45904173106646057% |\n",
      "| Epoch: 23 | Train Loss: 0.330 | Train Acc: 88.83% | Val. Loss: 0.413 | Val. Acc: 84.86% |Macro avg f1 0.45904173106646057% |\n",
      "| Epoch: 24 | Train Loss: 0.326 | Train Acc: 88.83% | Val. Loss: 0.413 | Val. Acc: 84.86% |Macro avg f1 0.45904173106646057% |\n",
      "| Epoch: 25 | Train Loss: 0.325 | Train Acc: 88.83% | Val. Loss: 0.407 | Val. Acc: 84.86% |Macro avg f1 0.45904173106646057% |\n",
      "| Epoch: 26 | Train Loss: 0.324 | Train Acc: 88.83% | Val. Loss: 0.406 | Val. Acc: 84.86% |Macro avg f1 0.45904173106646057% |\n",
      "| Epoch: 27 | Train Loss: 0.323 | Train Acc: 88.83% | Val. Loss: 0.409 | Val. Acc: 84.86% |Macro avg f1 0.45904173106646057% |\n",
      "| Epoch: 28 | Train Loss: 0.320 | Train Acc: 88.83% | Val. Loss: 0.405 | Val. Acc: 84.86% |Macro avg f1 0.45904173106646057% |\n",
      "| Epoch: 29 | Train Loss: 0.320 | Train Acc: 88.83% | Val. Loss: 0.402 | Val. Acc: 84.86% |Macro avg f1 0.45904173106646057% |\n",
      "| Epoch: 30 | Train Loss: 0.318 | Train Acc: 88.83% | Val. Loss: 0.404 | Val. Acc: 84.86% |Macro avg f1 0.45904173106646057% |\n",
      "| Epoch: 31 | Train Loss: 0.317 | Train Acc: 88.83% | Val. Loss: 0.404 | Val. Acc: 84.86% |Macro avg f1 0.45904173106646057% |\n",
      "| Epoch: 32 | Train Loss: 0.316 | Train Acc: 88.83% | Val. Loss: 0.401 | Val. Acc: 84.86% |Macro avg f1 0.45904173106646057% |\n",
      "| Epoch: 33 | Train Loss: 0.317 | Train Acc: 88.83% | Val. Loss: 0.402 | Val. Acc: 84.86% |Macro avg f1 0.45904173106646057% |\n",
      "| Epoch: 34 | Train Loss: 0.316 | Train Acc: 88.83% | Val. Loss: 0.403 | Val. Acc: 84.86% |Macro avg f1 0.45904173106646057% |\n",
      "| Epoch: 35 | Train Loss: 0.317 | Train Acc: 88.83% | Val. Loss: 0.402 | Val. Acc: 84.86% |Macro avg f1 0.45904173106646057% |\n",
      "| Epoch: 36 | Train Loss: 0.317 | Train Acc: 88.83% | Val. Loss: 0.402 | Val. Acc: 84.86% |Macro avg f1 0.45904173106646057% |\n",
      "| Epoch: 37 | Train Loss: 0.317 | Train Acc: 88.83% | Val. Loss: 0.403 | Val. Acc: 84.86% |Macro avg f1 0.45904173106646057% |\n",
      "| Epoch: 38 | Train Loss: 0.318 | Train Acc: 88.83% | Val. Loss: 0.404 | Val. Acc: 84.86% |Macro avg f1 0.45904173106646057% |\n",
      "| Epoch: 39 | Train Loss: 0.318 | Train Acc: 88.83% | Val. Loss: 0.403 | Val. Acc: 84.86% |Macro avg f1 0.45904173106646057% |\n",
      "| Epoch: 40 | Train Loss: 0.318 | Train Acc: 88.83% | Val. Loss: 0.404 | Val. Acc: 84.86% |Macro avg f1 0.45904173106646057% |\n",
      "| Epoch: 41 | Train Loss: 0.320 | Train Acc: 88.83% | Val. Loss: 0.404 | Val. Acc: 84.86% |Macro avg f1 0.45904173106646057% |\n",
      "| Epoch: 42 | Train Loss: 0.319 | Train Acc: 88.83% | Val. Loss: 0.404 | Val. Acc: 84.86% |Macro avg f1 0.45904173106646057% |\n",
      "| Epoch: 43 | Train Loss: 0.321 | Train Acc: 88.83% | Val. Loss: 0.404 | Val. Acc: 84.86% |Macro avg f1 0.45904173106646057% |\n",
      "| Epoch: 44 | Train Loss: 0.319 | Train Acc: 88.83% | Val. Loss: 0.405 | Val. Acc: 84.86% |Macro avg f1 0.45904173106646057% |\n",
      "| Epoch: 45 | Train Loss: 0.320 | Train Acc: 88.83% | Val. Loss: 0.405 | Val. Acc: 84.86% |Macro avg f1 0.45904173106646057% |\n",
      "| Test Loss: 0.349 | Test Acc: 88.14%\n",
      "   10 | 03m43s |    0.45904 |    0.0500 |    0.0100 | \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Thomas\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1143: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| Epoch: 01 | Train Loss: 0.686 | Train Acc: 57.40% | Val. Loss: 2.651 | Val. Acc: 84.86% |Macro avg f1 0.45904173106646057% |\n",
      "| Epoch: 02 | Train Loss: 1.937 | Train Acc: 88.83% | Val. Loss: 2.013 | Val. Acc: 84.86% |Macro avg f1 0.45904173106646057% |\n",
      "| Epoch: 03 | Train Loss: 1.434 | Train Acc: 88.83% | Val. Loss: 0.979 | Val. Acc: 84.86% |Macro avg f1 0.45904173106646057% |\n",
      "| Epoch: 04 | Train Loss: 0.669 | Train Acc: 88.83% | Val. Loss: 0.499 | Val. Acc: 84.86% |Macro avg f1 0.45904173106646057% |\n",
      "| Epoch: 05 | Train Loss: 0.329 | Train Acc: 88.83% | Val. Loss: 0.534 | Val. Acc: 84.86% |Macro avg f1 0.45904173106646057% |\n",
      "| Epoch: 06 | Train Loss: 0.412 | Train Acc: 88.73% | Val. Loss: 0.597 | Val. Acc: 83.14% |Macro avg f1 0.47022755842889763% |\n",
      "| Epoch: 07 | Train Loss: 0.405 | Train Acc: 87.17% | Val. Loss: 0.634 | Val. Acc: 84.86% |Macro avg f1 0.45904173106646057% |\n",
      "| Epoch: 08 | Train Loss: 0.290 | Train Acc: 88.80% | Val. Loss: 0.831 | Val. Acc: 84.86% |Macro avg f1 0.45904173106646057% |\n",
      "| Epoch: 09 | Train Loss: 0.272 | Train Acc: 88.83% | Val. Loss: 1.025 | Val. Acc: 84.86% |Macro avg f1 0.45904173106646057% |\n",
      "| Epoch: 10 | Train Loss: 0.247 | Train Acc: 88.83% | Val. Loss: 1.183 | Val. Acc: 84.86% |Macro avg f1 0.45904173106646057% |\n",
      "| Epoch: 11 | Train Loss: 0.217 | Train Acc: 88.90% | Val. Loss: 1.320 | Val. Acc: 84.86% |Macro avg f1 0.45904173106646057% |\n",
      "| Epoch: 12 | Train Loss: 0.191 | Train Acc: 88.97% | Val. Loss: 1.454 | Val. Acc: 84.86% |Macro avg f1 0.45904173106646057% |\n",
      "| Epoch: 13 | Train Loss: 0.174 | Train Acc: 89.03% | Val. Loss: 1.590 | Val. Acc: 84.86% |Macro avg f1 0.45904173106646057% |\n",
      "| Epoch: 14 | Train Loss: 0.157 | Train Acc: 89.23% | Val. Loss: 1.730 | Val. Acc: 84.86% |Macro avg f1 0.45904173106646057% |\n",
      "| Epoch: 15 | Train Loss: 0.146 | Train Acc: 89.43% | Val. Loss: 1.872 | Val. Acc: 84.71% |Macro avg f1 0.4586233565351895% |\n",
      "| Epoch: 16 | Train Loss: 0.137 | Train Acc: 89.87% | Val. Loss: 2.013 | Val. Acc: 85.43% |Macro avg f1 0.528015018905841% |\n",
      "| Epoch: 17 | Train Loss: 0.130 | Train Acc: 91.13% | Val. Loss: 2.151 | Val. Acc: 85.29% |Macro avg f1 0.5721347567814564% |\n",
      "| Epoch: 18 | Train Loss: 0.122 | Train Acc: 92.50% | Val. Loss: 2.287 | Val. Acc: 84.43% |Macro avg f1 0.5807116362137656% |\n",
      "| Epoch: 19 | Train Loss: 0.114 | Train Acc: 93.90% | Val. Loss: 2.420 | Val. Acc: 83.29% |Macro avg f1 0.5938124593936449% |\n",
      "| Epoch: 20 | Train Loss: 0.107 | Train Acc: 94.70% | Val. Loss: 2.555 | Val. Acc: 80.57% |Macro avg f1 0.5783954225788737% |\n",
      "| Epoch: 21 | Train Loss: 0.100 | Train Acc: 95.37% | Val. Loss: 2.689 | Val. Acc: 79.57% |Macro avg f1 0.5811697956895217% |\n",
      "| Epoch: 22 | Train Loss: 0.095 | Train Acc: 95.63% | Val. Loss: 2.819 | Val. Acc: 78.57% |Macro avg f1 0.5765036138358286% |\n",
      "| Epoch: 23 | Train Loss: 0.089 | Train Acc: 95.83% | Val. Loss: 2.948 | Val. Acc: 78.71% |Macro avg f1 0.5809881929462998% |\n",
      "| Epoch: 24 | Train Loss: 0.081 | Train Acc: 96.20% | Val. Loss: 3.071 | Val. Acc: 79.14% |Macro avg f1 0.5811200734474391% |\n",
      "| Epoch: 25 | Train Loss: 0.072 | Train Acc: 96.43% | Val. Loss: 3.181 | Val. Acc: 79.57% |Macro avg f1 0.5702958991375869% |\n",
      "| Epoch: 26 | Train Loss: 0.067 | Train Acc: 96.87% | Val. Loss: 3.266 | Val. Acc: 79.43% |Macro avg f1 0.5691644868441299% |\n",
      "| Epoch: 27 | Train Loss: 0.058 | Train Acc: 97.23% | Val. Loss: 3.340 | Val. Acc: 79.14% |Macro avg f1 0.5741666666666667% |\n",
      "| Epoch: 28 | Train Loss: 0.054 | Train Acc: 97.33% | Val. Loss: 3.408 | Val. Acc: 79.29% |Macro avg f1 0.5788364267071647% |\n",
      "| Epoch: 29 | Train Loss: 0.049 | Train Acc: 97.57% | Val. Loss: 3.474 | Val. Acc: 80.14% |Macro avg f1 0.5859133950420257% |\n",
      "| Epoch: 30 | Train Loss: 0.046 | Train Acc: 97.57% | Val. Loss: 3.544 | Val. Acc: 80.71% |Macro avg f1 0.5907655931300586% |\n",
      "| Epoch: 31 | Train Loss: 0.041 | Train Acc: 97.93% | Val. Loss: 3.620 | Val. Acc: 81.29% |Macro avg f1 0.5957342691254723% |\n",
      "| Epoch: 32 | Train Loss: 0.038 | Train Acc: 98.20% | Val. Loss: 3.684 | Val. Acc: 80.86% |Macro avg f1 0.5919965202261853% |\n",
      "| Epoch: 33 | Train Loss: 0.034 | Train Acc: 98.30% | Val. Loss: 3.753 | Val. Acc: 80.43% |Macro avg f1 0.5847028611764299% |\n",
      "| Epoch: 34 | Train Loss: 0.031 | Train Acc: 98.43% | Val. Loss: 3.820 | Val. Acc: 80.71% |Macro avg f1 0.5907655931300586% |\n",
      "| Epoch: 35 | Train Loss: 0.031 | Train Acc: 98.53% | Val. Loss: 3.874 | Val. Acc: 80.57% |Macro avg f1 0.5895419426048565% |\n",
      "| Epoch: 36 | Train Loss: 0.027 | Train Acc: 98.57% | Val. Loss: 3.926 | Val. Acc: 80.57% |Macro avg f1 0.5895419426048565% |\n",
      "| Epoch: 37 | Train Loss: 0.026 | Train Acc: 98.67% | Val. Loss: 3.976 | Val. Acc: 80.29% |Macro avg f1 0.5835057947019868% |\n",
      "| Epoch: 38 | Train Loss: 0.025 | Train Acc: 98.67% | Val. Loss: 4.024 | Val. Acc: 80.14% |Macro avg f1 0.5823155942666054% |\n",
      "| Epoch: 39 | Train Loss: 0.025 | Train Acc: 98.83% | Val. Loss: 4.083 | Val. Acc: 80.00% |Macro avg f1 0.5811321399873485% |\n",
      "| Epoch: 40 | Train Loss: 0.021 | Train Acc: 98.83% | Val. Loss: 4.144 | Val. Acc: 80.29% |Macro avg f1 0.5798173118747281% |\n",
      "| Epoch: 41 | Train Loss: 0.021 | Train Acc: 98.93% | Val. Loss: 4.190 | Val. Acc: 80.71% |Macro avg f1 0.594335289395624% |\n",
      "| Epoch: 42 | Train Loss: 0.021 | Train Acc: 98.93% | Val. Loss: 4.238 | Val. Acc: 80.71% |Macro avg f1 0.5907655931300586% |\n",
      "| Epoch: 43 | Train Loss: 0.023 | Train Acc: 98.90% | Val. Loss: 4.275 | Val. Acc: 80.43% |Macro avg f1 0.5847028611764299% |\n",
      "| Epoch: 44 | Train Loss: 0.020 | Train Acc: 98.93% | Val. Loss: 4.330 | Val. Acc: 80.43% |Macro avg f1 0.5987430909752761% |\n",
      "| Epoch: 45 | Train Loss: 0.019 | Train Acc: 98.93% | Val. Loss: 4.372 | Val. Acc: 80.14% |Macro avg f1 0.5962638849123856% |\n",
      "| Test Loss: 4.564 | Test Acc: 79.57%\n",
      "   11 | 03m30s |    0.59626 |    0.0409 |    0.0000 | \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Thomas\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1143: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| Epoch: 01 | Train Loss: 0.645 | Train Acc: 74.90% | Val. Loss: 1.928 | Val. Acc: 84.86% |Macro avg f1 0.45904173106646057% |\n",
      "| Epoch: 02 | Train Loss: 1.409 | Train Acc: 88.83% | Val. Loss: 1.657 | Val. Acc: 84.86% |Macro avg f1 0.45904173106646057% |\n",
      "| Epoch: 03 | Train Loss: 1.185 | Train Acc: 88.83% | Val. Loss: 0.987 | Val. Acc: 84.86% |Macro avg f1 0.45904173106646057% |\n",
      "| Epoch: 04 | Train Loss: 0.695 | Train Acc: 88.83% | Val. Loss: 0.554 | Val. Acc: 84.86% |Macro avg f1 0.45904173106646057% |\n",
      "| Epoch: 05 | Train Loss: 0.378 | Train Acc: 88.83% | Val. Loss: 0.436 | Val. Acc: 84.86% |Macro avg f1 0.45904173106646057% |\n",
      "| Epoch: 06 | Train Loss: 0.324 | Train Acc: 88.83% | Val. Loss: 0.518 | Val. Acc: 84.86% |Macro avg f1 0.45904173106646057% |\n",
      "| Epoch: 07 | Train Loss: 0.408 | Train Acc: 88.83% | Val. Loss: 0.529 | Val. Acc: 84.71% |Macro avg f1 0.4586233565351895% |\n",
      "| Epoch: 08 | Train Loss: 0.356 | Train Acc: 88.70% | Val. Loss: 0.566 | Val. Acc: 84.86% |Macro avg f1 0.45904173106646057% |\n",
      "| Epoch: 09 | Train Loss: 0.281 | Train Acc: 88.83% | Val. Loss: 0.694 | Val. Acc: 84.86% |Macro avg f1 0.45904173106646057% |\n",
      "| Epoch: 10 | Train Loss: 0.266 | Train Acc: 88.83% | Val. Loss: 0.828 | Val. Acc: 84.86% |Macro avg f1 0.45904173106646057% |\n",
      "| Epoch: 11 | Train Loss: 0.254 | Train Acc: 88.83% | Val. Loss: 0.932 | Val. Acc: 84.86% |Macro avg f1 0.45904173106646057% |\n",
      "| Epoch: 12 | Train Loss: 0.233 | Train Acc: 88.83% | Val. Loss: 1.011 | Val. Acc: 84.86% |Macro avg f1 0.45904173106646057% |\n",
      "| Epoch: 13 | Train Loss: 0.210 | Train Acc: 88.83% | Val. Loss: 1.074 | Val. Acc: 84.86% |Macro avg f1 0.45904173106646057% |\n",
      "| Epoch: 14 | Train Loss: 0.189 | Train Acc: 88.83% | Val. Loss: 1.130 | Val. Acc: 84.86% |Macro avg f1 0.45904173106646057% |\n",
      "| Epoch: 15 | Train Loss: 0.175 | Train Acc: 89.00% | Val. Loss: 1.185 | Val. Acc: 84.86% |Macro avg f1 0.45904173106646057% |\n",
      "| Epoch: 16 | Train Loss: 0.161 | Train Acc: 89.10% | Val. Loss: 1.241 | Val. Acc: 84.71% |Macro avg f1 0.4586233565351895% |\n",
      "| Epoch: 17 | Train Loss: 0.149 | Train Acc: 89.33% | Val. Loss: 1.299 | Val. Acc: 84.71% |Macro avg f1 0.4586233565351895% |\n",
      "| Epoch: 18 | Train Loss: 0.140 | Train Acc: 89.43% | Val. Loss: 1.360 | Val. Acc: 84.43% |Macro avg f1 0.45778466305189774% |\n",
      "| Epoch: 19 | Train Loss: 0.133 | Train Acc: 89.90% | Val. Loss: 1.422 | Val. Acc: 84.43% |Macro avg f1 0.4753525726977055% |\n",
      "| Epoch: 20 | Train Loss: 0.126 | Train Acc: 90.27% | Val. Loss: 1.486 | Val. Acc: 85.43% |Macro avg f1 0.5675348273773471% |\n",
      "| Epoch: 21 | Train Loss: 0.120 | Train Acc: 92.10% | Val. Loss: 1.551 | Val. Acc: 84.71% |Macro avg f1 0.567129589495524% |\n",
      "| Epoch: 22 | Train Loss: 0.116 | Train Acc: 93.13% | Val. Loss: 1.613 | Val. Acc: 84.57% |Macro avg f1 0.6014676113360324% |\n",
      "| Epoch: 23 | Train Loss: 0.111 | Train Acc: 94.17% | Val. Loss: 1.674 | Val. Acc: 83.71% |Macro avg f1 0.614448051948052% |\n",
      "| Epoch: 24 | Train Loss: 0.106 | Train Acc: 94.83% | Val. Loss: 1.735 | Val. Acc: 82.43% |Macro avg f1 0.6022561913605056% |\n",
      "| Epoch: 25 | Train Loss: 0.099 | Train Acc: 95.57% | Val. Loss: 1.797 | Val. Acc: 81.14% |Macro avg f1 0.5907955572089069% |\n",
      "| Epoch: 26 | Train Loss: 0.093 | Train Acc: 96.07% | Val. Loss: 1.862 | Val. Acc: 80.57% |Macro avg f1 0.5930997931305672% |\n",
      "| Epoch: 27 | Train Loss: 0.089 | Train Acc: 95.83% | Val. Loss: 1.929 | Val. Acc: 79.43% |Macro avg f1 0.5834641894907353% |\n",
      "| Epoch: 28 | Train Loss: 0.081 | Train Acc: 96.27% | Val. Loss: 1.994 | Val. Acc: 79.14% |Macro avg f1 0.5811200734474391% |\n",
      "| Epoch: 29 | Train Loss: 0.076 | Train Acc: 96.40% | Val. Loss: 2.052 | Val. Acc: 79.29% |Macro avg f1 0.5788364267071647% |\n",
      "| Epoch: 30 | Train Loss: 0.071 | Train Acc: 96.80% | Val. Loss: 2.101 | Val. Acc: 79.43% |Macro avg f1 0.5764634699742852% |\n",
      "| Epoch: 31 | Train Loss: 0.061 | Train Acc: 97.30% | Val. Loss: 2.144 | Val. Acc: 80.00% |Macro avg f1 0.5811321399873485% |\n",
      "| Epoch: 32 | Train Loss: 0.056 | Train Acc: 97.67% | Val. Loss: 2.184 | Val. Acc: 80.00% |Macro avg f1 0.5811321399873485% |\n",
      "| Epoch: 33 | Train Loss: 0.049 | Train Acc: 97.80% | Val. Loss: 2.220 | Val. Acc: 80.57% |Macro avg f1 0.5821922617793694% |\n",
      "| Epoch: 34 | Train Loss: 0.043 | Train Acc: 98.33% | Val. Loss: 2.255 | Val. Acc: 80.86% |Macro avg f1 0.5687673550451462% |\n",
      "| Epoch: 35 | Train Loss: 0.038 | Train Acc: 98.43% | Val. Loss: 2.285 | Val. Acc: 80.86% |Macro avg f1 0.5687673550451462% |\n",
      "| Epoch: 36 | Train Loss: 0.035 | Train Acc: 98.53% | Val. Loss: 2.313 | Val. Acc: 80.71% |Macro avg f1 0.5676224726504056% |\n",
      "| Epoch: 37 | Train Loss: 0.031 | Train Acc: 98.87% | Val. Loss: 2.340 | Val. Acc: 80.57% |Macro avg f1 0.5705443981306051% |\n",
      "| Epoch: 38 | Train Loss: 0.029 | Train Acc: 98.90% | Val. Loss: 2.374 | Val. Acc: 80.43% |Macro avg f1 0.5653533600134156% |\n",
      "| Epoch: 39 | Train Loss: 0.026 | Train Acc: 99.17% | Val. Loss: 2.415 | Val. Acc: 80.29% |Macro avg f1 0.555894738777837% |\n",
      "| Epoch: 40 | Train Loss: 0.023 | Train Acc: 99.27% | Val. Loss: 2.463 | Val. Acc: 80.29% |Macro avg f1 0.5515819964349377% |\n",
      "| Epoch: 41 | Train Loss: 0.018 | Train Acc: 99.30% | Val. Loss: 2.507 | Val. Acc: 80.29% |Macro avg f1 0.5515819964349377% |\n",
      "| Epoch: 42 | Train Loss: 0.018 | Train Acc: 99.33% | Val. Loss: 2.550 | Val. Acc: 79.86% |Macro avg f1 0.5395918367346939% |\n",
      "| Epoch: 43 | Train Loss: 0.017 | Train Acc: 99.33% | Val. Loss: 2.592 | Val. Acc: 79.71% |Macro avg f1 0.5340421143425025% |\n",
      "| Epoch: 44 | Train Loss: 0.017 | Train Acc: 99.27% | Val. Loss: 2.626 | Val. Acc: 79.29% |Macro avg f1 0.5442524168092281% |\n",
      "| Epoch: 45 | Train Loss: 0.015 | Train Acc: 99.30% | Val. Loss: 2.647 | Val. Acc: 79.00% |Macro avg f1 0.5463583019957589% |\n",
      "| Test Loss: 2.509 | Test Acc: 80.57%\n",
      "   12 | 03m33s |    0.54636 |    0.0316 |    0.0000 | \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Thomas\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1143: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| Epoch: 01 | Train Loss: 0.657 | Train Acc: 71.03% | Val. Loss: 0.824 | Val. Acc: 84.86% |Macro avg f1 0.45904173106646057% |\n",
      "| Epoch: 02 | Train Loss: 0.613 | Train Acc: 88.83% | Val. Loss: 0.920 | Val. Acc: 84.86% |Macro avg f1 0.45904173106646057% |\n",
      "| Epoch: 03 | Train Loss: 0.673 | Train Acc: 88.83% | Val. Loss: 0.766 | Val. Acc: 84.86% |Macro avg f1 0.45904173106646057% |\n",
      "| Epoch: 04 | Train Loss: 0.559 | Train Acc: 88.83% | Val. Loss: 0.578 | Val. Acc: 84.86% |Macro avg f1 0.45904173106646057% |\n",
      "| Epoch: 05 | Train Loss: 0.426 | Train Acc: 88.83% | Val. Loss: 0.449 | Val. Acc: 84.86% |Macro avg f1 0.45904173106646057% |\n",
      "| Epoch: 06 | Train Loss: 0.342 | Train Acc: 88.83% | Val. Loss: 0.404 | Val. Acc: 84.86% |Macro avg f1 0.45904173106646057% |\n",
      "| Epoch: 07 | Train Loss: 0.322 | Train Acc: 88.83% | Val. Loss: 0.409 | Val. Acc: 84.86% |Macro avg f1 0.45904173106646057% |\n",
      "| Epoch: 08 | Train Loss: 0.338 | Train Acc: 88.83% | Val. Loss: 0.421 | Val. Acc: 84.86% |Macro avg f1 0.45904173106646057% |\n",
      "| Epoch: 09 | Train Loss: 0.349 | Train Acc: 88.83% | Val. Loss: 0.421 | Val. Acc: 84.86% |Macro avg f1 0.45904173106646057% |\n",
      "| Epoch: 10 | Train Loss: 0.337 | Train Acc: 88.83% | Val. Loss: 0.414 | Val. Acc: 84.86% |Macro avg f1 0.45904173106646057% |\n",
      "| Epoch: 11 | Train Loss: 0.308 | Train Acc: 88.83% | Val. Loss: 0.419 | Val. Acc: 84.86% |Macro avg f1 0.45904173106646057% |\n",
      "| Epoch: 12 | Train Loss: 0.278 | Train Acc: 88.83% | Val. Loss: 0.444 | Val. Acc: 84.86% |Macro avg f1 0.45904173106646057% |\n",
      "| Epoch: 13 | Train Loss: 0.262 | Train Acc: 88.83% | Val. Loss: 0.480 | Val. Acc: 84.86% |Macro avg f1 0.45904173106646057% |\n",
      "| Epoch: 14 | Train Loss: 0.255 | Train Acc: 88.83% | Val. Loss: 0.510 | Val. Acc: 84.86% |Macro avg f1 0.45904173106646057% |\n",
      "| Epoch: 15 | Train Loss: 0.245 | Train Acc: 88.83% | Val. Loss: 0.529 | Val. Acc: 84.86% |Macro avg f1 0.45904173106646057% |\n",
      "| Epoch: 16 | Train Loss: 0.230 | Train Acc: 88.83% | Val. Loss: 0.540 | Val. Acc: 84.86% |Macro avg f1 0.45904173106646057% |\n",
      "| Epoch: 17 | Train Loss: 0.213 | Train Acc: 88.83% | Val. Loss: 0.546 | Val. Acc: 84.86% |Macro avg f1 0.45904173106646057% |\n",
      "| Epoch: 18 | Train Loss: 0.197 | Train Acc: 88.83% | Val. Loss: 0.550 | Val. Acc: 84.86% |Macro avg f1 0.45904173106646057% |\n",
      "| Epoch: 19 | Train Loss: 0.183 | Train Acc: 88.87% | Val. Loss: 0.555 | Val. Acc: 84.86% |Macro avg f1 0.45904173106646057% |\n",
      "| Epoch: 20 | Train Loss: 0.169 | Train Acc: 89.37% | Val. Loss: 0.563 | Val. Acc: 84.86% |Macro avg f1 0.4770965468639887% |\n",
      "| Epoch: 21 | Train Loss: 0.159 | Train Acc: 90.07% | Val. Loss: 0.573 | Val. Acc: 85.14% |Macro avg f1 0.49534161490683226% |\n",
      "| Epoch: 22 | Train Loss: 0.151 | Train Acc: 91.50% | Val. Loss: 0.588 | Val. Acc: 85.43% |Macro avg f1 0.5487410253817373% |\n",
      "| Epoch: 23 | Train Loss: 0.141 | Train Acc: 93.20% | Val. Loss: 0.606 | Val. Acc: 85.43% |Macro avg f1 0.5791186248850534% |\n",
      "| Epoch: 24 | Train Loss: 0.134 | Train Acc: 94.33% | Val. Loss: 0.630 | Val. Acc: 85.43% |Macro avg f1 0.605228237791932% |\n",
      "| Epoch: 25 | Train Loss: 0.126 | Train Acc: 95.13% | Val. Loss: 0.658 | Val. Acc: 84.43% |Macro avg f1 0.6046119963725872% |\n",
      "| Epoch: 26 | Train Loss: 0.118 | Train Acc: 95.67% | Val. Loss: 0.692 | Val. Acc: 82.86% |Macro avg f1 0.5982783357245336% |\n",
      "| Epoch: 27 | Train Loss: 0.109 | Train Acc: 95.83% | Val. Loss: 0.732 | Val. Acc: 82.00% |Macro avg f1 0.6057994851258581% |\n",
      "| Epoch: 28 | Train Loss: 0.100 | Train Acc: 96.17% | Val. Loss: 0.778 | Val. Acc: 80.57% |Macro avg f1 0.5965828191741813% |\n",
      "| Epoch: 29 | Train Loss: 0.093 | Train Acc: 96.53% | Val. Loss: 0.831 | Val. Acc: 80.14% |Macro avg f1 0.5962638849123856% |\n",
      "| Epoch: 30 | Train Loss: 0.085 | Train Acc: 96.80% | Val. Loss: 0.889 | Val. Acc: 80.14% |Macro avg f1 0.5962638849123856% |\n",
      "| Epoch: 31 | Train Loss: 0.077 | Train Acc: 97.07% | Val. Loss: 0.952 | Val. Acc: 80.71% |Macro avg f1 0.5978295563357803% |\n",
      "| Epoch: 32 | Train Loss: 0.067 | Train Acc: 97.47% | Val. Loss: 1.018 | Val. Acc: 80.86% |Macro avg f1 0.5919965202261853% |\n",
      "| Epoch: 33 | Train Loss: 0.059 | Train Acc: 97.73% | Val. Loss: 1.086 | Val. Acc: 80.86% |Macro avg f1 0.580770881006865% |\n",
      "| Epoch: 34 | Train Loss: 0.052 | Train Acc: 97.83% | Val. Loss: 1.150 | Val. Acc: 81.86% |Macro avg f1 0.5893214333559691% |\n",
      "| Epoch: 35 | Train Loss: 0.044 | Train Acc: 98.07% | Val. Loss: 1.211 | Val. Acc: 81.86% |Macro avg f1 0.5853061224489796% |\n",
      "| Epoch: 36 | Train Loss: 0.040 | Train Acc: 98.30% | Val. Loss: 1.266 | Val. Acc: 82.29% |Macro avg f1 0.5848876135820181% |\n",
      "| Epoch: 37 | Train Loss: 0.035 | Train Acc: 98.57% | Val. Loss: 1.316 | Val. Acc: 82.86% |Macro avg f1 0.5811643630706635% |\n",
      "| Epoch: 38 | Train Loss: 0.032 | Train Acc: 98.57% | Val. Loss: 1.358 | Val. Acc: 83.00% |Macro avg f1 0.5824121837386392% |\n",
      "| Epoch: 39 | Train Loss: 0.029 | Train Acc: 98.83% | Val. Loss: 1.397 | Val. Acc: 82.57% |Macro avg f1 0.5741837691218413% |\n",
      "| Epoch: 40 | Train Loss: 0.024 | Train Acc: 99.03% | Val. Loss: 1.436 | Val. Acc: 81.57% |Macro avg f1 0.561461019566706% |\n",
      "| Epoch: 41 | Train Loss: 0.024 | Train Acc: 99.03% | Val. Loss: 1.474 | Val. Acc: 81.57% |Macro avg f1 0.5659467695960855% |\n",
      "| Epoch: 42 | Train Loss: 0.023 | Train Acc: 99.00% | Val. Loss: 1.516 | Val. Acc: 80.86% |Macro avg f1 0.5645796197266786% |\n",
      "| Epoch: 43 | Train Loss: 0.022 | Train Acc: 99.10% | Val. Loss: 1.558 | Val. Acc: 80.57% |Macro avg f1 0.5664845173041895% |\n",
      "| Epoch: 44 | Train Loss: 0.019 | Train Acc: 99.23% | Val. Loss: 1.600 | Val. Acc: 80.29% |Macro avg f1 0.5642288745737021% |\n",
      "| Epoch: 45 | Train Loss: 0.018 | Train Acc: 99.37% | Val. Loss: 1.641 | Val. Acc: 79.43% |Macro avg f1 0.5535951533188075% |\n",
      "| Test Loss: 1.468 | Test Acc: 80.43%\n",
      "   13 | 03m37s |    0.55360 |    0.0163 |    0.0000 | \n",
      "| Epoch: 01 | Train Loss: 0.717 | Train Acc: 39.70% | Val. Loss: 0.713 | Val. Acc: 35.14% |Macro avg f1 0.34002857522594365% |\n",
      "| Epoch: 02 | Train Loss: 0.716 | Train Acc: 40.23% | Val. Loss: 0.711 | Val. Acc: 36.29% |Macro avg f1 0.34958333333333336% |\n",
      "| Epoch: 03 | Train Loss: 0.715 | Train Acc: 40.23% | Val. Loss: 0.710 | Val. Acc: 37.57% |Macro avg f1 0.35961178950717954% |\n",
      "| Epoch: 04 | Train Loss: 0.713 | Train Acc: 42.17% | Val. Loss: 0.709 | Val. Acc: 38.14% |Macro avg f1 0.3636510599167351% |\n",
      "| Epoch: 05 | Train Loss: 0.713 | Train Acc: 41.83% | Val. Loss: 0.708 | Val. Acc: 39.14% |Macro avg f1 0.3703760008107835% |\n",
      "| Epoch: 06 | Train Loss: 0.712 | Train Acc: 43.07% | Val. Loss: 0.706 | Val. Acc: 40.57% |Macro avg f1 0.3816832145678153% |\n",
      "| Epoch: 07 | Train Loss: 0.708 | Train Acc: 43.57% | Val. Loss: 0.705 | Val. Acc: 41.71% |Macro avg f1 0.38909090909090915% |\n",
      "| Epoch: 08 | Train Loss: 0.707 | Train Acc: 45.10% | Val. Loss: 0.704 | Val. Acc: 43.00% |Macro avg f1 0.3981628127969591% |\n",
      "| Epoch: 09 | Train Loss: 0.706 | Train Acc: 46.00% | Val. Loss: 0.703 | Val. Acc: 44.57% |Macro avg f1 0.41009869335557414% |\n",
      "| Epoch: 10 | Train Loss: 0.705 | Train Acc: 44.73% | Val. Loss: 0.701 | Val. Acc: 44.71% |Macro avg f1 0.40658604064305126% |\n",
      "| Epoch: 11 | Train Loss: 0.703 | Train Acc: 46.30% | Val. Loss: 0.700 | Val. Acc: 45.71% |Macro avg f1 0.41297976766357114% |\n",
      "| Epoch: 12 | Train Loss: 0.702 | Train Acc: 47.50% | Val. Loss: 0.699 | Val. Acc: 47.57% |Macro avg f1 0.42547372140507345% |\n",
      "| Epoch: 13 | Train Loss: 0.700 | Train Acc: 48.63% | Val. Loss: 0.698 | Val. Acc: 48.86% |Macro avg f1 0.43364159864037827% |\n",
      "| Epoch: 14 | Train Loss: 0.697 | Train Acc: 49.50% | Val. Loss: 0.696 | Val. Acc: 49.43% |Macro avg f1 0.4365928189457602% |\n",
      "| Epoch: 15 | Train Loss: 0.699 | Train Acc: 49.47% | Val. Loss: 0.695 | Val. Acc: 51.14% |Macro avg f1 0.4487732095490716% |\n",
      "| Epoch: 16 | Train Loss: 0.697 | Train Acc: 50.53% | Val. Loss: 0.694 | Val. Acc: 52.00% |Macro avg f1 0.44852941176470584% |\n",
      "| Epoch: 17 | Train Loss: 0.694 | Train Acc: 51.03% | Val. Loss: 0.693 | Val. Acc: 53.00% |Macro avg f1 0.45545128026879855% |\n",
      "| Epoch: 18 | Train Loss: 0.694 | Train Acc: 51.03% | Val. Loss: 0.692 | Val. Acc: 54.00% |Macro avg f1 0.45960201390553823% |\n",
      "| Epoch: 19 | Train Loss: 0.693 | Train Acc: 52.53% | Val. Loss: 0.690 | Val. Acc: 54.71% |Macro avg f1 0.46306418790575676% |\n",
      "| Epoch: 20 | Train Loss: 0.691 | Train Acc: 52.63% | Val. Loss: 0.689 | Val. Acc: 55.29% |Macro avg f1 0.46400831753409577% |\n",
      "| Epoch: 21 | Train Loss: 0.689 | Train Acc: 53.93% | Val. Loss: 0.688 | Val. Acc: 55.71% |Macro avg f1 0.46690381667387293% |\n",
      "| Epoch: 22 | Train Loss: 0.690 | Train Acc: 52.87% | Val. Loss: 0.687 | Val. Acc: 56.86% |Macro avg f1 0.4699044122809657% |\n",
      "| Epoch: 23 | Train Loss: 0.687 | Train Acc: 55.37% | Val. Loss: 0.686 | Val. Acc: 57.14% |Macro avg f1 0.46853741496598644% |\n",
      "| Epoch: 24 | Train Loss: 0.686 | Train Acc: 55.67% | Val. Loss: 0.684 | Val. Acc: 58.00% |Macro avg f1 0.47420057025477513% |\n",
      "| Epoch: 25 | Train Loss: 0.685 | Train Acc: 56.57% | Val. Loss: 0.683 | Val. Acc: 59.29% |Macro avg f1 0.4792088150760564% |\n",
      "| Epoch: 26 | Train Loss: 0.683 | Train Acc: 56.23% | Val. Loss: 0.682 | Val. Acc: 60.00% |Macro avg f1 0.4820898204225725% |\n",
      "| Epoch: 27 | Train Loss: 0.681 | Train Acc: 58.00% | Val. Loss: 0.681 | Val. Acc: 61.43% |Macro avg f1 0.4914761720263464% |\n",
      "| Epoch: 28 | Train Loss: 0.678 | Train Acc: 59.30% | Val. Loss: 0.680 | Val. Acc: 62.14% |Macro avg f1 0.4961963503431568% |\n",
      "| Epoch: 29 | Train Loss: 0.678 | Train Acc: 59.17% | Val. Loss: 0.679 | Val. Acc: 62.71% |Macro avg f1 0.49804244797032765% |\n",
      "| Epoch: 30 | Train Loss: 0.680 | Train Acc: 57.87% | Val. Loss: 0.677 | Val. Acc: 63.14% |Macro avg f1 0.5008788511922528% |\n",
      "| Epoch: 31 | Train Loss: 0.676 | Train Acc: 61.67% | Val. Loss: 0.676 | Val. Acc: 64.43% |Macro avg f1 0.5074030844536639% |\n",
      "| Epoch: 32 | Train Loss: 0.676 | Train Acc: 61.57% | Val. Loss: 0.675 | Val. Acc: 65.14% |Macro avg f1 0.510073890495204% |\n",
      "| Epoch: 33 | Train Loss: 0.674 | Train Acc: 62.30% | Val. Loss: 0.674 | Val. Acc: 66.14% |Macro avg f1 0.5123901114242969% |\n",
      "| Epoch: 34 | Train Loss: 0.675 | Train Acc: 60.97% | Val. Loss: 0.673 | Val. Acc: 66.71% |Macro avg f1 0.5139337984389855% |\n",
      "| Epoch: 35 | Train Loss: 0.673 | Train Acc: 62.70% | Val. Loss: 0.672 | Val. Acc: 67.43% |Macro avg f1 0.5163636363636365% |\n",
      "| Epoch: 36 | Train Loss: 0.671 | Train Acc: 63.27% | Val. Loss: 0.671 | Val. Acc: 68.43% |Macro avg f1 0.520635599391421% |\n",
      "| Epoch: 37 | Train Loss: 0.670 | Train Acc: 63.37% | Val. Loss: 0.669 | Val. Acc: 70.00% |Macro avg f1 0.5287192705728465% |\n",
      "| Epoch: 38 | Train Loss: 0.668 | Train Acc: 63.50% | Val. Loss: 0.668 | Val. Acc: 70.57% |Macro avg f1 0.5326133461254229% |\n",
      "| Epoch: 39 | Train Loss: 0.667 | Train Acc: 63.77% | Val. Loss: 0.667 | Val. Acc: 71.00% |Macro avg f1 0.5329329901820608% |\n",
      "| Epoch: 40 | Train Loss: 0.666 | Train Acc: 65.03% | Val. Loss: 0.666 | Val. Acc: 71.71% |Macro avg f1 0.5378521126760564% |\n",
      "| Epoch: 41 | Train Loss: 0.665 | Train Acc: 66.43% | Val. Loss: 0.665 | Val. Acc: 72.00% |Macro avg f1 0.534340677989112% |\n",
      "| Epoch: 42 | Train Loss: 0.665 | Train Acc: 65.97% | Val. Loss: 0.664 | Val. Acc: 72.86% |Macro avg f1 0.5402632597754549% |\n",
      "| Epoch: 43 | Train Loss: 0.661 | Train Acc: 66.83% | Val. Loss: 0.663 | Val. Acc: 73.43% |Macro avg f1 0.5413619647461638% |\n",
      "| Epoch: 44 | Train Loss: 0.660 | Train Acc: 68.47% | Val. Loss: 0.662 | Val. Acc: 73.86% |Macro avg f1 0.5413845817864162% |\n",
      "| Epoch: 45 | Train Loss: 0.659 | Train Acc: 67.60% | Val. Loss: 0.660 | Val. Acc: 74.43% |Macro avg f1 0.5391874547186396% |\n",
      "| Test Loss: 0.657 | Test Acc: 77.43%\n",
      "   14 | 03m39s |    0.53919 |    0.0000 |    0.0100 | \n",
      "| Epoch: 01 | Train Loss: 0.581 | Train Acc: 86.83% | Val. Loss: 0.586 | Val. Acc: 84.86% |Macro avg f1 0.5018261897088839% |\n",
      "| Epoch: 02 | Train Loss: 0.578 | Train Acc: 87.37% | Val. Loss: 0.585 | Val. Acc: 84.86% |Macro avg f1 0.5018261897088839% |\n",
      "| Epoch: 03 | Train Loss: 0.576 | Train Acc: 87.13% | Val. Loss: 0.584 | Val. Acc: 84.86% |Macro avg f1 0.5018261897088839% |\n",
      "| Epoch: 04 | Train Loss: 0.575 | Train Acc: 87.33% | Val. Loss: 0.583 | Val. Acc: 84.86% |Macro avg f1 0.5018261897088839% |\n",
      "| Epoch: 05 | Train Loss: 0.574 | Train Acc: 87.40% | Val. Loss: 0.582 | Val. Acc: 84.86% |Macro avg f1 0.5018261897088839% |\n",
      "| Epoch: 06 | Train Loss: 0.574 | Train Acc: 87.57% | Val. Loss: 0.581 | Val. Acc: 84.86% |Macro avg f1 0.5018261897088839% |\n",
      "| Epoch: 07 | Train Loss: 0.574 | Train Acc: 88.00% | Val. Loss: 0.580 | Val. Acc: 84.86% |Macro avg f1 0.5018261897088839% |\n",
      "| Epoch: 08 | Train Loss: 0.573 | Train Acc: 87.80% | Val. Loss: 0.579 | Val. Acc: 84.86% |Macro avg f1 0.5018261897088839% |\n",
      "| Epoch: 09 | Train Loss: 0.572 | Train Acc: 87.73% | Val. Loss: 0.579 | Val. Acc: 84.71% |Macro avg f1 0.493148367450516% |\n",
      "| Epoch: 10 | Train Loss: 0.571 | Train Acc: 87.90% | Val. Loss: 0.578 | Val. Acc: 84.71% |Macro avg f1 0.493148367450516% |\n",
      "| Epoch: 11 | Train Loss: 0.567 | Train Acc: 87.90% | Val. Loss: 0.577 | Val. Acc: 84.57% |Macro avg f1 0.48432512073340434% |\n",
      "| Epoch: 12 | Train Loss: 0.569 | Train Acc: 87.90% | Val. Loss: 0.576 | Val. Acc: 84.57% |Macro avg f1 0.48432512073340434% |\n",
      "| Epoch: 13 | Train Loss: 0.566 | Train Acc: 88.20% | Val. Loss: 0.575 | Val. Acc: 84.57% |Macro avg f1 0.48432512073340434% |\n",
      "| Epoch: 14 | Train Loss: 0.563 | Train Acc: 87.90% | Val. Loss: 0.574 | Val. Acc: 84.57% |Macro avg f1 0.48432512073340434% |\n",
      "| Epoch: 15 | Train Loss: 0.565 | Train Acc: 87.77% | Val. Loss: 0.573 | Val. Acc: 84.57% |Macro avg f1 0.48432512073340434% |\n",
      "| Epoch: 16 | Train Loss: 0.564 | Train Acc: 88.07% | Val. Loss: 0.572 | Val. Acc: 84.57% |Macro avg f1 0.48432512073340434% |\n",
      "| Epoch: 17 | Train Loss: 0.564 | Train Acc: 87.93% | Val. Loss: 0.571 | Val. Acc: 84.57% |Macro avg f1 0.48432512073340434% |\n",
      "| Epoch: 18 | Train Loss: 0.563 | Train Acc: 87.83% | Val. Loss: 0.571 | Val. Acc: 84.71% |Macro avg f1 0.4849791309968301% |\n",
      "| Epoch: 19 | Train Loss: 0.562 | Train Acc: 88.03% | Val. Loss: 0.570 | Val. Acc: 84.71% |Macro avg f1 0.4849791309968301% |\n",
      "| Epoch: 20 | Train Loss: 0.559 | Train Acc: 88.17% | Val. Loss: 0.569 | Val. Acc: 84.71% |Macro avg f1 0.4849791309968301% |\n",
      "| Epoch: 21 | Train Loss: 0.561 | Train Acc: 88.30% | Val. Loss: 0.568 | Val. Acc: 84.71% |Macro avg f1 0.4849791309968301% |\n",
      "| Epoch: 22 | Train Loss: 0.559 | Train Acc: 88.07% | Val. Loss: 0.567 | Val. Acc: 84.71% |Macro avg f1 0.4677335683170005% |\n",
      "| Epoch: 23 | Train Loss: 0.557 | Train Acc: 88.03% | Val. Loss: 0.566 | Val. Acc: 84.57% |Macro avg f1 0.45820433436532504% |\n",
      "| Epoch: 24 | Train Loss: 0.555 | Train Acc: 88.47% | Val. Loss: 0.565 | Val. Acc: 84.57% |Macro avg f1 0.45820433436532504% |\n",
      "| Epoch: 25 | Train Loss: 0.556 | Train Acc: 88.57% | Val. Loss: 0.565 | Val. Acc: 84.57% |Macro avg f1 0.45820433436532504% |\n",
      "| Epoch: 26 | Train Loss: 0.553 | Train Acc: 88.30% | Val. Loss: 0.564 | Val. Acc: 84.57% |Macro avg f1 0.45820433436532504% |\n",
      "| Epoch: 27 | Train Loss: 0.554 | Train Acc: 88.27% | Val. Loss: 0.563 | Val. Acc: 84.57% |Macro avg f1 0.45820433436532504% |\n",
      "| Epoch: 28 | Train Loss: 0.553 | Train Acc: 88.23% | Val. Loss: 0.562 | Val. Acc: 84.57% |Macro avg f1 0.45820433436532504% |\n",
      "| Epoch: 29 | Train Loss: 0.553 | Train Acc: 88.20% | Val. Loss: 0.561 | Val. Acc: 84.57% |Macro avg f1 0.45820433436532504% |\n",
      "| Epoch: 30 | Train Loss: 0.553 | Train Acc: 88.33% | Val. Loss: 0.560 | Val. Acc: 84.57% |Macro avg f1 0.45820433436532504% |\n",
      "| Epoch: 31 | Train Loss: 0.550 | Train Acc: 88.43% | Val. Loss: 0.560 | Val. Acc: 84.71% |Macro avg f1 0.4586233565351895% |\n",
      "| Epoch: 32 | Train Loss: 0.548 | Train Acc: 88.40% | Val. Loss: 0.559 | Val. Acc: 84.71% |Macro avg f1 0.4586233565351895% |\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Thomas\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1143: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| Epoch: 33 | Train Loss: 0.551 | Train Acc: 88.33% | Val. Loss: 0.558 | Val. Acc: 84.86% |Macro avg f1 0.45904173106646057% |\n",
      "| Epoch: 34 | Train Loss: 0.547 | Train Acc: 88.43% | Val. Loss: 0.557 | Val. Acc: 84.86% |Macro avg f1 0.45904173106646057% |\n",
      "| Epoch: 35 | Train Loss: 0.545 | Train Acc: 88.53% | Val. Loss: 0.556 | Val. Acc: 84.86% |Macro avg f1 0.45904173106646057% |\n",
      "| Epoch: 36 | Train Loss: 0.545 | Train Acc: 88.40% | Val. Loss: 0.556 | Val. Acc: 84.86% |Macro avg f1 0.45904173106646057% |\n",
      "| Epoch: 37 | Train Loss: 0.544 | Train Acc: 88.53% | Val. Loss: 0.555 | Val. Acc: 84.86% |Macro avg f1 0.45904173106646057% |\n",
      "| Epoch: 38 | Train Loss: 0.544 | Train Acc: 88.40% | Val. Loss: 0.554 | Val. Acc: 84.86% |Macro avg f1 0.45904173106646057% |\n",
      "| Epoch: 39 | Train Loss: 0.541 | Train Acc: 88.70% | Val. Loss: 0.553 | Val. Acc: 84.86% |Macro avg f1 0.45904173106646057% |\n",
      "| Epoch: 40 | Train Loss: 0.542 | Train Acc: 88.60% | Val. Loss: 0.552 | Val. Acc: 84.86% |Macro avg f1 0.45904173106646057% |\n",
      "| Epoch: 41 | Train Loss: 0.539 | Train Acc: 88.37% | Val. Loss: 0.552 | Val. Acc: 84.86% |Macro avg f1 0.45904173106646057% |\n",
      "| Epoch: 42 | Train Loss: 0.538 | Train Acc: 88.47% | Val. Loss: 0.551 | Val. Acc: 84.86% |Macro avg f1 0.45904173106646057% |\n",
      "| Epoch: 43 | Train Loss: 0.538 | Train Acc: 88.60% | Val. Loss: 0.550 | Val. Acc: 84.86% |Macro avg f1 0.45904173106646057% |\n",
      "| Epoch: 44 | Train Loss: 0.537 | Train Acc: 88.50% | Val. Loss: 0.549 | Val. Acc: 84.86% |Macro avg f1 0.45904173106646057% |\n",
      "| Epoch: 45 | Train Loss: 0.537 | Train Acc: 88.73% | Val. Loss: 0.549 | Val. Acc: 84.86% |Macro avg f1 0.45904173106646057% |\n",
      "| Test Loss: 0.537 | Test Acc: 88.00%\n",
      "   15 | 03m31s |    0.45904 |    0.0000 |    0.0000 | \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Thomas\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1143: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| Epoch: 01 | Train Loss: 0.805 | Train Acc: 15.97% | Val. Loss: 1.268 | Val. Acc: 84.86% |Macro avg f1 0.45904173106646057% |\n",
      "| Epoch: 02 | Train Loss: 0.934 | Train Acc: 88.83% | Val. Loss: 1.248 | Val. Acc: 84.86% |Macro avg f1 0.45904173106646057% |\n",
      "| Epoch: 03 | Train Loss: 0.917 | Train Acc: 88.83% | Val. Loss: 0.847 | Val. Acc: 84.86% |Macro avg f1 0.45904173106646057% |\n",
      "| Epoch: 04 | Train Loss: 0.623 | Train Acc: 88.83% | Val. Loss: 0.514 | Val. Acc: 84.86% |Macro avg f1 0.45904173106646057% |\n",
      "| Epoch: 05 | Train Loss: 0.391 | Train Acc: 88.83% | Val. Loss: 0.413 | Val. Acc: 84.86% |Macro avg f1 0.45904173106646057% |\n",
      "| Epoch: 06 | Train Loss: 0.344 | Train Acc: 88.83% | Val. Loss: 0.442 | Val. Acc: 84.86% |Macro avg f1 0.45904173106646057% |\n",
      "| Epoch: 07 | Train Loss: 0.396 | Train Acc: 88.83% | Val. Loss: 0.445 | Val. Acc: 84.86% |Macro avg f1 0.45904173106646057% |\n",
      "| Epoch: 08 | Train Loss: 0.398 | Train Acc: 88.83% | Val. Loss: 0.418 | Val. Acc: 84.86% |Macro avg f1 0.45904173106646057% |\n",
      "| Epoch: 09 | Train Loss: 0.344 | Train Acc: 88.83% | Val. Loss: 0.478 | Val. Acc: 84.86% |Macro avg f1 0.45904173106646057% |\n",
      "| Epoch: 10 | Train Loss: 0.364 | Train Acc: 88.83% | Val. Loss: 0.457 | Val. Acc: 84.86% |Macro avg f1 0.45904173106646057% |\n",
      "| Epoch: 11 | Train Loss: 0.349 | Train Acc: 88.83% | Val. Loss: 0.414 | Val. Acc: 84.86% |Macro avg f1 0.45904173106646057% |\n",
      "| Epoch: 12 | Train Loss: 0.332 | Train Acc: 88.83% | Val. Loss: 0.406 | Val. Acc: 84.86% |Macro avg f1 0.45904173106646057% |\n",
      "| Epoch: 13 | Train Loss: 0.342 | Train Acc: 88.83% | Val. Loss: 0.407 | Val. Acc: 84.86% |Macro avg f1 0.45904173106646057% |\n",
      "| Epoch: 14 | Train Loss: 0.348 | Train Acc: 88.83% | Val. Loss: 0.405 | Val. Acc: 84.86% |Macro avg f1 0.45904173106646057% |\n",
      "| Epoch: 15 | Train Loss: 0.337 | Train Acc: 88.83% | Val. Loss: 0.418 | Val. Acc: 84.86% |Macro avg f1 0.45904173106646057% |\n",
      "| Epoch: 16 | Train Loss: 0.334 | Train Acc: 88.83% | Val. Loss: 0.429 | Val. Acc: 84.86% |Macro avg f1 0.45904173106646057% |\n",
      "| Epoch: 17 | Train Loss: 0.337 | Train Acc: 88.83% | Val. Loss: 0.418 | Val. Acc: 84.86% |Macro avg f1 0.45904173106646057% |\n",
      "| Epoch: 18 | Train Loss: 0.333 | Train Acc: 88.83% | Val. Loss: 0.407 | Val. Acc: 84.86% |Macro avg f1 0.45904173106646057% |\n",
      "| Epoch: 19 | Train Loss: 0.333 | Train Acc: 88.83% | Val. Loss: 0.405 | Val. Acc: 84.86% |Macro avg f1 0.45904173106646057% |\n",
      "| Epoch: 20 | Train Loss: 0.335 | Train Acc: 88.83% | Val. Loss: 0.405 | Val. Acc: 84.86% |Macro avg f1 0.45904173106646057% |\n",
      "| Epoch: 21 | Train Loss: 0.332 | Train Acc: 88.83% | Val. Loss: 0.412 | Val. Acc: 84.86% |Macro avg f1 0.45904173106646057% |\n",
      "| Epoch: 22 | Train Loss: 0.331 | Train Acc: 88.83% | Val. Loss: 0.418 | Val. Acc: 84.86% |Macro avg f1 0.45904173106646057% |\n",
      "| Epoch: 23 | Train Loss: 0.333 | Train Acc: 88.83% | Val. Loss: 0.414 | Val. Acc: 84.86% |Macro avg f1 0.45904173106646057% |\n",
      "| Epoch: 24 | Train Loss: 0.331 | Train Acc: 88.83% | Val. Loss: 0.406 | Val. Acc: 84.86% |Macro avg f1 0.45904173106646057% |\n",
      "| Epoch: 25 | Train Loss: 0.328 | Train Acc: 88.83% | Val. Loss: 0.403 | Val. Acc: 84.86% |Macro avg f1 0.45904173106646057% |\n",
      "| Epoch: 26 | Train Loss: 0.329 | Train Acc: 88.83% | Val. Loss: 0.404 | Val. Acc: 84.86% |Macro avg f1 0.45904173106646057% |\n",
      "| Epoch: 27 | Train Loss: 0.329 | Train Acc: 88.83% | Val. Loss: 0.409 | Val. Acc: 84.86% |Macro avg f1 0.45904173106646057% |\n",
      "| Epoch: 28 | Train Loss: 0.327 | Train Acc: 88.83% | Val. Loss: 0.412 | Val. Acc: 84.86% |Macro avg f1 0.45904173106646057% |\n",
      "| Epoch: 29 | Train Loss: 0.327 | Train Acc: 88.83% | Val. Loss: 0.409 | Val. Acc: 84.86% |Macro avg f1 0.45904173106646057% |\n",
      "| Epoch: 30 | Train Loss: 0.326 | Train Acc: 88.83% | Val. Loss: 0.405 | Val. Acc: 84.86% |Macro avg f1 0.45904173106646057% |\n",
      "| Epoch: 31 | Train Loss: 0.327 | Train Acc: 88.83% | Val. Loss: 0.404 | Val. Acc: 84.86% |Macro avg f1 0.45904173106646057% |\n",
      "| Epoch: 32 | Train Loss: 0.327 | Train Acc: 88.83% | Val. Loss: 0.405 | Val. Acc: 84.86% |Macro avg f1 0.45904173106646057% |\n",
      "| Epoch: 33 | Train Loss: 0.325 | Train Acc: 88.83% | Val. Loss: 0.408 | Val. Acc: 84.86% |Macro avg f1 0.45904173106646057% |\n",
      "| Epoch: 34 | Train Loss: 0.326 | Train Acc: 88.83% | Val. Loss: 0.410 | Val. Acc: 84.86% |Macro avg f1 0.45904173106646057% |\n",
      "| Epoch: 35 | Train Loss: 0.326 | Train Acc: 88.83% | Val. Loss: 0.408 | Val. Acc: 84.86% |Macro avg f1 0.45904173106646057% |\n",
      "| Epoch: 36 | Train Loss: 0.325 | Train Acc: 88.83% | Val. Loss: 0.405 | Val. Acc: 84.86% |Macro avg f1 0.45904173106646057% |\n",
      "| Epoch: 37 | Train Loss: 0.325 | Train Acc: 88.83% | Val. Loss: 0.404 | Val. Acc: 84.86% |Macro avg f1 0.45904173106646057% |\n",
      "| Epoch: 38 | Train Loss: 0.326 | Train Acc: 88.83% | Val. Loss: 0.405 | Val. Acc: 84.86% |Macro avg f1 0.45904173106646057% |\n",
      "| Epoch: 39 | Train Loss: 0.326 | Train Acc: 88.83% | Val. Loss: 0.407 | Val. Acc: 84.86% |Macro avg f1 0.45904173106646057% |\n",
      "| Epoch: 40 | Train Loss: 0.326 | Train Acc: 88.83% | Val. Loss: 0.408 | Val. Acc: 84.86% |Macro avg f1 0.45904173106646057% |\n",
      "| Epoch: 41 | Train Loss: 0.326 | Train Acc: 88.83% | Val. Loss: 0.407 | Val. Acc: 84.86% |Macro avg f1 0.45904173106646057% |\n",
      "| Epoch: 42 | Train Loss: 0.326 | Train Acc: 88.83% | Val. Loss: 0.406 | Val. Acc: 84.86% |Macro avg f1 0.45904173106646057% |\n",
      "| Epoch: 43 | Train Loss: 0.326 | Train Acc: 88.83% | Val. Loss: 0.406 | Val. Acc: 84.86% |Macro avg f1 0.45904173106646057% |\n",
      "| Epoch: 44 | Train Loss: 0.326 | Train Acc: 88.83% | Val. Loss: 0.406 | Val. Acc: 84.86% |Macro avg f1 0.45904173106646057% |\n",
      "| Epoch: 45 | Train Loss: 0.327 | Train Acc: 88.83% | Val. Loss: 0.408 | Val. Acc: 84.86% |Macro avg f1 0.45904173106646057% |\n",
      "| Test Loss: 0.355 | Test Acc: 88.14%\n",
      "   16 | 03m21s |    0.45904 |    0.0272 |    0.0100 | \n"
     ]
    }
   ],
   "source": [
    "# Bounded region of parameter space\n",
    "pbounds = {'lr': (0.00001, 0.05), 'wd': (0.000001, 0.01)}\n",
    "\n",
    "optimizer = BayesianOptimization(\n",
    "    f=f,\n",
    "    pbounds=pbounds,\n",
    "    random_state=1,\n",
    ")\n",
    "\n",
    "optimizer.maximize(\n",
    "    init_points=8,\n",
    "    n_iter=8,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
