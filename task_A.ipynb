{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task A: NLP Cousework"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# built-in\n",
    "import re\n",
    "import codecs\n",
    "import random\n",
    "\n",
    "# common libs\n",
    "import pandas as pd\n",
    "import numpy as np \n",
    "from tqdm import tqdm\n",
    "\n",
    "# plotting libs\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "sns.set_style(style='dark')\n",
    "\n",
    "# nltk\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "\n",
    "\n",
    "# torch\n",
    "import torch\n",
    "from torch.autograd import Variable\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "\n",
    "#sklearn\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import RandomizedSearchCV, GridSearchCV\n",
    "\n",
    "\n",
    "#bayes_opt\n",
    "from bayes_opt import BayesianOptimization\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PART I"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>tweet</th>\n",
       "      <th>subtask_a</th>\n",
       "      <th>subtask_b</th>\n",
       "      <th>subtask_c</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>86426</td>\n",
       "      <td>@USER She should ask a few native Americans wh...</td>\n",
       "      <td>OFF</td>\n",
       "      <td>UNT</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>90194</td>\n",
       "      <td>@USER @USER Go home youâ€™re drunk!!! @USER #MAG...</td>\n",
       "      <td>OFF</td>\n",
       "      <td>TIN</td>\n",
       "      <td>IND</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>16820</td>\n",
       "      <td>Amazon is investigating Chinese employees who ...</td>\n",
       "      <td>NOT</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>62688</td>\n",
       "      <td>@USER Someone should'veTaken\" this piece of sh...</td>\n",
       "      <td>OFF</td>\n",
       "      <td>UNT</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>43605</td>\n",
       "      <td>@USER @USER Obama wanted liberals &amp;amp; illega...</td>\n",
       "      <td>NOT</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      id                                              tweet subtask_a  \\\n",
       "0  86426  @USER She should ask a few native Americans wh...       OFF   \n",
       "1  90194  @USER @USER Go home youâ€™re drunk!!! @USER #MAG...       OFF   \n",
       "2  16820  Amazon is investigating Chinese employees who ...       NOT   \n",
       "3  62688  @USER Someone should'veTaken\" this piece of sh...       OFF   \n",
       "4  43605  @USER @USER Obama wanted liberals &amp; illega...       NOT   \n",
       "\n",
       "  subtask_b subtask_c  \n",
       "0       UNT       NaN  \n",
       "1       TIN       IND  \n",
       "2       NaN       NaN  \n",
       "3       UNT       NaN  \n",
       "4       NaN       NaN  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.read_csv(\"training-v1/offenseval-training-v1.tsv\", delimiter='\\t', engine='c')\n",
    "data_test = pd.read_csv(\"Test_A_Release/testset-taska.tsv\", delimiter='\\t', engine='c')\n",
    "data.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets = np.array(data.tweet)\n",
    "tweets_test = np.array(data_test.tweet)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['@USER She should ask a few native Americans what their take on this is.',\n",
       "       '@USER @USER Go home youâ€™re drunk!!! @USER #MAGA #Trump2020 ðŸ‘ŠðŸ‡ºðŸ‡¸ðŸ‘Š URL',\n",
       "       'Amazon is investigating Chinese employees who are selling internal data to third-party sellers looking for an edge in the competitive marketplace. URL #Amazon #MAGA #KAG #CHINA #TCOT',\n",
       "       '@USER Someone should\\'veTaken\" this piece of shit to a volcano. ðŸ˜‚\"',\n",
       "       '@USER @USER Obama wanted liberals &amp; illegals to move into red states'],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweets[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['go', 'home', 'you', 're', 'drunk', 'maga', 'trump']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "regexp1 = r'@[A-Za-z0-9]+'\n",
    "regexp2 = r'https?://[A-Za-z0-9./]+'\n",
    "\n",
    "combined_regexp = r'|'.join((regexp1, regexp2)) #getting rid of @USER and potentiel URLs\n",
    "\n",
    "def low_stemmed_token_sentence(string, regexp = combined_regexp, SW = False):\n",
    "# this tokenizer just accepts alphabetic word (remove numeric)\n",
    "        \n",
    "    cleaned = re.sub(regexp, '', string)\n",
    "    tokenizer = RegexpTokenizer('[a-z]+') #splits the string into substrings we strip the # \n",
    "    stemmer = PorterStemmer() #basically it is suffix stripping\n",
    "    \n",
    "    low = cleaned.lower().replace('url', '')\n",
    "    tokens = tokenizer.tokenize(low)\n",
    "    if SW == True:\n",
    "        stopWords = set(stopwords.words('english')) #creates a set of words that will be ignored\n",
    "        filtered_tokens = []\n",
    "        for tok in tokens:\n",
    "            if tok not in stopWords:\n",
    "                filtered_tokens.append(tok)\n",
    "        tokens = filtered_tokens\n",
    "        \n",
    "    stemmed_tokens = [stemmer.stem(i) for i in tokens]\n",
    "    return ((stemmed_tokens))\n",
    "\n",
    "low_stemmed_token_sentence(tweets[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "cleaned_tweets = [low_stemmed_token_sentence(x, SW=True) for x in tweets]\n",
    "cleaned_tweets_test_2 = [low_stemmed_token_sentence(x, SW=True) for x in tweets_test]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['ask', 'nativ', 'american', 'take'],\n",
       " ['go', 'home', 'drunk', 'maga', 'trump']]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cleaned_tweets[:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['@USER She should ask a few native Americans what their take on this is.',\n",
       "       '@USER @USER Go home youâ€™re drunk!!! @USER #MAGA #Trump2020 ðŸ‘ŠðŸ‡ºðŸ‡¸ðŸ‘Š URL',\n",
       "       'Amazon is investigating Chinese employees who are selling internal data to third-party sellers looking for an edge in the competitive marketplace. URL #Amazon #MAGA #KAG #CHINA #TCOT',\n",
       "       '@USER Someone should\\'veTaken\" this piece of shit to a volcano. ðŸ˜‚\"',\n",
       "       '@USER @USER Obama wanted liberals &amp; illegals to move into red states'],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweets[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing for PyTorch to have tensors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of labeled tweets in the trainining set :13240\n",
      "Number of unlabeled tweets in the test set 860\n",
      "which makes a total of 14100 tweets\n"
     ]
    }
   ],
   "source": [
    "nb_train = len(cleaned_tweets)\n",
    "print(f\"Number of labeled tweets in the trainining set :{nb_train}\")\n",
    "nb_test=len(cleaned_tweets_test_2)\n",
    "print(f\"Number of unlabeled tweets in the test set {nb_test}\")\n",
    "\n",
    "\n",
    "all_cleaned_tweets = np.concatenate([cleaned_tweets, cleaned_tweets_test_2])\n",
    "print(f\"which makes a total of {all_cleaned_tweets.shape[0]} tweets\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_word2idx(tokenized_corpus):\n",
    "    vocabulary = []\n",
    "    for sentence in tokenized_corpus:\n",
    "        for token in sentence:\n",
    "            if token not in vocabulary:\n",
    "                vocabulary.append(token)\n",
    "    word2idx = {w: idx+1 for (idx, w) in enumerate(vocabulary)}\n",
    "    word2idx['<pad>'] = 0     # we reserve the 0 index for the placeholder token\n",
    "    return word2idx\n",
    "\n",
    "def get_model_inputs(tokenized_corpus, word2idx, labels, max_len):\n",
    "    # we index our sentences\n",
    "    vectorized_sents = [[word2idx[tok] for tok in sent if tok in word2idx] for sent in tokenized_corpus]\n",
    "  \n",
    "    # we create a tensor of a fixed size filled with zeroes for padding\n",
    "\n",
    "    sent_tensor = Variable(torch.zeros((len(vectorized_sents), max_len))).long()\n",
    "    sent_lengths = [len(sent) for sent in vectorized_sents]\n",
    "  \n",
    "    # we fill it with our vectorized sentences \n",
    "  \n",
    "    for idx, (sent, sentlen) in enumerate(zip(vectorized_sents, sent_lengths)):\n",
    "\n",
    "        sent_tensor[idx, :sentlen] = torch.LongTensor(sent)\n",
    "        label_tensor = torch.FloatTensor(labels)\n",
    "  \n",
    "    return sent_tensor, label_tensor\n",
    "\n",
    "cleaned_tweets_train, cleaned_tweets_val, cleaned_tweets_test, cleaned_tweets_test_2 \\\n",
    "= all_cleaned_tweets[:nb_train-3000], all_cleaned_tweets[nb_train-3000:nb_train-1500], \\\n",
    "    all_cleaned_tweets[nb_train-1500:nb_train] , all_cleaned_tweets[nb_train:]\n",
    "\n",
    "word2idx = get_word2idx(all_cleaned_tweets)\n",
    "\n",
    "sent_lengths = [len(sent) for sent in all_cleaned_tweets]\n",
    "max_len = np.max(np.array(sent_lengths))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# label words to integers\n",
    "labels = np.array(data.subtask_a)\n",
    "labels[labels == 'OFF'] = 1\n",
    "labels[labels == 'NOT'] = 0\n",
    "labels = list(labels)\n",
    "labels_train, labels_val, labels_test = labels[:-3000], labels[-3000:-1500], labels[-1500:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_sent_tensor, train_label_tensor = get_model_inputs(cleaned_tweets_train, word2idx, labels_train, max_len)\n",
    "valid_sent_tensor, valid_label_tensor = get_model_inputs(cleaned_tweets_val, word2idx, labels_val, max_len)\n",
    "test_sent_tensor, test_label_tensor = get_model_inputs(cleaned_tweets_test, word2idx, labels_test, max_len)\n",
    "test_sent_tensor_2, _ = get_model_inputs(cleaned_tweets_test_2, word2idx, labels_test[:840], max_len)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PART II"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Traditional Machine Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are using BoW and Random Forest here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_valid_test = list(cleaned_tweets_train) + list(cleaned_tweets_val)+ list(cleaned_tweets_test) + \\\n",
    "        list(cleaned_tweets_test_2)\n",
    "    \n",
    "train_valid_test = [' '.join(s) for s in train_valid_test]\n",
    "train_list = [' '.join(s) for s in list(cleaned_tweets_train)]\n",
    "valid_list =[' '.join(s) for s in list(cleaned_tweets_val)]\n",
    "test_list =[' '.join(s) for s in list(cleaned_tweets_test)]\n",
    "aside_list = [' '.join(s) for s in list(cleaned_tweets_test_2)]\n",
    "\n",
    "\n",
    "vectorizer = CountVectorizer()\n",
    "#11687 words in the voc\n",
    "X = vectorizer.fit(train_valid_test)\n",
    "\n",
    "BOW_train = vectorizer.transform(train_list)\n",
    "BOW_valid = vectorizer.transform(valid_list)\n",
    "BOW_test = vectorizer.transform(test_list)\n",
    "BOW_aside = vectorizer.transform(aside_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.772"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf = RandomForestClassifier(n_estimators=100)\n",
    "clf.fit(BOW_train, labels_train)\n",
    "clf.score(BOW_valid, labels_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.91      0.78      0.84      1171\n",
      "           1       0.49      0.74      0.59       329\n",
      "\n",
      "   micro avg       0.77      0.77      0.77      1500\n",
      "   macro avg       0.70      0.76      0.71      1500\n",
      "weighted avg       0.82      0.77      0.79      1500\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(clf.predict(BOW_valid), labels_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 4 folds for each of 8 candidates, totalling 32 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=5)]: Using backend LokyBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  32 out of  32 | elapsed:  1.7min finished\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=4, error_score='raise-deprecating',\n",
       "       estimator=RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
       "            max_depth=None, max_features='auto', max_leaf_nodes=None,\n",
       "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "            min_samples_leaf=1, min_samples_split=2,\n",
       "            min_weight_fraction_leaf=0.0, n_estimators=100, n_jobs=None,\n",
       "            oob_score=False, random_state=None, verbose=0,\n",
       "            warm_start=False),\n",
       "       fit_params=None, iid=False, n_jobs=5,\n",
       "       param_grid={'max_depth': [10, 25, 50, 100], 'bootstrap': [1, 0]},\n",
       "       pre_dispatch='2*n_jobs', refit=True, return_train_score='warn',\n",
       "       scoring=None, verbose=True)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "parameters = {\n",
    "    'max_depth' : [ 10,25,50,100],\n",
    "    'bootstrap' : [1, 0]\n",
    "}\n",
    "\n",
    "grid = GridSearchCV(RandomForestClassifier(n_estimators=100),\n",
    "                        parameters,\n",
    "                        cv = 4,\n",
    "                        n_jobs = 5,\n",
    "                        verbose=True,\n",
    "                        iid = False)\n",
    "\n",
    "grid.fit(BOW_train, labels_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.95      0.76      0.84      1256\n",
      "           1       0.39      0.80      0.53       244\n",
      "\n",
      "   micro avg       0.77      0.77      0.77      1500\n",
      "   macro avg       0.67      0.78      0.69      1500\n",
      "weighted avg       0.86      0.77      0.79      1500\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(grid.best_estimator_.predict(BOW_test), labels_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweet</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>id</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>15923</th>\n",
       "      <td>NOT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27014</th>\n",
       "      <td>OFF</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30530</th>\n",
       "      <td>NOT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13876</th>\n",
       "      <td>NOT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>60133</th>\n",
       "      <td>NOT</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      tweet\n",
       "id         \n",
       "15923   NOT\n",
       "27014   OFF\n",
       "30530   NOT\n",
       "13876   NOT\n",
       "60133   NOT"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#to submit\n",
    "p= grid.best_estimator_.predict(BOW_aside)\n",
    "p = ['OFF' if x == 1 else 'NOT' for x in p]\n",
    "data_test.tweet = p\n",
    "data_test.index = data_test.id\n",
    "data_test = data_test.drop(columns=['id'])\n",
    "data_test.to_csv('test_A_ml.csv',sep=',', header = False)\n",
    "data_test.head(5)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deep Learning approach with CNNs "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 400000/400000 [00:06<00:00, 62530.13it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.         0.         0.        ...  0.         0.         0.       ]\n",
      " [-0.1683    -0.0019777  0.58425   ...  0.21466    0.091399   0.26541  ]\n",
      " [ 0.30152   -0.67398   -0.201     ...  0.071335  -0.15308    0.10143  ]\n",
      " ...\n",
      " [ 0.         0.         0.        ...  0.         0.         0.       ]\n",
      " [ 0.         0.         0.        ...  0.         0.         0.       ]\n",
      " [ 0.         0.         0.        ...  0.         0.         0.       ]]\n"
     ]
    }
   ],
   "source": [
    "wvecs = np.zeros((len(word2idx), 100))\n",
    "\n",
    "wvecs = np.zeros((len(word2idx), 100))\n",
    "\n",
    "with codecs.open('glove/glove.6B/glove.6B.100d.txt', 'r','utf-8') as f: \n",
    "  index = 0\n",
    "  for line in tqdm(f.readlines()):\n",
    "    if len(line.strip().split()) > 3:\n",
    "      word = line.strip().split()[0]\n",
    "      if word in word2idx:\n",
    "          (word, vec) = (word, list(map(float,line.strip().split()[1:])))\n",
    "          idx = word2idx[word]\n",
    "          wvecs[idx] = vec\n",
    "          \n",
    "print(wvecs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNN(nn.Module):\n",
    "    \n",
    "    def __init__(self, vocab_size, embedding_dim, out_channels, window_size, output_dim, dropout, embedding_matrix = wvecs ,non_trainable = False):\n",
    "        \n",
    "        super(CNN, self).__init__()\n",
    "        \n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
    "        self.embedding.weight.data.copy_(torch.from_numpy(embedding_matrix))\n",
    "        if non_trainable:\n",
    "            self.embedding.weight.requires_grad = False\n",
    "        self.conv = nn.Conv2d(in_channels=1, out_channels=out_channels, kernel_size=(window_size,embedding_dim))\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.fc = nn.Linear(out_channels, output_dim)\n",
    "        \n",
    "        \n",
    "        \n",
    "    def forward(self, x):\n",
    "\n",
    "        embedded = self.embedding(x)\n",
    "        embedded = embedded.unsqueeze(1)\n",
    "        \n",
    "        feature_maps = self.conv(embedded)\n",
    "        feature_maps = feature_maps.squeeze(3)    \n",
    "        feature_maps = F.relu(feature_maps)\n",
    "\n",
    "        pooled = F.max_pool1d(feature_maps, feature_maps.shape[2])\n",
    "        pooled = pooled.squeeze(2)\n",
    "  \n",
    "        dropped = self.dropout(pooled)\n",
    "        preds = self.fc(dropped)\n",
    "        \n",
    "        return preds\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy(output, target):\n",
    " \n",
    "    output = torch.round(torch.sigmoid(output))\n",
    "    correct = (output == target).float()\n",
    "    acc = correct.sum()/len(correct)\n",
    "    return acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| Epoch: 01 | Train Loss: 0.694 | Train Acc: 50.53% | Val. Loss: 0.654 | Val. Acc: 66.47% |\n",
      "| Epoch: 02 | Train Loss: 0.651 | Train Acc: 66.58% | Val. Loss: 0.657 | Val. Acc: 66.73% |\n",
      "| Epoch: 03 | Train Loss: 0.651 | Train Acc: 66.75% | Val. Loss: 0.630 | Val. Acc: 67.73% |\n",
      "| Epoch: 04 | Train Loss: 0.625 | Train Acc: 67.36% | Val. Loss: 0.609 | Val. Acc: 68.27% |\n",
      "| Epoch: 05 | Train Loss: 0.605 | Train Acc: 69.10% | Val. Loss: 0.603 | Val. Acc: 71.47% |\n",
      "| Epoch: 06 | Train Loss: 0.598 | Train Acc: 72.52% | Val. Loss: 0.599 | Val. Acc: 72.53% |\n",
      "| Epoch: 07 | Train Loss: 0.595 | Train Acc: 73.10% | Val. Loss: 0.589 | Val. Acc: 72.67% |\n",
      "| Epoch: 08 | Train Loss: 0.585 | Train Acc: 73.58% | Val. Loss: 0.573 | Val. Acc: 73.47% |\n",
      "| Epoch: 09 | Train Loss: 0.569 | Train Acc: 73.89% | Val. Loss: 0.560 | Val. Acc: 72.47% |\n",
      "| Epoch: 10 | Train Loss: 0.555 | Train Acc: 74.44% | Val. Loss: 0.553 | Val. Acc: 73.13% |\n",
      "| Epoch: 11 | Train Loss: 0.547 | Train Acc: 74.10% | Val. Loss: 0.548 | Val. Acc: 73.47% |\n",
      "| Epoch: 12 | Train Loss: 0.542 | Train Acc: 74.50% | Val. Loss: 0.540 | Val. Acc: 73.73% |\n",
      "| Epoch: 13 | Train Loss: 0.533 | Train Acc: 74.89% | Val. Loss: 0.530 | Val. Acc: 74.53% |\n",
      "| Epoch: 14 | Train Loss: 0.524 | Train Acc: 75.59% | Val. Loss: 0.523 | Val. Acc: 75.13% |\n",
      "| Epoch: 15 | Train Loss: 0.516 | Train Acc: 75.77% | Val. Loss: 0.520 | Val. Acc: 75.80% |\n",
      "| Epoch: 16 | Train Loss: 0.514 | Train Acc: 75.67% | Val. Loss: 0.516 | Val. Acc: 76.47% |\n",
      "| Epoch: 17 | Train Loss: 0.510 | Train Acc: 76.02% | Val. Loss: 0.511 | Val. Acc: 76.67% |\n",
      "| Epoch: 18 | Train Loss: 0.505 | Train Acc: 76.27% | Val. Loss: 0.508 | Val. Acc: 76.27% |\n",
      "| Epoch: 19 | Train Loss: 0.498 | Train Acc: 76.73% | Val. Loss: 0.507 | Val. Acc: 76.13% |\n",
      "| Epoch: 20 | Train Loss: 0.494 | Train Acc: 76.97% | Val. Loss: 0.505 | Val. Acc: 75.80% |\n",
      "| Epoch: 21 | Train Loss: 0.491 | Train Acc: 76.72% | Val. Loss: 0.500 | Val. Acc: 76.47% |\n",
      "| Epoch: 22 | Train Loss: 0.486 | Train Acc: 77.02% | Val. Loss: 0.495 | Val. Acc: 77.07% |\n",
      "| Epoch: 23 | Train Loss: 0.482 | Train Acc: 77.32% | Val. Loss: 0.493 | Val. Acc: 77.00% |\n",
      "| Epoch: 24 | Train Loss: 0.478 | Train Acc: 77.97% | Val. Loss: 0.492 | Val. Acc: 77.13% |\n",
      "| Epoch: 25 | Train Loss: 0.474 | Train Acc: 78.21% | Val. Loss: 0.489 | Val. Acc: 77.20% |\n",
      "| Epoch: 26 | Train Loss: 0.472 | Train Acc: 77.94% | Val. Loss: 0.488 | Val. Acc: 76.93% |\n",
      "| Epoch: 27 | Train Loss: 0.468 | Train Acc: 78.33% | Val. Loss: 0.488 | Val. Acc: 76.93% |\n",
      "| Epoch: 28 | Train Loss: 0.468 | Train Acc: 78.44% | Val. Loss: 0.487 | Val. Acc: 77.40% |\n",
      "| Epoch: 29 | Train Loss: 0.464 | Train Acc: 78.54% | Val. Loss: 0.485 | Val. Acc: 77.60% |\n",
      "| Epoch: 30 | Train Loss: 0.462 | Train Acc: 78.62% | Val. Loss: 0.485 | Val. Acc: 77.53% |\n",
      "| Epoch: 31 | Train Loss: 0.458 | Train Acc: 79.22% | Val. Loss: 0.485 | Val. Acc: 77.60% |\n",
      "| Epoch: 32 | Train Loss: 0.455 | Train Acc: 79.17% | Val. Loss: 0.484 | Val. Acc: 77.67% |\n",
      "| Epoch: 33 | Train Loss: 0.454 | Train Acc: 79.30% | Val. Loss: 0.484 | Val. Acc: 77.53% |\n",
      "| Epoch: 34 | Train Loss: 0.449 | Train Acc: 79.44% | Val. Loss: 0.485 | Val. Acc: 77.07% |\n",
      "| Epoch: 35 | Train Loss: 0.451 | Train Acc: 79.07% | Val. Loss: 0.485 | Val. Acc: 77.13% |\n",
      "| Epoch: 36 | Train Loss: 0.448 | Train Acc: 79.58% | Val. Loss: 0.485 | Val. Acc: 77.13% |\n",
      "| Epoch: 37 | Train Loss: 0.445 | Train Acc: 79.72% | Val. Loss: 0.484 | Val. Acc: 77.27% |\n",
      "| Epoch: 38 | Train Loss: 0.442 | Train Acc: 79.53% | Val. Loss: 0.484 | Val. Acc: 77.60% |\n",
      "| Epoch: 39 | Train Loss: 0.440 | Train Acc: 79.85% | Val. Loss: 0.483 | Val. Acc: 77.87% |\n",
      "| Epoch: 40 | Train Loss: 0.439 | Train Acc: 80.22% | Val. Loss: 0.483 | Val. Acc: 77.73% |\n",
      "| Test Loss: 0.470 | Test Acc: 78.20%\n"
     ]
    }
   ],
   "source": [
    "epochs=40\n",
    "\n",
    "INPUT_DIM = len(word2idx)\n",
    "EMBEDDING_DIM = 100\n",
    "OUTPUT_DIM = 1\n",
    "\n",
    "#the hyperparamerts specific to CNN\n",
    "\n",
    "# we define the number of filters\n",
    "N_OUT_CHANNELS = 100\n",
    "# we define the window size\n",
    "WINDOW_SIZE = 1\n",
    "# we apply the dropout with the probability 0.5\n",
    "DROPOUT = 0.2\n",
    "\n",
    "model = CNN(INPUT_DIM, EMBEDDING_DIM, N_OUT_CHANNELS, WINDOW_SIZE, OUTPUT_DIM, DROPOUT)\n",
    "\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.008, weight_decay=0.001)\n",
    "loss_fn = nn.BCEWithLogitsLoss()\n",
    "\n",
    "feature_train = train_sent_tensor\n",
    "target_train = train_label_tensor\n",
    "\n",
    "feature_valid = valid_sent_tensor\n",
    "target_valid = valid_label_tensor\n",
    "\n",
    "train_acc, val_acc = [], []\n",
    "train_loss, val_loss = [], []\n",
    "for epoch in range(1, epochs+1):\n",
    "   \n",
    "    model.train()\n",
    "  \n",
    "    optimizer.zero_grad()\n",
    "    \n",
    "    predictions = model(feature_train).squeeze(1)\n",
    "    loss = loss_fn(predictions, target_train)\n",
    "    acc = accuracy(predictions, target_train)\n",
    "    train_acc.append(acc)\n",
    "    train_loss.append(loss)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "  \n",
    "    epoch_loss = loss.item()\n",
    "    epoch_acc = acc\n",
    "  \n",
    "    model.eval()\n",
    "  \n",
    "    with torch.no_grad():\n",
    " \n",
    "        predictions_valid = model(feature_valid).squeeze(1)\n",
    "        loss = loss_fn(predictions_valid, target_valid)\n",
    "        acc = accuracy(predictions_valid, target_valid)\n",
    "        val_acc.append(acc)\n",
    "        val_loss.append(loss)\n",
    "        valid_loss = loss.item()\n",
    "        valid_acc = acc\n",
    "  \n",
    "    print(f'| Epoch: {epoch:02} | Train Loss: {epoch_loss:.3f} | Train Acc: {epoch_acc*100:.2f}% | Val. Loss: {valid_loss:.3f} | Val. Acc: {valid_acc*100:.2f}% |')\n",
    "    model.eval()\n",
    "    \n",
    "    feature = test_sent_tensor\n",
    "    target = test_label_tensor\n",
    "\n",
    "with torch.no_grad():\n",
    " \n",
    "    predictions = model(feature).squeeze(1)\n",
    "    loss = loss_fn(predictions, target)\n",
    "    acc = accuracy(predictions, target)\n",
    "    print(f'| Test Loss: {loss:.3f} | Test Acc: {acc*100:.2f}%')\n",
    "#    f_measure(predictions, test_labels)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tests on holdout test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.81      0.89      0.84      1002\n",
      "         1.0       0.71      0.57      0.64       498\n",
      "\n",
      "   micro avg       0.78      0.78      0.78      1500\n",
      "   macro avg       0.76      0.73      0.74      1500\n",
      "weighted avg       0.78      0.78      0.78      1500\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(target, torch.round(torch.sigmoid(predictions))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAbgAAAEPCAYAAAAwBdF+AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDMuMC4yLCBo\ndHRwOi8vbWF0cGxvdGxpYi5vcmcvOIA7rQAAIABJREFUeJzs3Xd4VFX6wPHv9MnMpJJCKIFQQgkE\nDAiIEAUEpRdxjZS10HT157ILiIuFYujKimAXQVEBGwqWVYMIioBSQg09tBDS22RmMu38/giMxCQk\nKEmYeD7PM0/m9vfeJPPOOffccxRCCIEkSZIk1THK2g5AkiRJkqqDTHCSJElSnSQTnCRJklQnyQQn\nSZIk1UkywUmSJEl1kkxwkiRJUp0kE1wd4nA46NGjB+PHj6/tUK6rOXPmsGzZsmvaJj09nfj4+OsW\nw0cffcT7779/zdtNmDCBEydOXHWdpUuX8tlnn/3R0EpxuVw88sgj3Hnnnbz33nt/eD/Lly8nMTER\ngCeffJIVK1b86dj27NnDuHHjGDp0KIMHD2bixIkcO3YMgPPnz9OqVSs++uijUtusWLGCJ598EoBl\ny5Zxyy23kJmZWWqdQYMGsXPnzj8d3+999tlnDB06lKFDh9KlSxd69uzpmd61a9c17+/MmTN07ty5\nyuuvWrWKVq1acfDgwWs+llRCXdsBSNfPd999R+vWrTl48CAnT56kefPmtR1SrQkLC2Pt2rXXbX+7\nd++mZcuW17zdm2++Wek6//znP/9ISOVKT0/np59+IikpCZVK9Yf3s3PnTlq0aHHd4vr111+ZNm0a\ny5cvp127dgBs2LCBsWPH8vXXXwOgVCpZuHAhnTp1olmzZuXux2w2M336dFasWIFCobhu8ZVn2LBh\nDBs2DChJ8i1btmTcuHHVeszLhBCsWbOGwYMHs2rVKp5//vkaOW5dIxNcHbJmzRoGDBhAREQE77zz\nDnPmzAHg448/ZuXKlSiVSgIDA1m4cCHh4eHlzj979izPPfccX3zxBVDyQXd5etmyZSQlJZGRkUGr\nVq148sknefbZZ8nOziYzM5OGDRvy4osvUq9ePVJSUnj22WfJyclBqVTyyCOPEBYWxpQpU/j+++9R\nKpVYrVZ69+7Nl19+SVBQkOc8zGYzTz31FEeOHCE0NBSVSkWnTp0A6N27N0uXLqV9+/alpgMDAxk9\nejTNmzcnNTWVBQsW8NBDD7F3716WLVtGamoqmZmZpKamEhYWxuLFiwkNDWX//v3MmjULh8NBREQE\nFy5c4Mknn6Rr166eeL777ju+//57tm3bhl6vJycnp8rX4XJ8FouF//73vzRu3Jjjx4/jdDqZPXs2\nnTp1KvXh2b59eyZOnMi2bdvIyMhg/PjxjBo1CpfLxaJFi/j+++/x9fUlJiaGkydPsnr16lLXbfz4\n8TidTkaMGMGyZcvIyMhg0aJFWK1WNBoNkydPJi4ujk8//ZSPP/4Yq9WKyWQqtZ/333+fgwcPsmjR\nIk+S3Lt3L/Hx8WRlZdGyZUteeOEFDAYDJ0+eZO7cueTl5eFyuRg7diwjR44s87f50ksv8Y9//MOT\n3ACGDBmCTqfD5XIBoNfrefDBB5k6dSpr165Fq9WW2c+QIUPYt28fb7/9dqXJprCwkNmzZ3PkyBEU\nCgU9e/bk3//+N2q1usLrfC12797NkiVLKC4uJjMzk7i4OJ577jkcDgdz5swhKSkJjUZDREQE8+fP\nL7Xt8ePHmTRpEk899RR9+vQps+/t27djtVqZOnUq/fr1Iz09nbCwsGuKTwKEVCccP35cREdHi5yc\nHLFv3z4RExMjcnJyRHJysujatau4cOGCEEKIlStXimeeeabC+Tt27BADBw707PfK6Zdeeknceeed\nwuFwCCGEWLVqlXj99deFEEK43W4xfvx4sWLFCiGEEMOGDRPvvfeeEEKICxcuiD59+ojCwkIxZMgQ\n8cMPPwghhPjoo4/Ev/71rzLnMnfuXPHEE08It9stsrOzRVxcnHjppZeEEEL06tVL7N+/37Pu5elz\n586JqKgo8euvvwohhDh37pzo2LGjJ+7LxxdCiEmTJomlS5cKh8Mh4uLiPPFs375dtGrVSuzYsaNM\nTNOnTxdvvfXWNV+Hy/Ht2LFDtGnTRhw+fFgIIcSKFSvE6NGjy+w7KipKrF69WgghxIEDB0S7du2E\nzWYTa9asEaNHjxY2m00UFxeLhx56SIwZM6ZMnFeed05OjrjllltEUlKSEEKIY8eOiS5duoizZ8+K\nTz75RNx8882ea/J7Y8aMEV9//bUnvpEjRwqLxSKcTqcYPny4WL9+vXA4HGLAgAHi4MGDQgghCgoK\nRP/+/cXevXvL7K9jx47i+PHj5R7ryrhdLpcYPXq0WLBggRBCiLfeektMnz7dc91nz54tjhw5ImJj\nYz3HHThwYLm/syeeeEI899xzwu12e67Z5d9TRde5Ilf+ji57/PHHPX9vhYWF4uabbxbJycli+/bt\npf6HFixYIJKSksTp06dFp06dRHJysrjjjjvKjfmyRx99VCxevFgIIcRDDz0klixZUuG6UsXkPbg6\nYs2aNfTq1YvAwEBiYmJo1KgRH374Idu3b6dHjx6Eh4cD8MADDzBnzpwK51emY8eOqNUlBf/777+f\n2NhYVq5cyaxZszh+/DgWi4W8vDyOHDnCPffcA0B4eDiJiYmYTCZGjx7Nhx9+CMC6deu47777yhxj\n+/btDBs2DIVCQVBQEH379q3SNVCr1XTs2LHcZV26dMFkMgHQtm1b8vPzPfd/brvtNgC6detW5WrI\nqlyH32vQoAFt2rQpFUN5Ln+jj46Oxm63Y7FY2LJlC0OHDkWn06HVarn33nsrjXH//v1ERETQoUMH\nAFq2bElsbCy//PILAK1atfJck8rccccd+Pj4oFKpaNmyJTk5OZw+fZqzZ88yY8YMhg4dypgxY7DZ\nbBw+fLjM9kqlErfbXelxlEolixcv5tNPP2Xbtm3lrtOqVSsmT57MlClTyr3Ol23dupUxY8agUCjQ\narXEx8ezdetWz/LyrvO1WLx4Mbm5ubz66qvMnj2b4uJiioqKaN26NS6Xi3vuuYelS5fSv39/z+/A\nZrNx//33065du1K1BFdKT09n8+bNDB8+HCipKl27di02m+2a4pNkFWWdYLFY+Pzzz9FqtfTu3Rso\nqa567733GD9+fKl7FTabjdTUVFQqVbnzFQoF4oruSR0OR6ljGQwGz/vFixezf/9+7r77brp27YrT\n6UQI4fngv3L/p06dokGDBgwePJglS5awY8cOLBYLN998c7nndGUMv7+XdOUyu93uea/Vaj3H/j29\nXu95f/kcVSpVqX2Vd6yKVOU6VCWG8uh0Os86QKlreplSWfl3U5fLVeY+lRACp9OJRqMpdQ6VufL4\nl2N3uVz4+vry+eefe5ZlZWXh6+tbZvuOHTuyb98+oqKiSs2fPXs2ffv2JSIiwjMvPDyc2bNnM336\ndM89sN8bO3YsP/30E3Pnzq0wZrfbXer83W43TqfTM13eda4qIQTx8fG0a9eOnj17MnDgQPbu3YsQ\ngoCAADZu3MiePXvYsWMHkydPZtKkSXTr1g2AV155hWnTppGYmMgdd9xRZt/r1q1DoVAwYcIET9yF\nhYV8/vnnVfpiI/1GluDqgI0bNxIQEMCPP/7I999/z/fff09iYiIWi4XCwkK2b99ORkYGAGvXrmXx\n4sV07dq13PlBQUFcuHCB7OxshBB8+eWXFR73p59+4v7772fYsGHUq1ePn3/+GZfLhclkIjo62tMy\nMC0tjfvuu4/CwkJ8fHwYMmQIM2bMqLCVY8+ePfn4449xu93k5+ezadMmz7KgoCBPq7KdO3eWaVF3\nLZo3b45Wq/V8q9+/fz/Hjh0rt/GCSqUq9eFYletwPd12221s2LABu92O0+lk/fr1lW7TsWNHTp06\nxf79+4GS+z6//vorXbp0qXTbq53vZZGRkej1ek+CS0tLY9CgQeW2+nvkkUdYvnx5qWWffvop33zz\nTZmkB3DXXXcRFxfHO++8U+Hx58+fz5YtWzhz5ky5y3v06MF7772HEAK73c6HH35I9+7dr3pOVZWb\nm8uRI0eYNm0affv2JTU1lfPnz+N2u0lMTGTcuHHExsby+OOPM3jwYA4cOACUfMnp1KkTc+fO9dy3\nvZLT6eSjjz4iISHB87/8ww8/MH78eN59991rSsKSLMHVCWvWrOHBBx8sVfrw8/Nj7NixbN68mWnT\npnkeHQgJCWHevHmEhYVVOD8+Pp67776bkJAQbr/9ds8/5+89+uijLFq0iKVLl6LRaIiNjeXs2bMA\nvPDCC8yePZvVq1ejUCiYO3cuISEhAIwYMYIPP/ywwm/n//d//8fMmTPp378/QUFBpT4Ap06dyqxZ\ns1i3bh3R0dFER0f/4eumVqtZtmwZM2fOZMmSJTRt2pTg4OBSJa3L4uLiWLBgwTVfh+tlxIgRpKSk\nMGzYMAwGA40aNcLHx+eq2wQFBbF06VKee+45bDYbCoWC+fPnExkZyd69e6+6be/evVmyZEmZEvyV\ntFotr7zyCnPnzuWtt97C6XTyz3/+09Mg6EqdO3cmISGBuXPnYrFYPI163n33XYKDgzl//nyZbZ5+\n+ml279591fNbsGBBhY/FPP300yQkJDB48GAcDgc9e/bk4Ycfvup5V1VQUJDnkQcfHx/Cw8O56aab\nOHPmDMOHD2fr1q0MGjQIg8FAQEAACQkJpa7lLbfcQr9+/Xjqqad47bXXPPM3bdqESqVi0KBBpY73\n4IMPsnr1an766Sd69ux5Xc7hr0Ah5FcCqQYJIXjzzTdJTU1l9uzZtR0OCxcuZNy4cQQHB5OWlsbQ\noUNJTEzEz8+vtkMr5aeffiI7O5uhQ4cCkJCQgE6nY9q0abUcmSTduGQJTqpRffr0ITQ0lFdeeaW2\nQwGgYcOGPPDAA6jVaoQQJCQk3HDJDUoaiKxYsYK33noLt9tN69atmTVrVm2HJUk3NFmCkyRJkuok\n2chEkiRJqpNkgpMkSZLqJJngJEmSpDrJqxqZZGYWXpf9mEw6zObi67KvmuKNMYN3xu2NMYN3xu2N\nMYOMuyZVJeaQkLKdC8BftASnVv/xXtZrizfGDN4ZtzfGDN4ZtzfGDDLumvRnYv5LJjhJkiSp7pMJ\nTpIkSaqTZIKTJEmS6iSZ4CRJkqQ6SSY4SZIkqU6SCU6SJEmqk6rlOTi3282sWbM4evQoWq2WhIQE\nmjRp4lm+YsUKvvzySxQKBQ8//DB9+/bFZrMxbdo0srOzMRqNLFy4kKCgoOoIT5IkSfoLqJYSXGJi\nIna7nXXr1jFlypRS42gVFBSwevVq1q5dy9tvv828efOAkjHNoqKi+OCDDxg2bNgN09t8ZYqLi9m4\n8bMqr//VVxv56actFS5fvXoVhw+XHTBSkiTJ26UXFrPh4EWe+iKZu17bwTu/nKvW41VLCW737t2e\nQfk6duxYahRfHx8fGjRogNVqxWq1ekZP3r17t2fgwri4OK9JcDk52Wzc+BmDB5c/eOfvDRgw+KrL\nx4594DpEJUmSVPssdhd7zuex43Quv5zJIyXHAkCQQUPXJoH0aFa9tXTVkuDMZjMmk8kzrVKpcDqd\nqNUlhwsPD2fgwIG4XC4mTZrk2cbXt6S7FaPRSGFh2W65TCbdVZ9qX783lY/3lB0Z+PcUCkWVh34f\nGduI4Tc1rHD5f//7LmfOpLBmzSrcbjdJSUlYLBbmzHmODRs+59ChQ1gsRTRr1oyEhHm8/PJygoOD\niYxsxttvv4VGo+H8+VTuuusuJk16mKeemkH//v3Jysrixx+3YrXaOHfuHBMmjGfIkGEcOLCfhIQE\njEYjQUFB6HQ65s6d54nHbDYzc+YzFBQUkpeXy91330N8fDz79+9jwYL5CCEIDQ1j4cJFHDt2tMy8\nhx+exLPPzqRZs2asW7eWrKwshg0bzmOP/QN//wDi4uJo3z6GV18t+QJis1mZN28BTZs25fXXX2PT\npk24XE7uvTcehULBuXNn+fe/p+JyuRg5cgTr1n2EVqut0rWvLSqVkoAAQ22Hcc28MW5vjBlk3BWx\nOVzsO5/PrjO5/Hwyi73n8nC4BDq1ki5Ng7i3S2N6NA8mKszkKdxUZ8zVkuBMJhNFRUWeabfb7Ulu\nW7duJSMjg02bNgEwbtw4YmNjS21TVFRU7qCTlfVHZrHYcTrdlcanViurtN7lfeblWSpcHh//d5KT\nj3DffQ+wYsXrNGwYweTJUykqMqPV+vD888twu92MHfs3jh8/jc3mwGq1YzbbOH8+lVWr1uBwOBg2\n7C7uvffv2O1OzOZiLBY7ubn5LFmynHPnzjJjxhTi4voxc+ZMnn56Ds2aNef1118mKyuzVHxHjx4j\nLq4Pt93Wm6ysTB57bCJ33TWEZ555ltmz59G0aSSffvoR+/YdZt682WXmOZ0uCgtt5OVZsFrt2GwO\nCgqsZGZm8sYb76LRaPj004+YMWMWwcEhvPvu23z++UZuueVWfvjhB1599W0cDgevvbaciRMfYfz4\nsdx//yR27txOhw6xWCxOLBZnla59bQkIMFz1d36j8sa4vTFm8P647U432RY7mWY7WUV2sszFZBXZ\nybE4qGfQ0NDfh4YBehoF+BBi0qKsIBnlWR3sSy0gKTWffan5JKebcbpLCg+tQk3cF9uQrk0C6dDQ\nH536tzti+fnWa475airqi7JaElxsbCybN29mwIABJCUlERUV5Vnm7++PXq9Hq9WiUCjw9fWloKCA\n2NhYtmzZQkxMDFu3bqVTp07XfNyB0WEMjA6rdL3q/OOMiChpTKPT6cnNzWXmzBkYDAasVitOZ+kP\n9mbNWqBWq1Gr1eh0+jL7atGi5LqFhoZRXFyS3LOysmjWrDkAHTrcxKZN35bapl69enz44Qds2bIZ\ng8HoOWZubg5Nm0YCMGLEPRXOu9KVhdzw8AZoNBoAQkJCePHFxfj4GMjMzKB9+w6cPXuGNm2iUalU\nqFQqJk+eCkDnzjfzyy/b+eqrDTzwwISqXkZJkq6TQpuT745lsvVENlkWOxfzbeTbyn7JVCnA30dD\nvtWB64r/fZ1aSQM/PQ0D9DT011PfT8/ZXAtJ5ws8VY4alYK2Yb6M7tyIjg39iGngh59eU1OnWKFq\nSXB9+/Zl27ZtxMfHI4Rg3rx5rFy5koiICPr06cPPP//M3/72N5RKJbGxsdx666106tSJ6dOnc999\n96HRaHjhhReqI7TrTqFQIsRvpUGlsuSbzo4d28jISGfOnPnk5uaydevmMtWilZXQyyvCh4aGkZJy\nisjIZhw6dKDM8jVrVtOuXQzDh49kz55dbN/+EwDBwcGcO3eWxo0jeO+9VTRu3KTceVqtjuzsLJo0\nacqxY0cIDg7xnOdlCxcm8OGHn2MwGElImAlAkyZN+eyzT3C73bjdbqZOfZxFi15k5MiRvPbaG+Tn\n59GiRcsqXFFJ+mtxC0Ge1UFGYTEZZjuZ5uJS7+0uQWwjf7o2CaR9uC9qVeVtA51uwc4zuXx5KJ0t\nJ7KwuwQRgT5EhfnSrr4vISYtwUYtwSYdwUYtISYtAT4alAoFTpebi4XFnM+zcj7PRmq+jfN5VlLz\nbew+l4fV4cakU9GhgT/924ZyU0N/2tT3LVVCu1FUS4JTKpXMmTOn1LzmzZt73j/++OM8/vjjpZb7\n+Pjw0ksvVUc41SowMBCHw8krr7yETqfzzG/TJppVq1YwceIDaLVaGjRoSFZW5p8+3pQp05k/fw4+\nPgY0GjUhIaGllt96axzPPz+fb7/9Gn9/f1QqFXa7nWnTZjB//hyUSiX16tXjb38bRWhoaJl5Wq2G\nJUsWEhoa5kluv3fnnQOYOPEBfH19CQysR1ZWJi1btqJr11t45JFxuN1uhg8fiVarJSamA6mp5xg+\nvGwJUZK80ansIvLSzThtDnRqJXq1Cp1aiU6jRK9Wors07XC5ybE4yC6yk2Oxk11kJ7vIQfaV74tK\nEtnlar3LVAqoZ9QS6qvDLWDlzrOs2HEWo1ZFbCN/ujUNpGuTQCICfUp9ET6RVcSXh9L5X3IGWUV2\n/PVqhrUPZ2B0GG3CTAQGGiutvVKrlDQK8KFRgE+ZZUII8m1O/PTqCqstbyQKUdXWFjeA6zUenDfW\nn1+O+ZNPPqR3774EBgbyxhuvoNFoePDBG7fqz89Pz3333ceSJcswGk2Vb3AD8Ma/D/DOuL0lZpdb\n8NOpHNbuOc+uc/l/al8BPhqCDBrqGbUlScykI8xXS4hJR6ivjlCTliCDFpXytwRSaHOy61weO8/k\nsvNMLufzbADU99XRtWkgjfz1fH88i+R0Myqlgh6RQQyMDqNHsyA0V5T4vOV6X+mGuwcnVZ+goCD+\n/e9H8fExYDKZeOqpWbUdUoUuXEhl3LgnGDRouNckN0m6krnYycZD6azbk0pqvo1Qk5bHekZye9sw\nsvMs2Bxuip2XXy7Pe5vTjUqhINioJch4KZkZtAQZNFWqYvw9X72aXi2D6dUyGIDzeVZ+OZPLjjN5\nfH8si8JiJ61CTfy7V3Puah1CoOHGbqlcU2QJzkt4Y8zgnXF7Y8zgnXHfqDGfz7Oybu8FNh68SJHd\nRUwDP+JjG9KrRT3Ul5qt3yhxO92CXIudEJOu0nVvpLirSpbgJEmSrpHLLTAXOym8/LI5ybM6+OZI\nJj+ezEapVNC3VQjxsQ2Jrl/+B+iNQK1UVCm5/RXJBCdJklezO93k2xzkW53k2xwU2JzkWy/9tDkp\nuDzv0k9zsZMCm5Miu6vc/QX4aHiwWwQjO4TLxOHlZIKTJOmGlZpvZfe5fLKL7ORZHVe8Skpb+VZH\nhYkKSko3/j4a/PRq/PVqwv30+OpUmHRq/PRqz09fnRrfSz8jAg03ZJN36drJBCdJ0nUhhCCtoJgT\nWUWcyCwiu8hO82ADrcN8aRFsRFuFpGF3utl7Pp9tKTn8nJLDmdzferzw0SgJ8NEQ4KPB30dDk0Cf\nK6bVBFxKZH56Df56Nf4+GvRqZZW7hJLqHpngatBjj01k2rQZHDp0AD8/P3r0uK3U8iFD7mTDhm8q\n3H7Lls1ER7dDoVCwcuVbTJ36ZHWHLEnlMhc7OZlVxPHMIk9CO5FVVKo05aNRYnWUdIKgVipoHmyk\nTZiJNmEmT9KDklLazym5/JySw66zedicbrQqBbGNAhjRIZxuTQNp4KdHr6m4H1pJKo9McLWgshEF\nKvLRR2to2nQGTZo0lclNqnFCCPacz2fVznPsOJPrmW/SqWgRbKR/m1BahhhpHlzyMmpVXCiwkXzR\nTHK6mSPphXx/PIvPDlwESpJesEnHxYKSZ7oa+usZ3K4+3SMD6dQ4AB+Z0LySwpaLOuswAG5jGG5D\nKELrW3nXTdWgTiU43ZGP0SevrXQ9lVqFv7Pievsr2drEU9x6ZIXLZ8yYxj33xHPTTZ1ITj7EO++s\n4Jln5rBgQQJmcyH5+XkMHjyc4cN/28eKFa9Tr149Bg8ezqJFc0lJOUXDho2w2+0AnDp1gmXL/ovb\nLTCbC5k8eSput50TJ46RkPAszzzzHAkJM3njjVX8+usO3njjVXQ6HX5+/vznP89y/PhR3n//XTQa\nNWlpF+jduy/33z+uVNybNyfy6acfeboPS0hYhJ+fHy++uJjk5EM4HE7GjZvIrbfGlZlnNJr4/PNP\nmD17PvBbyXPu3Fnk5+dTUJDPwoVLePXVZeTkZJGdnUO3bt2ZMOERzp07y8KFCTgcDvR6PTNnJvDI\nI+N488138PPzZ/36j7FaLYwa9fcq/X6k6ucWgh9PZrPql3McTCskyKBhXLcIouv70jLESJivrsJq\nwIb+PjT09+GOViW94gghSiW9LKuD1iFGujct2yuHdONTFqWjzjyIOvMA6qyDqDMPoiosO6KLUOtx\nG8JwG0NxG0JxGUJxG8MobjkMt1+jaouvTiW42jB48DC+/voLbrqpE1999QWDBw/n/Pnz3HFHv1I9\n+l+Z4C7bseNn7HY7b7yxiosXL/LDDyUjLKSknOKxx/5F8+Yt+Pbb//HVVxuZP38eLVpEMW3aDE+n\nx0IIFi2axyuvvEVISCgffriGd95ZQffuPUhPTys1UsHvE9y5c2dZvHgper2eRYvm8ssv29Hp9OTn\n5/Hmm++SnZ3FJ598iNstyszr3LlLhdejU6fO3HvvaNLSLhAd3Z6xY0eRnp7LiBEDmDDhEV5++UXG\njHmAbt26s2nTd5w4cZx+/fqTmPgtI0bcwzfffMW8eYuv429I+qOcLjffHMnknV/PkZJtoYG/nul9\nWjAoOuwPVxcqFIpSSa+6n8tS2HJR5aWgyk9BlX8ahEBojAitCaExIDSmS9PGkp8aEwgnCkcRCocF\nhd186f2ll92MwmFBqbRhMuejsJfMVzqKwFGE8tJyhcOMUKg8H+huQ+hv742hJR/2hlBcfo1B4yXD\n7jgsaC7uRnNhJ+rM/SXJzJIBgECBKyASR/1OWNvdjzMkGhQqlEXpKC0ZKIsyUFpK3quyj6A5txWl\nvRCFy46ly7+rLeQ6leCKW4+8amnrsoAAA/nX6Z+qa9dbeOWVpRQU5LN//14mT55Kbm5OuT36/15K\nyknatIkGoH79+oSGloyEEBwcyqpVb6HT6bBYLBiNxnK3z8vLw2Awevqj7NjxJl5//RW6d+9R6UgF\ngYFBJCTMxGAwcObMadq1iyE9/QzR0TEA1KsXzMSJ/2D16lVl5u3Zs6vUvq7sK+DyaAp+fn4kJx9i\n+vRpaDR67HYHAGfPnqFdu5L99enT99I2TZk58z907HgTQUH1CAqqV9lll6qRzeHi8wMXeW/XeS4W\nFtMi2MhzA1pzR6sQ1MobsIQlBKqcY6hzjqHKP1WS0PJOocpPQWn7rSpVoEDBn+/XQiiUoDWh1VxO\nipdepga4rphGOFEWZaCyZKDO3I/qdAYKZ+nPHaHUYm/Si+IWgylu2he05f+v1wZFcQGatF/RXNjh\nSWoKtxOhUOIKisIRcRvW4HY4QtrjCm6L0F5jb0VOK6jL9nd5PdWpBFcblEolvXrdwfPPL6Bnz9tR\nqVQV9uj/e02aNCUx8RvgPrKyMsnMLOmMeenSxTz7bAJNm0ayYsXrpKVd8BzL7f5t5IKAgAAsliKy\nsrIIDg4mKWkPjRtHAFev7jb4107NAAAgAElEQVSbzaxY8TqffPIFAP/616MIIWjatCmbN2/yrPPs\ns08yfPjIMvMeemgS2dnZAFy8mEZBwW99810edeCrr77AZPJlxownOXjwKBs2rEcIQZMmkSQnH+Lm\nm7vy7bdfU1CQz8iR8ZhMvrzzztsMGjT0Wn8F0p8khCA138b+CwUcuFDApmNZ5FoddGzox/Q7WnBr\nZNANWXWoyj6C7vgGdCc2oM4/7ZnvMoXj8m9GcfOBuPwjcQU0wxUQicsvApRqcFgvlbquKJ3ZL5fS\nzAiFuqR0pzWVTmCXSn2o9ARUodPi8ijs5kslmpLSjDp9L7oTX6BL+Qah1lPc5A6KWw7G3qR3tX/4\ne7iKURZllpSwzGloLu5Ck7oDdfZhFMKNUGpwhnXE2vFh7A274azf+dqTWXlq4PxkgrsOBg4cwt/+\nNpS1a9cDFffo/3s9e97O/v37mDDhfurXDycgIACAfv368+STUwgKCiIkJJT8/DwA2rWLISFhJk88\n8RRQUt3zxBNP8dRT01AqFfj6+jFjxixOnTpx1XiNRiPt23fgoYfG4OPjg6+vL1lZmQwYMJhdu37h\nkUfG4XK5ePDBCXTr1r3MvNat22AymZgw4X6aNo0kPLzsiOedOt3MrFkzGD16FBqNlkaNGpOVlcmj\nj/6TxYvn8c47K9Dr9Tz77HMADBkyjBdffN4zLVUfi93F4YuFHEgrSWgH0wrJtZaUsA0aFTdHBDCm\ncyM6NvIv2UAIFLbc0tVN1uzSVXelkkTJCwTOoNY4Q9rhDGmPM6QdwufPlc5VeadKktrxDahzjyEU\nShwNu1N408M4wmJx+UeCppIPTm1JlaSL0KuvVw2E1oRLa8IV0AyA4pZDKbr1WTRpv6I7sQHdiS/R\nn/wCt8aIPbIfxS2GYG/cE9yu0gn5d0lZ4ajCAKLChdKdh29O6qUqw0uJtjiv9GoqHY76nbB0noyj\nQVcc9WNrLtleZ7IvSi/hjTFD1ePetOk7UlJOMn78wzUQ1dXdKNfaLQQX8m2czCrCpFPTMsR41UEk\ny4tbCMG5rDys+z4i8vT7BBefwyx0FAk9Rehxqo2odSZ0Bj9Mvn4YTQEocf/u3kkmCnfZL2gAQu1T\nbimnpIrOhTorGVXBGc/6LlODkmQXHI0zpD2GyI4UFLnL3fdlCnsh2pRv0B3fgCbrEAIFjgZdKG4x\nhOLmAxCG8od1qk7V9jfidqJJ3VGS7E5+VSb5/FlCpcXtE3LF/cCw0vcFjWE4g6JAdeN01iz7opS8\n2uuvv8y+fXtZsMA7BrmtDvlWR6nnyU5kFXEyq8jzHNll9X11tAwxEhVqIurSzwb+es/YXOZip6d0\ndu7caWIyPuVu8R3BigKOiCbsNg4m3OAmTOegscqO1m29VAJIRZF1FEVaEfBb825Hw2blfBCG4vYJ\nLkliysobmyhseaizDpW0trvU0k6b8q3nflhVy3SOsFjMPWZR3HwgblN41S+uN1GqcTTugaNxD8xx\nc9Ge/xF1xr6rfpEQGhNCra/CCMpK/EPrk5dfhdJeHSFLcF7CG2MG74z7qjHbi1Dln0ZlSccR2hHh\nE/SHjlFoc7L1ZDabj2eRnF5Ihvm3EpK/Xu15nuzyz8JiJ8cyijieaeZYRhFnci1cHiPToFHRIsRI\nscvNsXQzbRWneUj9NUNU21Hh4mxQT4o6jKdeq9tR/YGhWqqFvQh1djK+ttNYi2xXX1epxt7oVtx+\njWsmtirwxr9r8M64ZQlOkq4nZzGqnGOlWuN5fhale1YTKHDWj8XepDf2Jn1wBkdf9Vu0ubgkqX13\nNJMdp3NxugXhfjo6RwTQIthIixAjLYON1DNqy23UcUvT35KpzeHiZLaF4xlmjmUWcSojn17qPbwR\n/DlNzEm4NUZsbf5OQfsHMQZEcuO0zbtEa8QZ3hkREIfNyz5wJe8hE5wkXaLKOYZh9zLUxzcQJH7r\nCMDtUw+XfySOxrdh84/EGRCJ8AlCk7oD7ZlNGHcuxrhzMS5jmCfZORr1QGhNmIud/Hgqm8SjWWw/\nnYPDJQjz1XHvTQ3p2yqYtvV9/1ALRb1GRXR9X6JDdOiPbsEnbTnq/NO4fBthvvVZbG3iETq/63l5\nJMnryAQn/eWpsg5j3LUU7cmvQK3H3WkcRQHtPM3Lhc6/3O0cDbtj6fJvFJZMtGc2oz2zCe3xjfgc\nXoNToeGIJpp91hAy3AFE6YLp3bwpbZs3J7JJU/AJrtL9qwq5itEnr8Ow+2VU5lQcITE4R6wkN6xX\nSVN4SZJkgpP+utTpSRh2vYTu9Le4NSYsnR7D2mEC/uGNKK6k2szudJOSbeFYZkkV4fHMVhzPbITF\ndi83K49yuzKJ3hxiuOYkBnchuIGzl16UPCxc0potDGdwWxwNu+EI71Z5t0UOKz6H38dn76uoitJx\n1O+E+fb52CN6ERBoBFndJ0keMsFJfznqtF8x7noR7dktuHX+FHWZgrX9gwh9QIXb2Bwu9l0oYM+5\nPPacz+dgWiHOS6089GolLUKM3BEVcqmFY2daBE/EoFVRBBQ5bSgtmaUe8L3cdZHKfBHdqa/xudSH\nqsvUsCTZNeiKo0G3kue6FAoUdjP6g+9iSHoDpTULe4NuFPZZiqPRrbXSia0keQOZ4KQ6z+kWXMy3\nYjv5I5FHXyUkbxd2bSCpHaZia/93jL6BqH7XBdWVCW33uXwOXSxJaCoFtKnvy32xDWlzqbPhxgE+\nZbYvRa3H7de44laAwl3SP9+FHWgv7ER7dgv6o58A4DKE4gy7Cc2FnSiL87A3jsPS+Z84GnS9XpdH\nkuosmeCkOsHpFpzKKuJ8vo3UPCup+TbO51k5n2slyvwL/1Cv5xblMTJEAM85R/OBrQ/WnXrYeRAF\nXBoos2SwTJVayeELBaUS2qhOjejU2J8ODf0waq/zv41CiSu4La7gtthiHirpWzHvpKcPQM3F3TjC\nO2Pp9DjO+rHX99iSVIfJBCd5Nadb8E1yBm/tOMP5vN+ep/LTqxhh2M8CPiZSexSzLozDUU9R1Dqe\nW50q2tuc5Nsc5NucFFgv/bw0LRSK6k1olVEocAW2wBXYAlv0mJo9tiTVIdXyn+t2u5k1axZHjx5F\nq9WSkJBAkyYlvcwnJyczb948z7pJSUm8/PLLxMTEcOeddxIVFQXAHXfcwf33318d4Ul1gMst+O5o\nJm9uP8PZXCtRIUZm3hVF83o+ROX+QPC+l1FnH8blF0Fh94XYWt9DiEpLVTp18saHYSVJKqtaElxi\nYiJ2u51169aRlJTEggULePXVVwFo06YNq1evBuDrr78mNDSUuLg4fv75ZwYNGsQzzzxTHSFJdYRb\nCBKPZvLW9rOk5FhoEWxk0ZC23NbMH58TGzF8vwx17nGcAc0p6PMixVHDZLN5SfqLqpb//N27d9Oz\nZ08AOnbsyMGDB8usY7FYWLZsGe+99x4ABw8e5NChQ4wZM4agoCCefvppQkNrvrdv6cbkFoIfjmfx\nxvYznMyyEFnPwILBbegdocVwZB0+769EVXgOZ1ArCvq9QnHzgX/uOTNJkrxetSQ4s9mMyfTbeEEq\nlQqn04la/dvhPv74Y+666y6Cgkq6H2rWrBnt2rWje/fubNiwgYSEBF566aXqCE/yIlnmYn5OyWXt\n3lSOZxbRJNCHuQNb0y/MjPHAS+h/+BClowh7g26Ye8zCHtkXFDdIf4uSJNWqaklwJpOJoqIiz7Tb\n7S6V3AA2btxYKoF169YNH5+SMYf69u1bbnIzmXSo1X/+W7lKpSQgwEuGib/EG2OGa4/b6XKz91we\nW49nseVYJskXSzrYjqxn4Pm72zPE/yTqX59Csel/oFQjou/GcfMkFOEdMADX4wr9Va71jcAbYwYZ\nd036MzFXS4KLjY1l8+bNDBgwgKSkJE/DkcsKCwux2+2Eh/825MXTTz9Nv379GDBgANu3byc6OrrM\nfs3m4usSnzc2IvDGmKFqcWeai9meksvPp3PYeSYXc7ELlQJiGvrzaI+m3Bphol3utxh2/Ad1djJu\nfRCWzo9ja/d33Mawkp1cx2tTl6/1jcYbYwYZd0264UYT6Nu3L9u2bSM+Ph4hBPPmzWPlypVERETQ\np08fUlJSaNiw9CjQU6ZMYcaMGaxZswYfHx8SEhKqIzTpOsgusvPd0UxMOhWdGgcQ7qe/pu1dbsHB\ntAK2peSw7VQOxzJLSvshJi19WobQPTKQLk0CMWlVaFP+hynxOVQFZ3EGtaKw12JsUcO8doRhSZJq\njhwPzkvUdsxCCHafy+eTfWlsPpGFy/3bn00DPx2xjQPo1Ni/TMK7HHe+1cGO07n8lJLD9pQc8m3O\nklJaAz+6RwZxa7MgWgQbPT3rq3KOYfpxJtrzP+IMaoW5+9M4Im6vkW6pavta/1HeGLc3xgwy7pp0\nw5XgpLoj3+rgy8PpfLIvjbO5Vvz0au69qQHD24fjcLvZfS6f3efy+PFkNl8cKhkr7XLCi23kj8UN\niYcvsv9CAW4BAT4abm0WxK2RQXRrGoifXlPqeIrifAy/LMHnwCqE1kRhz+ewtRsrm/pLknTN5KeG\nVIYQggNphXy67wKJx7IodrqJaeDHuG4R9G4ZjF7zW0OfliEm4mMb4haCU1kWdp/LY/f5/FIJr1Wo\niQe6RtAjMoi29X3L77fR7UKfvBbjjoUobLnYokdT1PWJPzxitiRJkkxwEkIIzufZOJBWwIELBew+\nn09KtgWDRsWg6DBGxIQTFWq66j6UCgUtQkpGpb73UsJLybbQKNQXndt91W3Vab9i+vFZNJkHcIR3\nwdxzDs6QdtfzFCVJ+guSCe4vyGJ3cfhioSehHUwrJNfqAMCgUdE23Jf4mxpwZ5vQP9wPo1KhoHmw\nkQA/fen6cyFQFpxBnXkQTeYB1OlJaFO34TLWp6DvcopbDpXDv0iSdF3IBPcXcSHfxjdHMvj+aCbH\nsoq43EakSaAPtzYLon0DP9qH+9KsnvHqQ79cC7cLMo+gO7ULdeZB1FkHUGceQmkvAEAo1TiDWlHU\neTKW2H+Axruez5Ek6cYmE1wdlmd1sOlYJv9LzuBYagaPqj/jU/V37IkYSfZNk4luEIS/j6byHVWV\ny446Y/+lcc12oE7bhdJhRgMIlQ5nvTYUtxyKM6QdzpD2OOu1ApXu+h1fkiTpCjLB1TE2h4ufTuXw\ndXIGP6fk4HS7meC3i9W+q/F1ZOEI7UiP9NU4kg5TELIct08Fg3BWhdOKJn0vmtRL45al70bhLBmy\nxhnYkuKo4WiadaPA2ApXYAvZElKSpBolP3HqACEEB9MKWb8/je+PZ1FkdxFs1PKv1mbG5r+CX/Ze\nHCEx5MatwFm/E7rjGzD9MJ3AdXdS2GsR9haDruVgaM5txbD3NTQXdqBwOxAocAa3xdp2FI4G3XCE\nd0EYgoGSZ1hcXvbcjSRJdYNMcF7M5nDx3dFMPkq6QHK6GaNWRe+WwQxppubW86/jk7wG4RNEYa/n\nsbX5m6cT4uKWQ3CEdsDv20fx/+ZhrOfHYO4x8+q9gwiB9swmDL++iCYjCZexPtaYh3A0vAVH/c4I\nfUANnbUkSVLVyATnhS7k2/hkXxqfH0gj3+Yksp6B6X1a0L91IMFHP8Cw5QUUTgvWDuOx3DwZofMv\nsw+3fxPyRqzHuHMRhr2vokn7lYJ+r+Cq16r0isKN9tTXGHa9hCbrEC7fxhTevgBb63vk/TNJkm5o\nMsF5CSEEO0/nsn7vaY6lnCJMkcuEcDe9wl001RWgzMlA8+lu1LknsDeOw9xjNq6gllffqUpDUfen\nsDfqgV/iPwn8aADmHrOxRY8G4UZ3YiOG3ctQ5xzF6R9JQe8lFEcNB9V1bJgiSZJUTWSC8wL5ez6k\nYOdSeriyGKQogssFp+ySl1AocfuE4PZrRH7/Fdgj+13Ts2SOiNvIif8Ov8TJ+G55Eu3pb1Hln0ad\ndwpnYBQFfZdR3GKIHEBUkiSvIhPcjcxlx7jtOUIOrOSwiORiowEoGzRBYQrDbQjFbQzDZQhF+NT7\n08lHGELIH7wan72vY9y5EGdQK/Lveh17s/5yAFFJkrySTHA3KGXRRfz+9zCai7t42zWAMzc9wWM9\nW3B9RsSrgEKJNfYRbNGjEFo/2aOIJEleTSa4G5AmdTt+3/wDhaOIL1skMOdgM764ObLGjl9eoxRJ\nkiRvI+uebiRC4LP3dfw/j8et8yNn5EYWpUbTPtyPVvXLH+9IkiRJKp9McDcIhd2M3zcPY/r5OezN\n7iTvni/41RLGmVwrIzrUr+3wJEmSvI6sorwBqHKO4/e/CajyTmHu/jTWjpNAoWD9/mRMOhV3RIXU\ndoiSJEleRya4Wqa5sAO/L+4HtQ/5Q9fiaNgdgDyLg++PZzEiJrzUAKOSJElS1cgEV8uM2xcg9IHk\njViP2xTumf/l4XQcLsGwmPCrbC1JkiRVRN6Dq0XqjP1oLu7CGjOuVHITQrB+fxoxDfxoEWysxQgl\nSZK8l0xwtcjnwEqE2lDSEfIV9pzP50yuleExsnGJJEnSHyUTXC1RWLPRHd+ArfXIMs+drd+fJhuX\nSJIk/UkywdUSn0MfoHAVY23/QKn5lxuXDGwbJhuXSJIk/QkywdUGlwP9wXewN+qJKyiq1CLZuESS\nJOn6kAmuFmhTvkFVdBFrzEOl5svGJZIkSddPtTwm4Ha7mTVrFkePHkWr1ZKQkECTJk0ASE5OZt68\neZ51k5KSePnll2nXrh1Tp07FZrMRGhrK/Pnz8fG5ygjTXsyw/21cfhHYm/QuNf9y45KZXRvXUmSS\nJEl1R7WU4BITE7Hb7axbt44pU6awYMECz7I2bdqwevVqVq9ezahRo+jXrx9xcXG88sorDBo0iA8+\n+IC2bduybt266git1qkyD6FJ+6Xk3tvvhrhZvz8NX51aNi6RJEm6Dqolwe3evZuePXsC0LFjRw4e\nPFhmHYvFwrJly3jqqafKbBMXF8fPP/9cHaHVOp8DbyPUPtja3Ftq/uXGJQPahsrGJZIkSddBtVRR\nms1mTCaTZ1qlUuF0OlGrfzvcxx9/zF133UVQUJBnG1/fkh7zjUYjhYWFZfZrMulQq//8h79KpSQg\nwPCn93PNLNmoj32Gu8Mo/MNKNyL55FAKDpfg77dGlhtbrcX8J3lj3N4YM3hn3N4YM8i4a9Kfibla\nEpzJZKKoqMgz7Xa7SyU3gI0bN/LSSy+V2Uav11NUVISfn1+Z/ZrN12e4z4AAA3l5luuyr2vhs3sF\nGlcx+VFjcF1xfCEEH+w8S0wDP0J1qnJjq62Y/yxvjNsbYwbvjNsbYwYZd02qSswhIeUPJ1YtVZSx\nsbFs3boVKGlEEhVVuil8YWEhdrud8PDwUtts2bIFgK1bt9KpU6fqCK32uJ34HHwXe6MeuOq1KrVo\nz/l8zuZaGSEfDZAkSbpuqiXB9e3bF61WS3x8PPPnz+c///kPK1euZNOmTQCkpKTQsGHDUts88sgj\nfPnll8THx7N3717GjBlTHaHVGm3KN6jMF7C2f7DMssuNS/pEBddCZJIkSXWTQgghajuIqsrMLHtf\n7o+ojWK6//q7URVeIGfMT6VaT+ZZHAx4YwcjYsKZ2rtFhdt7Y9UCeGfc3hgzeGfc3hgzyLhr0g1X\nRSmVpso6jPbCTqzt7y/zaMCaPedlzyWSJEnVQCa4GlAyaoC+zKMB21JyWLnzHAPahsqeSyRJkq4z\nmeCqmcKWi/7op9ii7kboAz3zz+dZefarI7QIMfKfO1rWYoSSJEl1k0xw1Ux/eE3JqAExD3jm2Rwu\nnthwGCFg0ZC28sFuSZKkalAtz8FJl7id+Bx4B3vD7rjqtQFKnnmbn3icE5lF/HdEOxoF1M3+NiVJ\nkmqbLMFVFyHQH/kYlTkVa8xvjwZ8lHSBrw5nMKF7E26NDKrFACVJkuq2SktwDocDjUZTE7HUCQpr\nDvpjn6I/vAZ1zlGcgS2wN+0LwL7UfJb8cIoezYIY1y2iliOVJEmq2ypNcCNGjKBbt27cc889ZXok\nkS4RbjTnf0J/eA26U9+gcNtxhHak8PaFFLccCko1WeZipm9MpoGfjjn9W6NUKGo7akmSpDqt0gT3\n+eef8+OPP7J8+XJyc3MZMmQIAwYMwGiUzdqV5gvokz9En7wOVeE53LoArO3GYmsb77nnBuBwuXly\nYzJFxU6W330Tvnp561OSJKm6VfpJq1QqiYuLA0pGAFi9ejWffPIJw4cP5957761k67pJYS/EtPUZ\ndMc+RSHc2Bv1pOiWJymOvBPU+jLrL91yin0XCpg7sDUtQuQXA0mSpJpQaYJbtGgRmzZtokuXLkyY\nMIGYmBjcbjcjRoz4SyY4dXoSft8+irLwPNaOE7G2+ztuv4rvp311OJ11ey8wqlND+rUOrcFIJUmS\n/toqTXBNmzZl/fr1GAwGHA4HUFKqW758ebUHd0MRbnz2vo5x50LchjDyhn+CM7zzVTfZl5rPvO+O\nc1Mjf/6vZ2QNBSpJkiRBFR4TEELw4osvAjBp0iQ+++wzABo1alS9kd1AFJZM/L8Yi2n7XOyR/ci9\n95sKk5sQgm0pOTzy0X7Gr92Hv17N/EFtUKvkExmSJEk1qdIS3Nq1a1m7di0Ar7/+OmPGjGHYsGHV\nHtiNQnN2C36Jk1HYCyi8bQG26NFQTgvIYqeb/yWn8/7uVFKyLYSYtDzWM5LhMfXx08vHLCRJkmpa\nlRqZ6HQ6ADQaDYq/SvN2lwPjzkUY9r6KMzCKgqFrcNVrXWa1PIuDj/dd4KOkC+RYHLQMMTK7fyv6\ntgpBI0ttkiRJtabSBNenTx9GjRpFTEwMhw4donfv3jURV+0RAlXOEXy/n4YmIwlr29GYe8wCTUmX\nWk6XmwyznQv5NhKPZfLFoXSKnW5uaRrImM6NuDki4K/zJUCSJOkGVmmC+8c//kGvXr1ISUlh2LBh\ntG5dthTjtdwuVHknUWceQJ15sORn1iGU9kIcal++az2fnzU9SPvuDBcLbKQVFJNpLsZ9aYhYjUpB\n/zahjOrUiOZyuBtJkqQbSqUJ7syZM2zduhWHw8GpU6f44IMPmDNnTk3EVi00F3ag3PE/AlL3os46\njMJpBUCodDiD21IcNZy3TvnzXk4r0pOCUCnOEeqro76fnk6N/anvpyfcV0e4n56oUCOBBm0tn5Ek\nSZJUnkoT3PTp0+nVqxd79uwhNDQUi8W7hjv/PcMvL6DMPIArOBpr9Gicwe1whrTDFdgClCWXY1Xy\nDtq38OWtXs0JNulQK2WVoyRJkrepNMHp9XomTZrE6dOnmT9/PqNGjaqJuKpN/tB1BAQYyM+3lbvc\nLQR5VgeR9QzU9yvbK4kkSZLkHar0HFxmZiYWiwWLxUJ+fn5NxFV9FMqSVwUKbU5cbiGrHiVJkrxc\npQnuscceIzExkSFDhtCnTx9Pv5R1Va6lpLeWIB/57JokSZI3q7SKcv/+/YwbNw4oeWSgrsux2gEI\nNMgEJ0mS5M0qLcFt2bIFl8tVE7HcEC6X4GSCkyRJ8m6VluByc3Pp2bMnjRo1QqFQoFAoPF131UU5\nngQn78FJkiR5s0oT3GuvvXbNO3W73cyaNYujR4+i1WpJSEigSZMmnuVbtmzh5ZdfBqBt27bMnDkT\ngLi4OJo2bQpAx44dmTJlyjUf+8/Ku5TgAuQ9OEmSJK9WaYJbv359mXmPPfbYVbdJTEzEbrezbt06\nkpKSWLBgAa+++ioAZrOZxYsX8+677xIUFMSbb75Jbm4uhYWFREdH/6GEej3lWOz469Xy2TdJkiQv\nV+k9uODgYIKDg6lXrx7p6emkpaVVutPdu3fTs2dPoKQkdvDgQc+yvXv3EhUVxcKFCxk1ahTBwcEE\nBQVx6NAh0tPTGTt2LBMmTODUqVN/4rT+uFyrgyBZPSlJkuT1Ki3BxcfHl5oeP358pTs1m82YTCbP\ntEqlwul0olaryc3NZefOnXz22WcYDAZGjx5Nx44dCQkJYeLEifTv359du3Yxbdo0Pvnkk1L7NZl0\nqNWqqp5bhVQqJQEBhnKXFdhdhPjpKlxeW64W843MG+P2xpjBO+P2xphBxl2T/kzMlSa4lJQUz/vM\nzMwqleBMJhNFRUWeabfbjVpdcqiAgADat29PSEgIAJ07dyY5OZlevXqhUqk889LT0xFClOqZ32wu\nruJpXV1AgIG8vPK7HMsssNE82Fjh8tpytZhvZN4YtzfGDN4ZtzfGDDLumlSVmENCfMudX2mCe/bZ\nZ1EoFAgh0Ov1PPHEE5UGFBsby+bNmxkwYABJSUlERUV5lrVr145jx46Rk5ODn58f+/bt429/+xvL\nly8nICCACRMmcOTIERo0aFArw87kWhwEygYmkiRJXq/SBPfWW29x8uRJ2rZtS2JiIt27d690p337\n9mXbtm3Ex8cjhGDevHmsXLmSiIgI+vTpw5QpUzxVnXfddRdRUVFMnDiRadOmsWXLFlQqFfPnz//z\nZ3eNnG5Bvs0p78FJkiTVAZUmuGnTpnHLLbfQtm1bUlJS+Prrr3nhhReuuo1SqSwzpE7z5s097wcO\nHMjAgQNLLff39+eNN964ltivuzyrfMhbkiSprqi0FWV6ejr33XcfABMmTCAjI6Pag6otuZaSbrqC\nZIKTJEnyepUmOPitocnZs2dxu93VGlBtutyLSYBMcJIkSV6v0irKGTNmMHnyZLKzswkNDWX27Nk1\nEVetyPOMJCDvwUmSJHm7ShNcmzZtmD9/vqeRSevWrWsirlqRI+/BSZIk1RmVVlFOnTqVffv2ASVV\nlU8++WS1B1Vbci12VEoFvvpK874kSZJ0g5ONTK6Qc+kZOGUtPH8nSZIkXV/X1MjkzJkzdbqRSa7F\nIasnJUmS6ohramSi1+sZPnx4TcRVK3ItdtmLiSRJUh1RaQmuQ4cOPPfcc3Tv3h2r1Up2dnZNxFUr\ncq2yBCdJklRXVFiCs9vtfPnll7z//vtotVrMZjObNm1Cr9fXZHw1Ktcih8qRJEmqKyoswfXu3Zuj\nR4/y/PPP88EHHxAaGjDW/w0AABHISURBVFqnk5vN4aLI7pIlOEmSpDqiwhLc3//+d7744gtSU1MZ\nOXIkQoiajKvGefqhlPfgJEmS6oQKS3ATJ05kw4YN/9/evcdGVed9HH9PZwqddorT2vJgVrm11Ejr\n2q3EbXaX+gdUiBZxjZdyaYmwumg2SAQssoBdmUCxazAY8BIUk4qmDSIxRoMQI83DzYRStVUga4BE\n2bhAy7NMp2Vu5/kDOgoUrZbpzDnn8/qrMyedfvoL6YfzmznfQ1VVFR988AFtbW3U19dz9OjRwcw3\naHrHdGVpi1JExBJ+9kMmd9xxB/X19ezcuZMRI0b0635wZtR58QxOg5ZFRKyhX9fBAQwbNoyqqiq2\nb98ezzwJ03snAb0HJyJiDf0uOKvr7B20rC1KERFLUMFd1BEIMdSVgjtVSyIiYgX6a35R7xQTh+ZQ\niohYggruIk0xERGxFhXcRZpiIiJiLSq4izp0JwEREUtRwQGGYehOAiIiFqOCA7qCEYIRQ2dwIiIW\nooJD18CJiFiRCo4fxnTpDE5ExDp+9o7ev0Y0GqW2tpYjR44wZMgQfD4fo0aNih3fvXs3GzZsAGD8\n+PE8++yznD9/niVLlnDmzBkyMjJYu3Yt2dnZ8Yh3hd4xXZpDKSJiHXE5g9u1axfBYJDGxkYWLVpE\nXV1d7Jjf76e+vp5XXnmFpqYmfvOb39DZ2ck777xDQUEBb7/9Nvfddx8bN26MR7Q+9d5JwKsPmYiI\nWEZcCu7gwYNMnDgRgOLiYtra2mLHDh06REFBAWvXrmXmzJnk5OSQnZ19yfeUlZWxb9++eETrU6du\nlSMiYjlx2aL0+/14PJ7YY6fTSTgcxuVy0dnZyYEDB9i+fTvp6enMmjWL4uJi/H4/mZmZAGRkZHDu\n3LkrXtfjGYrL5RxwPqczBa83Pfa4K2LgGerif3I8P/FdiXV5ZrMwY24zZgZz5jZjZlDuwTSQzHEp\nOI/HQ1dXV+xxNBrF5brwo7xeL7feeiu5ubkATJgwga+//vqS7+nq6mLYsGFXvK7ff/6a5PN60zl7\nNhB7/P3ZAFlu1yXPJZvLM5uFGXObMTOYM7cZM4NyD6b+ZM7Nzezz+bhsUZaUlNDc3AxAa2srBQUF\nsWNFRUUcPXqUjo4OwuEwn3/+Ofn5+ZSUlLB7924Ampubuf322+MRrU8Xpphoe1JExEricgZXXl7O\nnj17qKysxDAMVq9ezebNmxk5ciSTJk1i0aJF/OUvfwFg6tSpFBQUcNNNN1FTU8OMGTNITU3lhRde\niEe0PnUGQtzoTRu0nyciIvEXl4JLSUnhueeeu+S5vLy82Nf33HMP99xzzyXH3W4369evj0ecn9UR\nCFJ0Q9+nuCIiYk62v9A7ahj8X3dI18CJiFiM7Qvuvz1hIoYuERARsRrbF9wPcyh1BiciYiW2L7iO\ni2O6NIdSRMRabF9wsSkmbm1RiohYie0LriOgOwmIiFiR7QuuMxDEAVynQcsiIpaigusOcZ07FVeK\nI9FRRETkGlLBBULanhQRsSAVXCBIlrYnRUQsx/YF1xHQFBMRESuyfcF1dutOAiIiVmTrggtHovy3\nJ6z34ERELMjWBXe2W2O6RESsytYFF7vIWx8yERGxHFsXXGxMl96DExGxHFsXXEe3Bi2LiFiVrQtO\nt8oREbEu2xecM8VB5lBXoqOIiMg1ZvuCy3Kn4nBoDqWIiNXYuuA6AkG9/yYiYlG2LrjObo3pEhGx\nKlsXXEdAY7pERKzK1gV3VoOWRUQsy7YF1xOKEAhF8GqKiYiIJdm24Do1h1JExNLicgFYNBqltraW\nI0eOMGTIEHw+H6NGjYod9/l8tLS0kJGRAcDGjRuJRCJMmTKFgoICACZPnsycOXPiEQ/40RxKvQcn\nImJJcSm4Xbt2EQwGaWxspLW1lbq6Ol5++eXY8fb2djZt2kR2dnbsub1791JRUcGKFSviEekKnYEL\nY7p0BiciYk1x2aI8ePAgEydOBKC4uJi2trbYsWg0yokTJ1i5ciWVlZVs3boVgLa2Ntrb25k9ezYL\nFizgP//5TzyixfwwaFkFJyJiRXE5g/P7/Xg8nthjp9NJOBzG5XIRCASYPXs2jzzyCJFIhOrqaoqK\nihg7dixFRUX84Q9/4P3338fn87F+/fpLXtfjGYrL5RxwPqczhW7jwtejR1xHhglGdTmdKXi96YmO\n8YuZMbcZM4M5c5sxMyj3YBpI5rj8Zfd4PHR1dcUeR6NRXK4LP8rtdlNdXY3b7QagtLSUw4cPM3ny\n5Nhz5eXlV5QbgN9//prk83rTOXkmwFBXCsHAeUIX7yqQzLzedM6eDSQ6xi9mxtxmzAzmzG3GzKDc\ng6k/mXNzM/t8Pi5blCUlJTQ3NwPQ2toa++AIwPHjx5k5cyaRSIRQKERLSwuFhYUsX76cHTt2ALBv\n3z4KCwvjES2msztIdrrmUIqIWFVczuDKy8vZs2cPlZWVGIbB6tWr2bx5MyNHjmTSpElMmzaNhx56\niNTUVKZPn864ceNYtGgRy5Yt45133sHtduPz+eIRLUZTTERErM1hGIaR6BD9derUuWvyOl5vOhUv\n/S+5niGs+3PRNXnNeDPj1gKYM7cZM4M5c5sxMyj3YEq6LUoz6AwENcVERMTCbFlwhmHoTgIiIhZn\ny4Lznw8Tihh6D05ExMJsWXBnujTFRETE6uxZcP4LBacpJiIi1mXLguvoPYNza4tSRMSqbFlwvVuU\nXp3BiYhYlq0LLkuXCYiIWJZNC+48nqFOhrhs+euLiNiCLf/Cd/iDZOsSARERS7NnwQWC2p4UEbE4\nWxbcGX9QlwiIiFicPQuuSwUnImJ1tiu4SNSgMxDUmC4REYuzXcH9tydE1IBsvQcnImJptiu4jkAI\n0JguERGrs13Bne1WwYmI2IHtCu6HMzi9ByciYmW2K7jOgG6VIyJiB7YruI5ACIcDrktTwYmIWJnt\nCq4zECIrfQjOFEeio4iISBzZr+C6Q5pDKSJiA/YruECQ6z0qOBERq7NdwXUEQlyfoYITEbE62xXc\nUFcK+cM9iY4hIiJx5orHi0ajUWprazly5AhDhgzB5/MxatSo2HGfz0dLSwsZGRkAbNy4kVAoxOLF\ni+np6WH48OGsWbMGt9t9zbO9MaOY67Mz6DrXc81fW0REkkdczuB27dpFMBiksbGRRYsWUVdXd8nx\n9vZ2Nm3aRENDAw0NDWRmZrJx40YqKip4++23GT9+PI2NjfGIRlqqk1Sn7U5cRURsJy5/6Q8ePMjE\niRMBKC4upq2tLXYsGo1y4sQJVq5cSWVlJVu3br3ie8rKyti7d288oomIiE3EZYvS7/fj8fzwPpfT\n6SQcDuNyuQgEAsyePZtHHnmESCRCdXU1RUVF+P1+MjMzAcjIyODcuXNXvK7HMxSXyzngfE5nCl5v\n+oBfZzCZMTOYM7cZM4M5c5sxMyj3YBpI5rgUnMfjoaurK/Y4Go3icl34UW63m+rq6tj7a6WlpRw+\nfDj2PWlpaXR1dTFs2LArXtfvP39N8nm96Zw9G7gmrzVYzJgZzJnbjJnBnLnNmBmUezD1J3Nubmaf\nz8dli7KkpITm5mYAWltbKSgoiB07fvw4M2fOJBKJEAqFaGlpobCwkJKSEnbv3g1Ac3Mzt99+ezyi\niYiITcTlDK68vJw9e/ZQWVmJYRisXr2azZs3M3LkSCZNmsS0adN46KGHSE1NZfr06YwbN47HH3+c\nmpoampqayMrK4oUXXohHNBERsQmHYRhGokP016lTV74v92tY9TQ9GZkxtxkzgzlzmzEzKPdgSrot\nShERkUQz1RmciIhIf+kMTkRELEkFJyIilqSCExERS4rLZQLJ6OcGQCez++67Lzbl5cYbb2TNmjUJ\nTnR1n3/+Of/85z9paGjgxIkTLF26FIfDwbhx43j22WdJSUnO/1P9OHd7ezvz589n9OjRAMyYMYO7\n7747sQEvEwqFWLZsGd999x3BYJDHH3+c/Pz8pF7vvjKPGDEi6dc6EomwfPlyjh07htPpZM2aNRiG\nkdRr3Vfmc+fOJf1a9zpz5gz3338/b7zxBi6X69evtWETO3bsMGpqagzDMIxDhw4Z8+fPT3Ci/unp\n6TGmT5+e6Bj98tprrxkVFRXGgw8+aBiGYfz1r3819u/fbxiGYaxYscL4+OOPExnvqi7P3dTUZLz+\n+usJTvXTtm7davh8PsMwDKOjo8O48847k369+8pshrXeuXOnsXTpUsMwDGP//v3G/Pnzk36t+8ps\nhrU2DMMIBoPGE088Ydx1113Gv/71rwGtdfL8lyPOfmoAdDI7fPgw3d3dzJ07l+rqalpbWxMd6apG\njhzJSy+9FHvc3t7OHXfcAST3AO3Lc7e1tfHpp58ya9Ysli1bht/vT2C6vk2dOpUnn3wy9tjpdCb9\neveV2QxrPXnyZFatWgXAyZMnycnJSfq17iuzGdYaYO3atVRWVjJ8+HBgYH9HbFNwVxsAnezS0tKY\nN28er7/+Ov/4xz9YvHhx0uaeMmVKbOYogGEYOBwO4OoDtJPB5bl/+9vf8vTTT7NlyxZuuukmNmzY\nkMB0fcvIyMDj8eD3+1mwYAELFy5M+vXuK7MZ1hrA5XJRU1PDqlWrmDJlStKvNVyZ2QxrvW3bNrKz\ns2MnIzCwvyO2KbifGgCdzMaMGcO9996Lw+FgzJgxeL1eTp06lehY/fLjffKrDdBORuXl5RQVFcW+\n/uqrrxKcqG///ve/qa6uZvr06UybNs0U6315ZrOsNVw4s9ixYwcrVqzg/PkfBr8n61rDpZn/9Kc/\nJf1av/vuu+zdu5eqqiq+/vprampq6OjoiB3/pWttm4L7qQHQyWzr1q2xG8Z+//33+P1+cnNzE5yq\nf8aPH8+BAweACwO0J0yYkOBE/TNv3jy++OILAPbt20dhYWGCE13p9OnTzJ07lyVLlvDAAw8Ayb/e\nfWU2w1pv376dV199FbhwNxSHw0FRUVFSr3Vfmf/2t78l/Vpv2bKFt956i4aGBm655RbWrl1LWVnZ\nr15r20wy6f0U5dGjR2MDoPPy8hId62cFg0GeeeYZTp48icPhYPHixZSUlCQ61lV9++23PPXUUzQ1\nNXHs2DFWrFhBKBRi7Nix+Hw+nM6B388vHn6cu729nVWrVpGamkpOTg6rVq26ZHs7Gfh8Pj766CPG\njh0be+7vf/87Pp8vade7r8wLFy6kvr4+qdc6EAjwzDPPcPr0acLhMI8++ih5eXlJ/W+7r8w33HBD\n0v+7/rGqqipqa2tJSUn51Wttm4ITERF7sc0WpYiI2IsKTkRELEkFJyIilqSCExERS1LBiYiIJSX/\nlc4iFnTgwAEWLlxIfn5+7LmsrCzWr18/oNddunQpd999N2VlZQONKGJ6KjiRBCktLWXdunWJjiFi\nWSo4kSRSVVXFmDFjOHbsGIZhsG7dOnJzc6mrq+PgwYMAVFRUMGfOHI4fP87y5csJhUKkpaXFyrKx\nsZFNmzbh9/upra3l5ptv5sknn8Tv99PT08OSJUv4/e9/n8hfU2RQqOBEEmT//v1UVVXFHt95553A\nhbFyzz33HFu2bOHVV1/lj3/8I99++y1NTU2Ew2FmzpxJaWkpL774Io899hhlZWV8+OGHsdmChYWF\nPPHEE2zbto1t27Yxa9YsTp8+zZtvvsmZM2c4fvx4In5dkUGnghNJkL62KHfv3k1paSlwoeg++eQT\nRowYwYQJE3A4HKSmpnLbbbfxzTffcOzYMX73u98BxG5c+cEHH8RmDObk5NDT08O4ceOYNWsWTz31\nFOFw+JJSFbEyfYpSJMn03quwpaWF/Px88vLyYtuToVCIQ4cOMWrUKPLy8vjyyy8BeP/992loaACI\n3Vqk15EjR+jq6uK1116jrq4udp8wEavTGZxIgly+RQnQ09PDe++9x5tvvonb7eb5558nKyuLzz77\njIcffphQKMTUqVMpLCzk6aefZuXKlbz88sukpaVRX19Pe3v7FT9n9OjRbNiwge3bt5OamsqCBQsG\n61cUSSgNWxZJIr0T1M1wpwuRZKctShERsSSdwYmIiCXpDE5ERCxJBSciIpakghMREUtSwYmIiCWp\n4ERExJJUcCIiYkn/D2iH+BoeuWz0AAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 504x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure(figsize=(7,4))\n",
    "plt.plot([i for i in range(len(train_acc))], train_acc, label = 'training accuracy')\n",
    "plt.plot([i for i in range(len(train_acc))], val_acc, label = 'validation accuracy')\n",
    "plt.title('Accuracy during training for the CNN on Task A')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend()\n",
    "plt.grid()\n",
    "plt.show();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAbgAAAEPCAYAAAAwBdF+AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDMuMC4yLCBo\ndHRwOi8vbWF0cGxvdGxpYi5vcmcvOIA7rQAAIABJREFUeJzs3XlcVPX+x/HXrMAwrLKoKCgoLqAi\nWq5orllqasu95tZi164tt3sztczSDNe0ulpmpW2WXsv85W5ppbhnmjvghvuCbLIzzMz5/YFOEi4o\nDMPA5/l48GDmfM98z3sO43w861elKIqCEEIIUcWoHR1ACCGEsAcpcEIIIaokKXBCCCGqJClwQggh\nqiQpcEIIIaokKXBCCCGqJClw1dTOnTvp06dPhS7z2WefZdmyZXf0mgMHDvCvf/2r3DJ88MEHbNiw\n4Y5f169fPzIzM285z+uvv862bdvuNlox2dnZDBw4kN69e/PTTz/ddT/jx4/n4MGDAAwdOpR169aV\nOduvv/7K0KFD6devH7179+bf//43Fy5cAIo+V40bN2br1q3FXjNp0iTmzJkDwKuvvkqvXr3Izc0t\nNk/Lli05e/ZsmfP91SeffEK/fv3o168fLVu2pGvXrrbnp0+fvuP+tm3bRr9+/Uo9f2xsLJGRkSQn\nJ9/xskTZSIETlVqzZs2YPXt2ufW3c+dOzGbzHb9u+fLleHp63nKeyZMn0759+7uNVkx8fDypqams\nXr2anj173nU/27ZtozwvdV25ciXvvPMOsbGxLF++nFWrVtGkSROGDRuGyWQCQKfTMXbsWNLS0m7a\nz7lz55g8eXK55bqVESNGsHz5cpYvX05kZCRjxoyxPQ8ODrbrsvPy8li5ciU9e/bkm2++seuyREla\nRwcQjpeVlcVbb71FQkICKpWKmJgYXn75ZbRaLbNnz2b9+vXodDp8fHyYOnUqAQEBN51+vUuXLvHq\nq6+SnJxM7dq1SU1NtbU1atSI7du34+vrW+z50aNHmTx5MgaDgZycHMaMGcP06dNZtWoVr776Kkaj\nkcTERC5evEijRo2YPn067u7ubNq0iZkzZ6JWq2nSpAnbtm1j0aJF1KlTx7bMb775hoMHDzJjxgw0\nGg0///wzGRkZnDlzhvvuu49HH32USZMmkZOTw+XLl2ncuDHvv/8+Li4utnwbN25k/fr1qNVqTp06\nhaurK9OnTycsLIyhQ4cyePBgIiMjefLJJ+ncuTP79u0jMzOT0aNH06NHD/Ly8pgwYQL79u3Dw8OD\nBg0aADBt2jRbzhMnTjBu3DguXbpEv379WLJkCVu2bOGDDz7AarXi7u7Oa6+9RvPmzZkzZw579+4l\nOTmZRo0aMXPmTFs/7733HsnJybzyyivMmDEDgJ9//pkFCxaQkpJCu3btiI2NRa1Ws2fPHmbOnEle\nXh5qtZoXXniBLl26lPisvPfee7z99tuEhIQAoFKpGDFiBLVq1bIVuJCQEJo3b864ceOYN2/eDT9z\nw4YNY/ny5fz444/cf//9t/x8Xrx4kYkTJ3Lu3DkURaF///4888wznD179qbr+U5s2LCB+fPnYzKZ\nSEtL45FHHuHFF18kOzub1157jdOnT6NWq2nWrBlvvfVWsdf+9ttvjB07lvfee4+oqKgSfa9atYrQ\n0FCGDRvGP//5T0aOHImrq+sd5RNloIhqaceOHUrv3r0VRVGUMWPGKG+//bZitVqVgoIC5emnn1Y+\n/vhj5fz580p0dLRSUFCgKIqiLFiwQFm/fv1Np//Vc889p7z33nuKoijKyZMnlaioKOX7779XFEVR\nwsPDldTUVNu8157v2LFDady4sXL27NkSOceOHav8/e9/VwoKChSTyaT0799fWbp0qZKWlqbce++9\nSnx8vKIoirJs2TIlPDxcOXPmTIlMQ4YMUdauXWvr74knnrC1TZs2Tfnhhx8URVEUk8mk9OnTR1m3\nbl2xfN9//73SqlUr5cKFC4qiKMqkSZOUMWPGFOv7zJkzSnh4uPLLL78oiqIo69atU+677z5FURRl\n5syZyssvv6xYLBYlKytL6du3rzJ27Nhb/n2OHTumtG/fXjl9+rSiKIqybds2pUOHDkpWVpYye/Zs\n5f7771cKCwtv8FdWlC5duij79++35Rs5cqRiNpuV3NxcpUOHDsquXbuUjIwMpWfPnrb1dfHiRaVT\np07KuXPnivWVlpamhIeHK7m5uTdc1vW5c3JylJ49eyoLFy5UFEVR3nrrLWX27Nm29T5//nxl8+bN\nyr333qucP39eURRFiYqKuuHfbPDgwcpnn32mKIqiZGZmKn379lVWrVp1y/V8M9f//RVFUSwWizJo\n0CDbuj1//rzSuHFjJSMjQ1m6dKkyYsQIRVEUpbCwUHnttdeU06dPK1u3blUeeughZevWrUqPHj2U\nxMTEmy5vwIAByqJFixRFUZSePXsqS5YsuWU+Ub5kF6UgLi6OIUOGoFKp0Ov1DBw4kLi4OAIDA2nc\nuDEDBgxg+vTpNGnShO7du990+l9t27aNhx9+GCj6X32bNm1KladWrVoEBQXdsC0mJga9Xo9OpyM8\nPJwrV67w+++/ExYWRuPGjQEYMGAARqOxVMtq1aqV7fHo0aPx9fXl008/ZeLEiSQnJ5c4TgQQERFB\nzZo1AWjatClXrlwpMY9Op6Nz5862eTIyMgDYtGkTjz76KGq1GqPRyIABA26bcceOHbRt25a6desC\n0K5dO3x9fW3H1qKiotBqS7cz5sEHH0Sj0eDm5ka9evVITU1l7969XL58meeff55+/foxYsQIVCoV\niYmJxV6rVhd9XVit1tsux2Aw8O677/Lee+9x5MiRG87TsWNHBgwYwOjRo2/aZ25uLnv27GHw4MEA\neHh48PDDDxMXFwfcfD2Xllqt5uOPP2bfvn188MEHTJ8+HUVRyM/P55577iE+Pp5hw4Yxf/58hg8f\nbvsbnD9/npEjR9KzZ0/Cw8Nv2Pe+ffs4evQoDz74IFD0ufzqq6/uKJ8oGylwAqvVikqlKvbcbDaj\nVqv5+uuvmTp1Kt7e3kyZMoUZM2bcdPpfqVSqYsd/bvYlfG3X1jUGg+GmWa/fvXOtf41GU+I407Uv\n49u5flkvv/wy3377LUFBQTz55JNERETc8PjVjTL8lU6ns2W4ft1qtdpi85cm51//PgCKotiOJd5q\nff3V9X+Da9ktFgthYWG241LLly9nyZIldOzYsdhrvby8qFevHvv27SvR70svvURCQkKxaREREYwc\nOZJRo0ZRUFBwwzwvv/wyOTk5N92VabVaS6zfa59PuPl6Lq3s7GwGDBhAQkICERERjB071vZ5Cg4O\nZv369TzzzDNkZmbyxBNPsGnTJttyP//8c7777jsOHDhww74XLVqERqNhwIABdO3alUWLFnHs2DG2\nbNlyxznF3ZECJ+jYsSNff/01iqJgMpn49ttvad++PQkJCfTp04ewsDCeffZZnnzySQ4cOHDT6X8V\nExPDkiVLgKL/8e7cudPW5uvra3vNqlWrypQ/OjqakydP2r5gf/zxRzIzM2/4hafRaG56ksmWLVt4\n/vnnbf/j3rdvHxaLpUzZ/qpz5858//33WK1W8vLyWLVq1W2/mNu1a8eWLVs4c+YMANu3b+fChQu0\naNHitsu71fu9JioqilOnTrFr1y6g6ASX+++/n0uXLpWY94UXXmDy5MmcOnUKAIvFwty5c0lISCA0\nNLTE/MOHD8fPz48VK1bccNl6vZ5Zs2bx2WefkZ+fX6LdaDTSokUL2wkaWVlZ/PDDD+V2Mk9SUhJ5\neXm89NJLdOnShe3bt2M2m7FYLCxcuJA33niDmJgYxowZQ9u2bTl8+DAA/v7+REdH88orrzBmzJgS\n2TMyMli7di3z58/nl19+4ZdffiEuLo7evXvz5Zdflkt2cXtykolg/PjxxMbG0rdvXwoLC4mJieGf\n//wner2eBx54gEceeQSDwYCrqyvjx4+ncePGN5z+VxMmTOC1117jgQceoGbNmrZdiNeWOWnSJDw9\nPWnfvj3+/v53nd/b25t3332XsWPHolariYyMRKvV4ubmVmLerl278u6771JYWFii7T//+Q/PP/88\nBoMBo9HIPffcc1enkd/Ks88+y6RJk+jbty8eHh7UqFHjticdNGjQgAkTJvDCCy9gsVhwdXVl3rx5\neHh43HZ5PXr0YPTo0UycOPGm8/j6+jJ79mxmzJhBQUEBiqIwY8aMYifoXNO3b18UReHll1/GbDZT\nUFBAREQEX375JXq9vsT8KpWK6dOn89BDD910+aGhoYwdO/aGnyGAmTNnMmnSJJYtW4bJZKJv3748\n/PDDnDt37rbv/3aaNm1Kx44deeCBB9DpdDRu3JjQ0FBOnz7NgAED2LVrF71798bV1ZWgoCAGDx5s\n2zUM8Nhjj7F+/Xreeecd3njjDdv0ZcuW0aRJE1q3bl1sec899xx9+/bl+PHjhIWFlTm/uDWVcqP9\nK0I4kezsbObOncuLL76Im5sbhw4d4tlnn2Xz5s13tdvKnlavXo3RaKRz585YrVZefPFFOnTowKBB\ngxwdTYgqR7bghNMzGo3odDoeffRRtFotWq2W999/v9IVN4CGDRvy5ptv2rYi27Rpw2OPPeboWEJU\nSbIFJ4QQokqSk0yEEEJUSVLghBBCVElS4IQQQlRJTnWSyeXLWeXSj9HoQnb2jS88raycMTM4Z25n\nzAzOmdsZM4Pkrkilyezvf+NLZqrlFpxWq3F0hDvmjJnBOXM7Y2ZwztzOmBkkd0UqS2a7bMFZrVYm\nTpxIYmIier2e2NhY293H4+PjmTJlim3evXv38uGHHxIZGckrr7xCfn4+AQEBTJ069YYX6gohhBCl\nYZctuA0bNmAymViyZAmjRo0qNhRIkyZNWLhwIQsXLmTQoEH07NmTTp06MXfuXPr06cOiRYto2rSp\n7RZPQgghxN2wS4HbvXs3MTExQNF97q6/tc01ubm5zJkzh9dff73Eazp16lRuIyMLIYSonuyyizI7\nO7vYcCXXbvh6/Z3Mly5dSq9evWwDXmZnZ9vurefu7k5WVskTSoxGl3LZh6zRqPH2Lv0d2CsDZ8wM\nzpnbGTODc+Z2xswguStSWTLbpcAZjUZycnJsz61Wa4mhUlauXMns2bNLvMbV1ZWcnBw8PT1L9Fte\nZ/94exvIyCg5zldl5oyZwTlzO2NmcM7czpgZJHdFKk3mCj2LMjo62jYg4d69e0sMCJiVlYXJZKJW\nrVrFXnNtrKW4uLhiA1EKIYQQd8ouW3A9evRg69atDBw4EEVRmDJlCp9//jnBwcF069aNpKSkEiM2\njxw5krFjx/Ltt9/i4+PDrFmz7BFNCCFENeFUN1sujwu9//N/B+ncOID+TQLKIVHFccZdC+CcuZ0x\nMzhnbmfMDBWbu6CggJ9+Wkvfvv1LNf+aNSvx9PSkY8fOJdq8vQ3MmTOXVq1a07Rp5F3lWbNmJadO\nnWTkyBfv6vV3qtLtoqzM8gotfL+n7AMlCiFERUhLS2Xlyh9KPf+DD/a9YXG7ZujQJ++6uDkbp7pV\nV3loE+LD3C0nSc0xUcO95AjEQghxM6sPXWLFwYtotWrMZmu59PlQZE16RwTetP2rrz7j5MkkPv/8\nU6xWKwcP7icvL49XX32DdetWk5BwmNzcXOrVq8+4cRNYsOBjatSoQXBwPb755it0Oi0XLpyna9ce\nvPTSi0yePJFu3XqSlpbK9u1bKSjI59y5swwe/AQPPtiXw4cP8u67MzAYDPj4+KDXu/D66xNvmG3x\n4q/5+eef0Gg0tGjRkuee+xf79+/lgw/eR6vV4uHhwYQJsaSkpDBlyltotVo0Gg3jx7+Fv7/996JV\nuy24e0N8ANh1OsPBSYQQ4vaGDXuaevXq89RT/wAgJKQ+8+Z9hr+/Px4eHrz//lzmzfuMQ4cOcPly\ncrHXXrp0gdjYGcyb9zmLFn1Vou+cnGxmzHifadPe5euvvwBg5sypjBs3gdmz51G7dp2b5jp+/Bi/\n/LKeefM+Y968zzh79gxbt25m8+ZNdO7chQ8++ITevfuRmZnFrl07adSoMe+/P5dhw54mKyuz/FbQ\nLVS7LbjGAUa83HTsPJVOLyc7DieEcKzeEYH0jgh06LHD4OCi2x66uLiSnp7OhAnjMBgM5OXlYTab\ni80bGtrANsq9i4trib4aNCg6wz0gIBCTyQRASkoKoaFhALRo0ZKff/7phjlOnTpJREQz2yVgLVpE\nkZR0nKFDn+Krrz7jpZdG4u8fQNOmkfTp049vvvmSUaNexN3dyLPPPl8+K+M2qt0WnEatol2oL7+d\nSseJzq8RQlRTKpUaRflzd6harQJgx46tJCdf4q23pjBixPMUFOSX+E5TqW7Xd8kZAgICSUo6AcCh\nQwdu+tqQkHocPnwQs9mMoijs3fsHdeuGsH79Wh58sA9z5nxM/fqhrFixjC1bNtGiRUv++9+P6NKl\nG99882Vp336ZVLstOIAOYX6sO3SJk2l51K/hXFf1CyGqFx8fHwoLzcydOxsXFxfb9CZNIvjiiwWM\nGPEker2e2rWDSEm5XObljRo1lqlTJ+HmZkCn0970WFlYWAO6du3OyJHDURSF5s1b0KnTfRw+fIjY\n2IkYDAa0Wi1jxryOoihMmvQGGo0GtVrNiy++XOacpVHtLhMAyFKg67txjOoSxsDooNu/oBKQ06kr\njjNmBufM7YyZoWrn/v77b+natQc+Pj588slcdDqd7fifI5TlMoFquQVX18dAXW9Xdp5Kd5oCJ4QQ\nFcHX15eXX34eNzcDRqPxpmdQOoNqWeCg6GzKNYcvUWixotNUu0ORQghxQ126dKdLl+6OjlEuqu03\ne5sQH/IKrRy4UDGnqwohhKhY1bbAta7rjVoFO0/J9XBCCFEVVdsC5+GqJaKmB7+dSnd0FCGEEHZQ\nbQscFB2HO3wxi8z8QkdHEUIIUc6qdYFrE+KDVYHfz1xxdBQhhCizF14YwalTJ1mzZiVbtmwq0d65\nc8wtX79p06+kpFwmNTWFmTOnlSnLo4/2paCgfAapvlvVusA1q+WBQaeR3ZRCiCrldiMK3Mx33y0m\nJyeHGjX8eOWVV+2QrGJV28sEALQaNdF1vdgpBU4IUQouCUtxjf8fGq0GL7OlXPrMbzKQgsaP3rR9\n3LjRPPbYQFq2bEV8/CG+/HIBb7wxiWnTYsnOzuLKlQz69h3AgAF/9nFtRIG+fQcwY8ZkkpJOEBRU\nh8LCosMxJ04cY86c97BaFbKzs/j3v18hKyuLY8eOEBv7Jm+88TaxsRP45JMv2LVrB5988hEuLi54\nenrx2mtvcvRoYomRCp54YvgN81+4cJ5p097GbDajUql46aVXaNgwnMmTJ3Lu3FlMJhOPPz6Ebt16\n8vHHH7Jnz+9YrVZ69Lifv/1tUJnWbbUucFC0m3LLiTTOXckjyMvN0XGEEKKYvn37s3btKlq2bMWa\nNavo23cAZ8+epXv3nnTu3JWUlMu88MKIYgXumh07tmEymfjkky+4ePEiGzf+DEBS0gleeOE/hIU1\n4Kef1rFmzUrGjh1PgwbhjB49Dp1OB4CiKMyYMYW5c+fj7x/At98u5ssvF9C+fUcuXbrAF18sprCw\nkP79e920wH344fs8+ujfiYm5j6NHE5k27W3mzJnHnj2/M3/+QlQqFb/9tgOAH39cwwcffIKfnz9r\n1qws87qr9gWu7dXhc3aeyuDh5lLghBA3V9D4UQoaP4q3t4ErFXSrrjZt2jF37n/JzLzC/v1/8O9/\nv0J6ehrffruITZt+xWBwLzGKwDVJScdp0iQCgJo1a1KzZk0A/PwC+OKL+bi4uJCbm4u7u/sNX5+R\nkYHB4G67H2VUVEs+/ngu7dt3vO1IBdecPHmSFi2iAWjYsBHJyZcwGNz5z3/GMGPGZHJzc+jZ8wEA\nJk6czMcff0Bqaipt27a/uxV2nWp9DA4gxNeNAKNejsMJISoltVpNly7dmTlzGjEx96HRaFi8eCGR\nkc1588236dq1+01HRgkJqcehQ/sBSEm5THJy0Xhx//3vOwwf/izjx79FWFgD2+vVajVW658jF3h7\ne5Obm0NKSgoAe/fuoW7dYOD2IxVcU69ePfbv/wOAo0cT8fWtQUpKComJ8UydOpMZM97no49mYzKZ\n+PXXn5k4cQqzZ89j7dpVXLx44c5X2HWq/RacSqWiTYgPm46nYrEqaNSl/KsJIUQF6d37If72t378\n73//B0CHDp2YOXMqP/20Fi8vLzQajW08t+vFxNzH/v37+Mc/nqBmzVp4exftserZ8wFefXUUvr6+\n+PsHcOVK0Q0vIiObExs7gTFjXgeKvh/HjHmd118fjVqtwsPDk3HjJnLixLFSZ3/++X8zfXosixd/\njdls5rXX3qBGjRqkpaXy1FODcHMzMHDgEPR6PZ6enjz55CA8PDy45562BAbWLNN6q5ajCfz17tQ/\nxiczfk0CXwyKIqKWZ7kso7xV5buXVzbOmBmcM7czZgbJXZHKMppAtd9FCXBPiDcgt+0SQoiqRAoc\n4GvQE+7vLpcLCCFEFSIF7qo2IT7sP59Jrql8rm0RQgjhWFLgrmoT4oPZqvDHWbltlxBCVAVS4K5q\nEeSJXqOS3ZRCCFFFSIG7ylWnISpIbtslhBBVhRS467St50NSajaZp/eiyktzdBwhhBBlUO0v9EZR\n0FxJQnd2K8POb+Qpl634rMzGVLsNVwZ87+h0Qggh7lL1LHDZl3BJXI/u7Fb0ZzejyT4PgMG9FqtV\nrfA3utDu/E9ok/djDmju4LBCCCHuRrUrcB7r/4XuyDJ0gNXFi8Kg9uRGP09h3RgsXvVZvSaB+FPn\n2Kjbitv+BWR1/6+jIwshhLgLdilwVquViRMnkpiYiF6vJzY2lpCQEFv7pk2b+PDDDwFo2rQpEyZM\nAKBTp07Uq1cPgKioKEaNGlXu2UzBndAFNSPTrw1mvwhQa4q13xviw48Jl7kUOYCaR5eQ3e51FPeA\ncs8hhBDCvuxS4DZs2IDJZGLJkiXs3buXadOm8dFHHwGQnZ3NO++8w1dffYWvry+ffvop6enpZGVl\nERERwbx58+wRyaag0aO4eRsw3+TeZm2uDp/zmbkn46zfkLThQ36tOZzsAgs5JjM5Jgs5BWayTRba\nhHgzvG3IDfsRQgjhWHYpcLt37yYmJgYo2hI7ePCgre2PP/4gPDyc6dOnc+bMGR577DF8fX3ZsWMH\nly5dYujQobi6uvLaa68RGhpqj3i3FOjhQkN/dz5NyKGNLooWZ75jwbEYLGoXjC5a3PUa3PUa8got\nfLrtFL2bBlLT8+ZjIQkhhHAMuxS47OxsjEaj7blGo8FsNqPVaklPT2fnzp388MMPGAwGBg8eTFRU\nFP7+/owYMYIHHniA33//ndGjR/P998XPYjQaXdBqNX9d3B3TaNR4extu2r5weBsuXsnHP2Us/isH\nsu/hLDQtHyo2z9n0XLq9F8eK+MuMub9RmTPdzu0yV1bOmNsZM4Nz5nbGzCC5K1JZMtulwBmNRnJy\ncmzPrVYrWm3Rory9vWnWrBn+/v4AtG7dmvj4eLp06YJGo7FNu3TpEoqioLpuVL3s7IJyyXe74Rc0\nQJBBC3U7YPZthOa3eWTU619shD+jCro09ON/u84wpGVtDPqyF96yZK6snDG3M2YG58ztjJlBclek\nSjdcTnR0NHFxcQDs3buX8PBwW1tkZCRHjhwhLS0Ns9nMvn37aNCgAR988AFffvklAAkJCdSuXbtY\ncXMIlYq85k+jSzmE7vyOEs2PRweRVWBm1aFLDggnhBDiVuyyBdejRw+2bt3KwIEDURSFKVOm8Pnn\nnxMcHEy3bt0YNWoUzzzzDAC9evUiPDycESNGMHr0aDZt2oRGo2Hq1Kn2iHbH8hs9jPuOabjtX0Bh\nULtibc1rexJR04Mlf5zj0ahaqB1dkIUQQtjIiN6l4L59Gm57PiRt6FasnsHF2n5KSOb11QnM6h9B\np7Aa5ZLvRpxx1wI4Z25nzAzOmdsZM4PkrkiVbhdlVZPXbBioNbjt/6JEW9eGfgQY9Szefbbigwkh\nhLgpKXClYDXWpiCsN67xi1GZsou1aTVq/t4yiN/PXCExOfsmPQghhKhoUuBKKa/5cNSmLFwSvivR\n1r95TVy1ahbvOeeAZEIIIW5EClwpmWtGUxjYErf9n4FiLdbm6aqjb2RNfkpIJiXH5KCEQgghricF\n7g7kNR+O9koS+lO/lmgbGB2E2aKwdO95ByQTQgjxV1Lg7kBBWG8s7oG47Ztfoi3Yx42Oob58v+8C\n+YUWB6QTQghxPSlwd0KjIz/yyaIx5FITSzQPalWHjLxC1sUnOyCcEEKI60mBu0N5EYNRNC647V9Q\noq1VXS8a+ruzeM85nOjyQiGEqJKkwN0hxc2X/EYP45r4Par89GJtKpWKQa2COJGay85T6TfpQQgh\nREWQAncX8po/jcpSgOuhb0q09WwUgK9Bxze75ZIBIYRwJClwd8FSowmmOjEY9s0vceG3Xqvmsaja\n7DiZzonUnJv0IIQQwt6kwN2lnDajUeel4PbHRyXaHmlRC71GxWLZihNCCIeRAneXzDWjyW/QF8Pe\nT1DnXCzW5mPQ80DTQNbGJ5ORW+ighEIIUb1JgSuDnLavgtWMYefMEm2PRwdRYLby/X658FsIIRxB\nClwZWL1CyGv2BK4J36JJTSjWFubnTtt6PizZc57sArODEgohRPUlBa6Mclu/hKIz4r5tcom25zrW\nIyOvkE+3n3JAMiGEqN6kwJWR4upDbqsXcTn9K7ozW4q1NQn0oF+zmiz54zxJqc41yKAQQjg7KXDl\nIK/5U1g86uC+LbbESAPPdayHQadh5i/H5O4mQghRgaTAlQetKzltxqBLOYjLkWXFmnwMep5tH8Jv\npzP49ViqgwIKIUT1IwWunBSE96fQvxnuO2aAOa9Y2yNRtWng5877G4/LSANCCFFBpMCVF5WanPbj\n0WSfLxoU9TpatYpXuoZxIbOAr3adcVBAIYSoXqTAlaPCOh0oCOmGYfcHqPLSirW1qutNj0b+fLXr\nLOev5DsooRBCVB9S4MpZTvvXURXmYPj9/RJtL3UORQW8t/F4xQcTQohqRgpcObP4hpPfZCBuB79C\nnZFUrC3Qw4Wn2waz8VgqO09eenx1AAAgAElEQVTKcDpCCGFPUuDsIPfeUaDWY9wxrUTb4FZ1qOPt\nysxfj1Fosd7g1UIIIcqDFDg7sLoHktvyWVyOr0Z7cXexNr1Wzcv3hXEyLY8lf8h9KoUQwl6kwNlJ\nbtQ/sRgC8PhlNNoLu4q1xYTVoEN9X+ZvP0VKdoGDEgohRNUmBc5e9O5kd3kHdX46PssG4LVyMNpL\nf9iaX+4Shsli5YPNSbfoRAghxN2SAmdHpnrdSB26jez249EmH8BnaV88Vz+J9vIBgn3cGNSqDqsP\nJ7Pv3BVHRxVCiCpHCpy96dzIa/lP0oZuI7vtq+gu7MLn2wfwXDOcZxvkEGDUM2X9UY6l5Dg6qRBC\nVClS4CqIojeS1+oF0oZuJ+feUejObaP2sl78n98neGUdYdCXvzNxXSIXMuUicCGEKA9ae3RqtVqZ\nOHEiiYmJ6PV6YmNjCQkJsbVv2rSJDz/8EICmTZsyYcIECgoKGD16NKmpqbi7uzN9+nR8fX3tEc+h\nFBdPcu/5D3nNnsJt36fU2jef/1P/RLpHTdYfbcKnic3xi+jO4+0j8DHoHR1XCCGcll224DZs2IDJ\nZGLJkiWMGjWKadP+vB4sOzubd955h3nz5vHtt98SFBREeno6ixcvJjw8nEWLFtG/f3/mzp1rj2iV\nhuLqTW6b0aQN3U5W5ykY6kbxiMsu5mj/yxsJD5H/WQ+OLH0d86mtYDE5Oq4QQjgdu2zB7d69m5iY\nGACioqI4ePCgre2PP/4gPDyc6dOnc+bMGR577DF8fX3ZvXs3zzzzDACdOnWq8gXuGsXNl/zIYeRH\nDgOrGW3yPnITN+B25GeaXlyIdtWXmNRuqBr2QNV+EorBz9GRhRDCKdilwGVnZ2M0Gm3PNRoNZrMZ\nrVZLeno6O3fu5IcffsBgMDB48GCioqLIzs7Gw8MDAHd3d7Kyskr0azS6oNVqypxPo1Hj7W0ocz92\n4RuDsXEM8BYHTpzml3XLCLy8lUcT16FK2s2hLp8T1TwKg94uf7pyV6nX9U04Y2ZwztzOmBkkd0Uq\nS2a7fEsajUZycv48K9BqtaLVFi3K29ubZs2a4e/vD0Dr1q2Jj48v9pqcnBw8PT1L9JtdThdFe3sb\nyMjILZe+7KmOrx9DH/8Hv516jFm7f+H5C68Tse5Rnlw9Bn1QNO3r+9K+ni8hvm6oVCpHx70hZ1nX\n13PGzOCcuZ0xM0juilSazP7+HjecbpdjcNHR0cTFxQGwd+9ewsPDbW2RkZEcOXKEtLQ0zGYz+/bt\no0GDBkRHR7Np0yYA4uLiaNWqlT2iOR2VSkWbej6MGT6Ugr+vwGAw8j/9ZEIydvLexhM89sXv9J//\nG9M2HGVrUhqKojg6shBCVAoqxQ7fiNfOojxy5AiKojBlyhTi4uIIDg6mW7durF69mgULFgDQq1cv\nRowYQV5eHmPHjuXy5cvodDpmzZpl28q75vLlkrst74Yz/y9GnXMRr5XD0KQf4UzbqaxVd2ZbUjq/\nnU4nr9BK76YBjOsRjl5bOa4AceZ17WycMbczZgbJXZHKsgVnlwJnL1LgijKrTFl4rv0H+rNbyG77\nKnnRz1NoVfjitzN8su0ULet4MeOhpni76Ryc2vnXtTNxxtzOmBkkd0WqdLsohX0peg+u9PmK/Ib9\nMO6YhnHzG+hUCv9oF0Lsg405eCGT4Yv3cjo9z9FRhRDCYaTAOSuNnqwec8iNeha3A1/g+dNIMOdz\nf5MAPnqsOZn5Zp5e9Ad7zmY4OqkQQjiEFDhnplKT0+ENsju8icvxNXitHAyFubQI8uLzQVH4GHQ8\n/90B1hy+5OikQghR4aTAVQF5USPI7PEBugu78Nj4KigKdbzdWPB4FFF1vJiwNpF5W0/KGZZCiGpF\nClwVURDen9x7Xsb1yDJcDy8CwNNVx5yHI+kXWZMFO07zxpoECsxWBycVQoiKIQWuCslt/S9MdTtj\n3PwmmsuHANBq1LzesyEvxNTnx4TLPPfdfjLzCx2cVAgh7E8KXFWiUpPZYzZWVx+81o1AVZBZNFml\n4ol76zKtbxMOX8zi7R+PyO5KIUSVJwWuilHcapB5/0eos87i8esrcF0h6xbuzwsx9dl4LJWl+y44\nMKUQQtifFLgqyFzrHnLavYbL8TW47V9QrO3xVkG0r+/D+xuPc/RytoMSCiGE/UmBq6Lyop6loP79\nuG+LRXtxt226WqViQq9GeLrqGLcqnrxCiwNTCiGE/UiBq6pUKrK6zsLqXgvPH59DlZ9ua/I16Jn0\nYCNOpeUx65fjDgwphBD2IwWuClNcvcnsNQ917mU8NrwEyp+XCNwT7MNTbeqy/OBFfkpIdmBKIYSw\nDylwVZw5oAXZHSfgcuoX3PYUHyX9H+3r0by2J1PWH+Vshty3UghRtUiBqwbyI4eR37Af7jtnoDu3\n3TZdq1YR27sxapWK8asTKLTIReBCiKqjVAUuOTmZY8eOkZSUxLhx44iPj7d3LlGeVCqy75uOxase\nHj89jzrnz3tT1vJ0ZXzPhhy6mMVHW046LqMQQpSzUhW4sWPHkpKSwnvvvUeHDh2YMmWKvXOJcqbo\njWT2+hi1KRvPVcNQmf4cW69ruD+PtKjFwt/Psv1kmgNTCiFE+SlVgTObzdxzzz1kZmbSu3dvrFbZ\nleWMLDWacOWBT9CmJeK55hmwFNja/t05lDA/AxPXJpKSY3JgSiGEKB+lKnCFhYVMnTqV1q1bs2PH\nDiwWuXbKWRUG30dW11noz23FY8O/bWdWuuo0TO7dhByThQlrErDKrbyEEE6uVAVu2rRp1K9fnxEj\nRpCWlsY777xj71zCjgoaPUJ2+/G4HluJ++YJttt5hfm5M6pLGL+dzmDB9tMOTimEEGVTqgIXEBBA\nt27dyMzMJCkpCbVaTr50dnkt/0luixEYDnyO254PbdP7N6tJ74hAPtl+il+OXHZgQiGEKJtSVapX\nXnmFQ4cOMWPGDHQ6HW+++aa9c4kKkNNhPPnhAzDumIZL/BKgaOSB17o3pFktDyasTSQxWe5XKYRw\nTqUqcJmZmXTt2pVLly4xYsQITCY5CaFKUKnJ6joLU93OePw6Bv3JDQC4aNXM6BeBp6uWUT8cIlVO\nOhFCOKFSn2Ty2Wef0bRpU44dO0ZOTo69c4mKotGT2etjzH4ReP74T9uNmf3c9czqH0FGXiFjVhzG\nJCOBCyGcTKmvg0tNTeW5555j586dTJw40c6xREVS9Eau9PkKi3tNvFY9gSbtKACNAz2Y2KsR+89n\nMm3DURkkVQjhVEpV4KKjo7n33ntZsmQJgYGBNG/e3N65RAVTDH5c6fsNqHV4rRyMJuMEAN0b+fNM\n22BWHrrE4j3nHJxSCCFKr1QFbtasWSxbtgytVssPP/zAtGnT7J1LOIDVK4QrfReiMmXjs7g7hp3v\nQGEe/2gfQteGfvx30wm2JcmdToQQzqFUBW7Xrl3Mnj2bJ598kjlz5rB79+7bv0g4JbN/JOmDfqEg\n7EHcf/8vvou74Jr0IxN7hRPm5864VfGcTM11dEwhhLitUt+q69rtuaxWKyqVyq6hhGNZ3WuS1fMD\nMvp/h6Jzx2vtMwT++BRzurrjolUzavkhMvMLHR1TCCFuqVQFrnfv3jz++ONMmTKFwYMH8+CDD9o7\nl6gECoPakf63dWR3nIjuwi4arXyQpQ02kHblCq+tjMdslZNOhBCVl/ZWjbNmzbJtrQUGBvLrr7/S\npEkT0tLkOEy1odGR1+IZChr0xX3bZEKPfMx2jxWMOjuQ/yyD13uGU9PT1dEphRCihFsWuNDQUNvj\n+vXr06VLl1J1arVamThxIomJiej1emJjYwkJCbG1x8bGsmfPHtzd3QGYO3cuFouF+++/n/DwcAC6\nd+/OE088ccdvSNiH1T2QrB6zyY8YhDFuPJ8UvMf+iytY/GVPgtsNZECrUNSy61oIUYncssANGDDg\nrjrdsGEDJpOJJUuWsHfvXqZNm8ZHH31kaz906BDz58/H19fXNm3btm306dOHN954466WKSpGYe22\npP9tHa6HFxO+dz5TrnxExo6v+HVfT0K7PUftek0cHVEIIYBSHoO7U7t37yYmJgaAqKgoDh48aGuz\nWq2cOnWKN998k4EDB7J06VIADh48yKFDhxgyZAj/+te/SE5Otkc0UR7UWvIjh5I1eCPp/b4lPaAd\nffJX0GJ1D7IWPoL6+DqwypBKQgjHuuUW3N3Kzs7GaDTanms0GsxmM1qtltzcXIYMGcJTTz2FxWJh\n2LBhREZGEhoaSmRkJO3bt2fFihXExsYye/bsYv0ajS5otZoy59No1Hh7G8rcT0WqtJl9umOM7E76\npdPsXPZfWqcup8a6Zyhwr4W29dOo7hmOt7e3o1PekUq7rm/DGXM7Y2aQ3BWpLJntUuCMRmOx+1Va\nrVa02qJFubm5MWzYMNzc3ABo27YtCQkJdO/e3TatR48eJYobQHZ2QYlpd8Pb20BGhnNdy1XZM6tc\n/Gj7+NtsOfoC2zf8j4ey1tJx02Ssv31M7n3TMIX2cnTEUqvs6/pmnDG3M2YGyV2RSpPZ39/jhtPt\nsosyOjqauLg4APbu3Ws7cQTg5MmTDBo0CIvFQmFhIXv27CEiIoLx48fz448/ArB9+3YiIiLsEU3Y\nWceGgYx8+nn+r8kcHiiYSkKeB15rn8G66gVUBZmOjieEqEZUih3uoHvtLMojR46gKApTpkwhLi6O\n4OBgunXrxqeffsq6devQ6XT069ePxx9/nDNnzjBu3DigaCsvNjaWgICAYv1evpxVLvmq6v9iKptD\nFzJZefAcofFzeVa9nDS1L79Hvk1kuwdx05V9V7O9OOO6BufM7YyZQXJXpLJswdmlwNmLFDjnygxF\nuU9duMKe336l7aE3CFbO8bXSi4PhL9GnRX0aBRpv30kFc+Z17Wy5nTEzSO6KVJYCZ5djcEJcz8tN\nR5fOPVHadeT0z5MYcuJrko7s4z8HR1IQEMVTbYLp0tDP0TGFEFWMXY7BCXEjKr0BtwemkdFvCXWM\nKpa5TGRQzkJeX7Gf9zeekFt/CSHKlRQ4UeEK63TgyuMbKGj8GEPN37HBeyrrdh/ihaX7Scs1OTqe\nEKKKkAInHEJx8SS72yyu9PqYYHMSG71jybmQyNCFezh0Qc62FEKUnRQ44VCmsN5k9PsWd1U+Kw1v\nEU0C/1iyj+UHLjg6mhDCyUmBEw5nrhlN+iPLUbn5MleZxHN++4n96ShT1x/FZLY6Op4QwklJgROV\ngtWrHhmPrsAS0IL/XJnKx/W3sGz/eZ79dh+XssrnDjZCiOpFCpyoNBRXHzIeWkx+WB/uvzCX9Q1X\ncDIlm2Ff72H3mQxHxxNCOBkpcKJy0bqSdf9ccqOepeGZJWyu+yn+ejPPf7efhbvO4ET3JRBCOJgU\nOFH5qNTkdHiDrE6xeF/YyArjVB4K1TA7LokxKw6TlW92dEIhhBOQAicqrfxmT5L5wHz0GUeYmf4i\n8yKPsPlEKsO+2UNicraj4wkhKjkpcKJSM9XvScbDP2A11qLXsYn8Xutd6hQmMXzxXlYcuOjoeEKI\nSkwKnKj0zP6RZDyygqz7puGVfYzF1jG867GY93/ay6R1ieQXyujhQoiSpMAJ56DWkB8xhLQhm8lv\nOogH81aw3Tgaffx3DF+0hzPpeY5OKISoZKTACaeiuPqQfd9UMh5bjUuNeszSz2N69qtM/noZvx5N\ncXQ8IUQlIgVOOCVzQHMyHllOVpeZNHNJ5lv1a6jX/os1yz6lIPOSo+MJISoBGQ9OOC+VmvymAykI\n7YXrzpn0PbwElwtxsPAtcjzDUAd3xBTUjsLabVEMMt6cENWNFDjh9BRXb/I6x5LXcQJHDm7jj21r\naJZxkHbZ3+F18EsAzD7hFAa1w1SnPabgLqAzODi1EMLepMCJqkOjI7xFZwLD2zNtw1GGH7nII4GX\n+XfoRWqk7sIlcSluB79E0bpiCr6PgrDemOp1R9HfeLh7IYRzkwInqhwvNx1T+jRh9WFf3vn5OKsz\n6vBa98fp+aAPuou7cDm+Bv2JtbicWIei1mMK7nS12PVAcfV2dHwhRDmRAieqJJVKRZ+ImkQFefHm\nmgReX53AliYBjOl2L8ag9hAzCe3FPbgcX43L8TW4nNyAotZSWKcDqmaPQNADoHNz9NsQQpSBSnGi\nu9devpxVLv14exvIyMgtl74qijNmhsqR22xV+GzHKRbsOI2Hi5bQGgZqeblS29O16LeHC2Hmo9S+\nuB63E2vQZJ7C6upLXvOnyWv2BIqrj0Pzl1ZlWNd3yhkzg+SuSKXJ7O9/48MMsgUnqjytWsWI9vVo\nW8+XZfsvcD4jj91nrrA2K5nr/3enUcUQYOzGg34nGWL5gfq/zcSw50Pymj5OXosRWD3rOOw9CCHu\nnBQ4UW00r+1J89qetueFFiuXsgo4fyWfC5n5nL+Sz/nMAuJSw1mQ/Bzhqr6M1f/EfQe+xO3AlxQ0\nfIjcliOx+DV14LsQQpSWFDhRbek0aup4u1HHu/ixNm9vA3tPpPDzkRCmHWnM+Mz+PK1dy+Aj6/A9\n8n9k1e6E5Z7nKQxqByq5V4IQlZUUOCFuoJ6vgeFtQxjeNoRTaU345WgrhiQMpm36cp46tw7/838n\nX+eDUr8rhSFdMAV3dppjdUJUF1LghLiNEF8DT7UJ5qk2wZxOv5eFCSPJObCcZvm/cd+Rn/A+8j2K\nSo05IApTSBdMwV0wBzSXrTshHEwKnBB3INjHjaHtGmJpM4pfj6bw9x1JuKUepK/hIH1zD+H/27u4\n/zYLq1sNTHU7YfGoi+LiheLihdX16m8Xb9s0RecOKpWj35YQVZIUOCHugkatonsjf7qG+7H5eCgL\ndkTx9qVsGhnzeaX+GToof+BydisueSmoFOtN+1HUOixe9TH7NcXsF4HZPwKzXwSKW40KfDdCVE1S\n4IQoA7VKRecGfnQKq8H2k+ks2HGafxxwpYZ7BENbv8ojzQNxU/JQFVxBXXAFVcEVVAUZRY/zr6Au\nSEeTdhTdhd9wPfqDrV+LeyBmvwgsNYoKn6luRznGJ8QdkgInRDlQqVS0r+9Lu3o+/H4mg892nOb9\nTSdY8sc5XuwUSvfwOlg96966j/x0tCmHr/4cQptyCP2ZOFRWM4rGhYIGfciLHIY5MFp2awpRCnYp\ncFarlYkTJ5KYmIheryc2NpaQkBBbe2xsLHv27MHd3R2AuXPnUlhYyCuvvEJ+fj4BAQFMnToVNze5\nVZJwLiqVinuCfbgn2IffT2fw7sbjjFsVz7dBnrzcJYwmgTe/sbPi6kNhnQ4U1unw50RLAdqUw7gm\nLMUl8XtcE7/HXKMpeZFDKQgfgKI3VsC7EsI52eU0rw0bNmAymViyZAmjRo1i2rRpxdoPHTrE/Pnz\nWbhwIQsXLsTDw4O5c+fSp08fFi1aRNOmTVmyZIk9oglRYVoHe7NwSDTjejTkVFoeT3z9B5PWJZKS\nYyp9JxoXzIEtye48mdQnd5N13zQUlQqPTa/h+0VrjJvGoUmNt9+bEMKJ2aXA7d69m5iYGACioqI4\nePCgrc1qtXLq1CnefPNNBg4cyNKlS0u8plOnTmzbts0e0YSoUBq1igHNa7Fs+D0Mbl2HtfHJPLJg\nF5/vPE2B+eYnn9yQ3p38iCFk/G0d6Y8sxxTaC9f4Jfj+rwfeywag2r8YlSnbPm9ECCdkl12U2dnZ\nGI1/7jrRaDSYzWa0Wi25ubkMGTKEp556CovFwrBhw4iMjCQ7OxsPj6LdN+7u7mRllbyxstHoglar\nKXM+jUaNt7dzDXjpjJnBOXPbI7M3MKFfJE90rM/0dYnM3XKSFYcuMfb+RtzfNBDVnR5T84mBxjGY\nc9NQ71+Eds8XqFY+Tw2dAaVRb6zN/o5SrzOoy/7vxZ6c8fMBkrsilSWzXQqc0WgkJyfH9txqtaLV\nFi3Kzc2NYcOG2Y6vtW3bloSEBNtrXF1dycnJwdPTs0S/2dkF5ZKvqt5RuzJyxtz2zOytUTG1d2MG\nRAby7sbjvPi/vQR6uNAx1JeY0Bq0DvbGRXsnO1ZcofHT0OgpfLIPULh7ES5HV6I9+B0WQyAF4f3J\nb/RIpb1/pjN+PkByV6SyjCZgl12U0dHRxMXFAbB3717Cw8NtbSdPnmTQoEFYLBYKCwvZs2cPERER\nREdHs2nTJgDi4uJo1aqVPaIJUSncG+LD10Nb8dYDjWgSaGTN4Uv8+/8O0v3DbYz64RA/7L9Ayp38\nh06lQqnbluz7ppH65G6u9PoYc0AL3PYvwHdJT3z+1xO3Pz5GnZEEt7guT4iqxC7jwV07i/LIkSMo\nisKUKVOIi4sjODiYbt268emnn7Ju3Tp0Oh39+vXj8ccfJyUlhbFjx5KTk4OPjw+zZs3CYCi+WSrj\nwTlXZnDO3I7IXGC2sudsBpuPp7H5eCoXs4qKW5NAIzGhNegUVoPwAPdb7sq8UW5VXiouR1fgmrgU\nXfI+AKw6dyw1GmOu0QRzjcZYrv5WXLzs9wbvILMzkNwVpyxbcDLgqZNwxszgnLkdnVlRFI6n5LL5\nRCqbj6dx8EImClDL04XODfy4r0ENWgR5oVUXL3a3y61JP47uwk40KfFoU4t+1AVXbO0WYxBmvyZY\nfMMxe4dh8Q7F4h2K4uprt+vuHL2u75bkrjgy4KkQVYhKpaKBvzsN/N15qk0wabkmthxPY+OxFJbt\nO8//9pzDy1VLTFgN7mvgR5sQb1x1tz+ZxOIThsUn7M8JioI65wLalHg0aQlorxY+/elNqKyFttms\nLl5YvOrZCp7FOxSzT0MsPg1Bo7PHKhCiXEiBE6KS8zXoeahZTR5qVpNck4UdJ9PYeCyVTcdSWXXo\nEq5aNe3q+zIgug6taxnRaUp5aF2lwmqsjclYG+p1+3O61Yw66yzajBNoMk6gyUhCk3EC3fnfcD3y\nf7bZFI0LZv9ICgNaYA5ogTkgCot3fRlFQVQasovSSThjZnDO3M6S2WyxsvvsFTYdS2XTsRSSs034\nGnT0iajJgOY1SwzkWj4LzUNz5RTa1AS0yfvQJe9De/kAKnMeAFa9J2b/ZpgDW2D2a4bFIwirsRZW\nQ8ANL1lwlnX9V5K74sgxuDtUVf/IlZEz5nbGzFZF4WBKLgu3nWTz8VQsCrQJ8ebh5rXoFFYDbWm3\n6u5q4RY06Uf/LHjJ+9CmHC62m1NRabC6B2B1r4XVWAuLsRZWY21c/eqQm1dYdGanYgVFAcV6dQQG\nq+2MT0XvYRtmyOp6dbghvafDrvNzxs8IOGduOQYnRDWnVqno1NCf5v7uJGcVsOLgRX44cJGxK+Px\nNeh4KLIm/ZvXJMjLDlt1ag2WGo2x1GhMQZO/F02zFKBJP44m+wLq7Auocy7YHmtSE9Cf+sW21Vfy\nitfSUVChuHiiuHhjdfXG6hGExat+0fBD3qFYvOujuPnJjamrMdmCcxLOmBmcM7czZoaSuS1Whe0n\n01i27wJbk9JQFGgV7E2XBkWXHdT0dHVcWEVBZcrES5NFVlY+qNQoqIqO36lUgPq6xwoqU3bRkEP5\nGVeHHspAlZ9+ddihDNQF6aivnEaTdQaV1WxbjFVnxOJdVPQs3vWxGgJQXH2KtgJdfbC6+KC4et/x\nwLN2/4woVjDnoyrMRWXOR2UpAEvBXx4XXH1sQmUtRNG6omgNKLqrP1q3q78NRe9P54a3j9HpPtuy\ni/IOOeMXmDNmBufM7YyZ4da5L2bms+LgRTYkppCUVjRPk0AjnRvUoHOYH2F+hju/XVg5KPd1bSks\nOkHmSlLRyTFXrv5knESddeamg88qat3VLUEfFI0OlaJQtItUKfpBubq7tOi3RqPBYuVqES76UVSa\nP4uySnO1YKpsv4sKOMWmoVhRmfOuFrJrv4uKmj0oLh5Y9V5Yr40o7+J59fHV3b8unihaA2jdUHRu\nKBpX22+uf67Wg0YPaq3dt5ClwN0hZ/wCc8bM4Jy5nTEzlD73ybRc4o6lsvFYKgcuZAIQ5OVK5wZF\nlx00r+2JRl0xxa5C17Wl8OpWXwbq/PSix/kZtmlFz9PBav7zTNCrBUvhWuEqmq7XazEVFKJSLNcd\nP7RePX5osR1LLCqIVwvk9Y+VopKnqFRFBaXYFpdbscdo3a5unbmAxgVF44Kidb36WI+icQG1tmjr\nzpyLqjAXrhXKwjzbNJUpG1dVLoWZaTcegPcuiqqCCq5l0OhQNHpQFz1XNHrQXs2rcSma7y+5Cxo/\nitm/2S2XIcfghBClVs/XQL17DQy7ty4p2QXEnUhj07EUvtt7nkW7z1HDXc8DTQLoExFImJ+7o+OW\nH40OxT0Ai3sAljJ25e1tIMsJ/xOkv1VuSwGq/CtFW5LmfFSW/KLjpOb8P6fZnhegspqKdo9a/vr7\nujZz0S5UdWFa0e5UcwGqq/NgKcDiFXLbAlcWUuCEqMb8jC483LwWDzevRXaBmW1JafyUcJnFe87x\n9e9naVrTgz4RgfRs5I+Xm1zUXaVpXFDcA3CaXXqlIAVOCAGA0UVLz8YB9GwcQFquiXXxyaw8eIkZ\nPx/jvY3H6RzmR5/IQNqE+JS4TZgQlZEUOCFECb4GPYNa1eHx6CCOJOew8tBF1sUns+HIZfzc9TzY\nNIAHmgTSwL8K7cIUVY4UOCHETalUKhoFGmkU2IB/dQplS1IaKw9e5Jvfz/LVrrM08HOnV5MA7m/s\n79jLDoS4ASlwQohS0WvVdG3oR9eGfqTlmtiQeJl18cl8sDmJDzcn0bKOF72aBNAt3A9PVzleJxxP\nCpwQ4o75GvT8rWUQf2sZxNmMPNbFJ7M2Ppkp64/yzi/H6FDfl67hftT2dKWGu54a7nrcSjHigRDl\nSQqcEKJM6ni78Uy7EIa3DSb+UjZr45P5KSGZjcdSi81n0GnwdddRw1BU8HwNOuoFeNC6tkfVuhxB\nVBpS4IQQ5UKlUtG0pgdNa3rwUudQTqTkkJJjIi3XRGpO4dXfRT9JabnsPmPiyr4LADT0d6dX4wB6\nyrE8UY6kwAkhyp1WrRYc5HMAAA/gSURBVCI8wEj4beYzazV8/9tpfkxIZs7mJOZsTqJlkCe9mgTQ\nNdwfb7n2TpSBFDghhMP4GV34e3QQf48uOpb3Y0Iy6+KTmbrhGDN+OU67ej70aORPDYMejVqFWg0a\nlQqN+uqPSoVarUKnVlHH263CbjEmnIMUuP9v796Doq7/PY4/l73AwoK7sCB4YbnqJGbqQaWU7deY\naWZZTRfNg57RI5nTTx0Tb2lRMIrZHB0dx3SynDHnjB41x/FXWf3yUir40ySVvMtFRVGuugvIwn7P\nH+jmBYtfhnvh/Zhh4Pvly/LaN8y++X74fj8fIYRH6GLUMyHFwvgB0Zy6auebm//L++lcZau+PsKg\nY3iPjjyX1JGY0MA2Tiu8gTQ4IYRHUalUdI8w0D3CwN+tsZy+YqfO0USTotDkVHAqCk1O7ti23Whk\n5+kK1v3rPGsPnKdnVDDP9ejIEJlirF2TBieE8Fh+N280b42Rj0ZRbm+eYuwfBWUs+ucZ/mfXWazx\nYTzXoyOPx5jadmVz4XGkwQkhfIY5SMd/JndhzH80TzG2/dcydhy/wj9PlRMaqCW5q9F1dtg9woAx\nUM7ufJk0OCGEz/ltijEDU62x7C2s4pvjVzhSeo1vT151Hdcx2P9mswtyNb2Owf5uWfxV/PWkwQkh\nfJpG7de8cnlCGADVdQ5OXbFx8ubbqSt2fjpXgfPmOjGRwf6kxocxKC6U5K5GdBoZ1vRW0uCEEO2K\nUa+lv8VEf4vJta/O0cTpq3ZOlNnIK65i27HL/F9+KXqtHwMsJlLjwxgYG0pYkM6NycW/SxqcEKLd\n02vV9OoUQq9OIbzWpxP1jiYOna/hx3MV/Hi2gl1nKlABSVHBpMaFMbRXJ6L0avxkKNOjSYMTQoi7\nBGjVDIwLZWBcKLMGJ3Dqqp0fz1bw47lKVu4tYuXeIkIDm88EH49pPhs0y9mdx5EGJ4QQv+P2+/L+\n+3EL5fYGjl6x88PxMvKKmi9egeb5NFMsJgbEmOjduQP+8r87t5MGJ4QQ/wZzkI6X+hh5KtaEU1E4\nfcXO/qJK8oqr+N+fL7Lu4AX8NX7Em4MI1KkJ1KrRa/0I1KnRa9UE3XwfqFMTFRJAf4sJjUwx1iak\nwQkhxJ/kd9vtCP81IJrahiZ+vlBNblEVxVV11DuauFznoNbRRG1DE3WOJuoczjseI8Kg48VHoxj5\naCQRwf5ueia+qU0anNPpJDMzk5MnT6LT6cjOzsZisdxzTHp6OoMHD2b06NEoioLVaiUmJgaA3r17\n884777RFPCGEaBOBOjWD4sIYFBd232OcikK9w0mto4mjpdfYcuQSq/cXsya3mNT4MF7qFUVKjKnV\nF7BU1jZQaXcQExYoZ4J3aZMG9/3339PQ0MCGDRvIz88nJyeHlStX3nHM0qVLqampcW2XlJSQlJTE\nJ5980haRhBDCI/ipVM1Dlzo1TyWaeSrRzIXqOrYevcy2o5fZdaaCTh0CeOnRSJ7vGem6NaGmzsG5\nilrOlts5V1HLuQo758prqapzABCkU5Pc1Uh/i5H+0SYsofp2f8N6mzS4Q4cOkZqaCjSfiR07duyO\nz3/zzTeoVCqsVqtrX0FBAWVlZaSlpREQEMCcOXOIi4tri3hCCOFRuhj1vJ0ay5tPWNh5upwtRy6x\n4qciVu0r5pGOwZReq6fC3uA6PkinJjYsEGt8GHHmQIx6LfkXazhQXM3us80rqUcYdPSzmOgfbaR/\ntBGzof0Nf7ZJg7PZbBgMv02QqlaraWxsRKPRcOrUKbZv386yZctYsWKF65jw8HDS09N59tlnOXjw\nIBkZGWzevPmOxzUY/NFo1A+cT632w2j0ruU0vDEzeGdub8wM3pnbGzND2+Z+LczAaykxnL1qY8PB\n8/xyoQZrNzOJEcF0izCQGGEgqkPAPWdnb9x8X1JZy/5zFew7W8G+cxX8o6AMgMQIA6mJZgbFh9Ev\nJpQA7YO/lj4MD1LrNmlwBoMBu93u2nY6nWg0zd9q69atlJWVMW7cOC5evIhWq6Vz587069cPtbq5\n4MnJyZSVlaEoyh0/RJvtxl+Sz2gMpLq69i95rIfFGzODd+b2xszgnbm9MTM8nNxhWj8mP25p4TMK\nNTV19/26ED8YmhDG0IQw11WeB0qqyCuu4ou8Ej7bW4S/xo8+nTuQEmMiJcZEXFigxw5ntqbW4eHB\nLe5vkwbXt29fdu7cyfDhw8nPz6dbt98Wrp85c6br4+XLl2M2m7FarSxevBij0cjEiRM5ceIEnTp1\n8tiCCyGEN7j9Ks+0fl3xD/RnZ8Elcouq2F9UydLd52B383Bmc7MLpX+00WfW0GuTBjdkyBD27t3L\nqFGjUBSFBQsW8PnnnxMdHc3gwYNb/Jr09HQyMjLYvXs3arWahQsXtkU0IYRot/Q6NU/EhvJEbCgQ\nz+Vr9ewvqiK3qIofTpez7VgZKuCRyGAGWIwMsJjo1SkErZeuo6dSFEVxd4jWunr1+l/yON44LOKN\nmcE7c3tjZvDO3N6YGXwzd6NToeDSNfKKq8grrqbg0jWaFNBr/ejbpfnqzAGWhz+c6XFDlEIIIbyL\nxk/FY5078FjnDqQ/AbYbjRwsqSavuIoDJdXsLawEINygI94cRFSIP1EhATffmj82G3QeNQG1NDgh\nhBD3MPhr+Fuimb8lmgEoraknr7iKQ+erKamq40SZjeqb9+DdovFTERniT2RIAF2NAUSbAok26Yk2\n6enSIQDNQx7qlAYnhBDiD3XqEMBLvaJ4qVeUa1+do4nL125Qeq2ey9fquXTtBpdq6rl0rZ6dpyuo\nrrvsOlatgs5GvavhRZv0DE4MxxjYdhe0SIMTQgjxp+i1zTecx4a1fJ9aTZ2D89V1lFTVUVxVR0ll\nHcVVtfyrpJobjU5Ka+r5u7XtJvSQBieEEKJNdNBr6aDX0jMq5I79TkWhwt5AaGDbrqEnDU4IIcRD\n5adSEf4Qpg7zzpsbhBBCiD8gDU4IIYRPkgYnhBDCJ0mDE0II4ZOkwQkhhPBJ0uCEEEL4JGlwQggh\nfJJXrSYghBBCtJacwQkhhPBJ0uCEEEL4JGlwQgghfFK7mYvS6XSSmZnJyZMn0el0ZGdnY7FY3B2r\nVV588UWCg5tXrO3SpQsLFy50c6L7++WXX/j4449Zt24dxcXFzJ49G5VKRWJiIu+//z5+fp75N9Xt\nuQsKCpg0aRIxMTEAjB49muHDh7s34F0cDgdz587l4sWLNDQ08NZbb5GQkODR9W4pc2RkpMfXuqmp\niXnz5lFYWIharWbhwoUoiuLRtW4p8/Xr1z2+1rdUVFTw8ssv89lnn6HRaP58rZV2YseOHcqsWbMU\nRVGUw4cPK5MmTXJzotapr69XRo4c6e4YrbJ69WplxIgRyquvvqooiqK8+eabSm5urqIoijJ//nzl\n22+/dWe8+7o798aNG5U1a9a4OdXv27Rpk5Kdna0oiqJUVlYqTz75pMfXu6XM3lDr7777Tpk9e7ai\nKIqSm5urTJo0yeNr3VJmb6i1oihKQ0ODMnnyZOWZZ55Rzpw580C19pw/OdrYoUOHSE1NBaB3794c\nO3bMzYla58SJE9TV1TF+/HjGjh1Lfn6+uyPdV3R0NMuXL3dtFxQU0L9/fwCsViv79u1zV7TfdXfu\nY8eOsWvXLsaMGcPcuXOx2WxuTNeyYcOGMXXqVNe2Wq32+Hq3lNkbav3000+TlZUFQGlpKWaz2eNr\n3VJmb6g1wKJFixg1ahQRERHAg72OtJsGZ7PZMBgMrm21Wk1jY6MbE7VOQEAAEyZMYM2aNXzwwQfM\nmDHDY3MPHToUjea3UW9FUVCpVAAEBQVx/fp1d0X7XXfn7tWrFzNnzmT9+vV07dqVFStWuDFdy4KC\ngjAYDNhsNqZMmcK0adM8vt4tZfaGWgNoNBpmzZpFVlYWQ4cO9fhaw72ZvaHWW7ZsITQ01HUyAg/2\nOtJuGpzBYMBut7u2nU7nHS9qnio2NpYXXngBlUpFbGwsRqORq1evujtWq9w+Tm632wkJCfmdoz3H\nkCFD6Nmzp+vjX3/91c2JWnbp0iXGjh3LyJEjef75572i3ndn9pZaQ/OZxY4dO5g/fz43btxw7ffU\nWsOdmQcNGuTxtd68eTP79u0jLS2N48ePM2vWLCorK12f/3dr3W4aXN++fdmzZw8A+fn5dOvWzc2J\nWmfTpk3k5OQAUFZWhs1mIzw83M2pWqdHjx7k5eUBsGfPHpKTk92cqHUmTJjAkSNHANi/fz9JSUlu\nTnSv8vJyxo8fT0ZGBq+88grg+fVuKbM31Hrr1q2sWrUKAL1ej0qlomfPnh5d65Yyv/322x5f6/Xr\n1/PFF1+wbt06HnnkERYtWoTVav3TtW43M5ncuory1KlTKIrCggULiI+Pd3esP9TQ0MCcOXMoLS1F\npVIxY8YM+vbt6+5Y93XhwgWmT5/Oxo0bKSwsZP78+TgcDuLi4sjOzkatVrs7Yotuz11QUEBWVhZa\nrRaz2UxWVtYdw9ueIDs7m6+//pq4uDjXvnfffZfs7GyPrXdLmadNm8bixYs9uta1tbXMmTOH8vJy\nGhsbmThxIvHx8R79u91S5qioKI//vb5dWloamZmZ+Pn5/elat5sGJ4QQon1pN0OUQggh2hdpcEII\nIXySNDghhBA+SRqcEEIInyQNTgghhE/y/DudhfBBeXl5TJs2jYSEBNc+k8nEsmXLHuhxZ8+ezfDh\nw7FarQ8aUQivJw1OCDdJSUlhyZIl7o4hhM+SBieEB0lLSyM2NpbCwkIURWHJkiWEh4eTk5PDoUOH\nABgxYgTjxo2jqKiIefPm4XA4CAgIcDXLDRs28Omnn2Kz2cjMzKR79+5MnToVm81GfX09GRkZDBgw\nwJ1PU4iHQhqcEG6Sm5tLWlqaa/vJJ58EmqeV+/DDD1m/fj2rVq1i4MCBXLhwgY0bN9LY2Mgbb7xB\nSkoKS5cuJT09HavVyldffeWaWzApKYnJkyezZcsWtmzZwpgxYygvL2ft2rVUVFRQVFTkjqcrxEMn\nDU4IN2lpiHL37t2kpKQAzY3uhx9+IDIykuTkZFQqFVqtlscee4yzZ89SWFhInz59AFwLV27fvt01\nx6DZbKa+vp7ExETGjBnD9OnTaWxsvKOpCuHL5CpKITzMrbUKf/75ZxISEoiPj3cNTzocDg4fPozF\nYiE+Pp6jR48CsG3bNtatWwfgWlrklpMnT2K321m9ejU5OTmudcKE8HVyBieEm9w9RAlQX1/Pl19+\nydq1a9Hr9Xz00UeYTCYOHDjA66+/jsPhYNiwYSQlJTFz5kzee+89Vq5cSUBAAIsXL6agoOCe7xMT\nE8OKFSvYunUrWq2WKVOmPKynKIRbyWTLQniQWzOoe8NKF0J4OhmiFEII4ZPkDE4IIYRPkjM4IYQQ\nPkkanBBCCJ8kDU4IIYRPkgYnhBDCJ0mDE0II4ZOkwQkhhPBJ/w8Q8K2tsZk4LgAAAABJRU5ErkJg\ngg==\n",
      "text/plain": [
       "<Figure size 504x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure(figsize=(7,4))\n",
    "plt.plot([i for i in range(len(train_loss))], train_loss, label = 'training loss')\n",
    "plt.plot([i for i in range(len(train_loss))], val_loss, label = 'validation loss')\n",
    "plt.title('loss during training for the CNN on Task A')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('loss')\n",
    "plt.legend()\n",
    "plt.grid()\n",
    "plt.show();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tests on official test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>id</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>15923</th>\n",
       "      <td>OFF</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27014</th>\n",
       "      <td>NOT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30530</th>\n",
       "      <td>NOT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13876</th>\n",
       "      <td>NOT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>60133</th>\n",
       "      <td>NOT</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         0\n",
       "id        \n",
       "15923  OFF\n",
       "27014  NOT\n",
       "30530  NOT\n",
       "13876  NOT\n",
       "60133  NOT"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions_off = model(test_sent_tensor_2).squeeze(1)\n",
    "predictions_to_submit = torch.round(torch.sigmoid(predictions_off))\n",
    "\n",
    "p = predictions_to_submit.detach().numpy()\n",
    "p = ['OFF' if x == 1 else 'NOT' for x in p]\n",
    "\n",
    "df_to_submit = pd.DataFrame(p)\n",
    "df_to_submit.index = data_test.id\n",
    "df_to_submit.to_csv('test_A.csv',sep=',', header = False)\n",
    "\n",
    "df_to_submit.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bayesian Optimization\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def f(lr, wd, dp, epochs=28):\n",
    "    epochs=epochs\n",
    "\n",
    "    INPUT_DIM = len(word2idx)\n",
    "    EMBEDDING_DIM = 100\n",
    "    OUTPUT_DIM = 1\n",
    "\n",
    "    #the hyperparamerts specific to CNN\n",
    "\n",
    "    # we define the number of filters\n",
    "    N_OUT_CHANNELS = 100\n",
    "    # we define the window size\n",
    "    WINDOW_SIZE = 1\n",
    "    # we apply the dropout with the probability 0.5\n",
    "    DROPOUT = dp\n",
    "\n",
    "    model = CNN(INPUT_DIM, EMBEDDING_DIM, N_OUT_CHANNELS, WINDOW_SIZE, OUTPUT_DIM, DROPOUT)\n",
    "\n",
    "    optimizer = optim.Adam(model.parameters(), lr=lr, weight_decay=wd)\n",
    "    loss_fn = nn.BCEWithLogitsLoss()\n",
    "\n",
    "    feature_train = train_sent_tensor\n",
    "    target_train = train_label_tensor\n",
    "\n",
    "    feature_valid = valid_sent_tensor\n",
    "    target_valid = valid_label_tensor\n",
    "\n",
    "\n",
    "    for epoch in range(1, epochs+1):\n",
    "   \n",
    "        model.train()\n",
    "  \n",
    "        optimizer.zero_grad()\n",
    "    \n",
    "        predictions = model(feature_train).squeeze(1)\n",
    "        loss = loss_fn(predictions, target_train)\n",
    "        acc = accuracy(predictions, target_train)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "  \n",
    "        epoch_loss = loss.item()\n",
    "        epoch_acc = acc\n",
    "      \n",
    "        model.eval()\n",
    "  \n",
    "        with torch.no_grad():\n",
    " \n",
    "            predictions_valid = model(feature_valid).squeeze(1)\n",
    "            loss = loss_fn(predictions_valid, target_valid)\n",
    "            acc = accuracy(predictions_valid, target_valid)\n",
    "            valid_loss = loss.item()\n",
    "            valid_acc = acc\n",
    "            f1 = classification_report(target_valid, torch.round(torch.sigmoid(predictions_valid)), output_dict=True)['macro avg']['f1-score']\n",
    "        print(f'| Epoch: {epoch:02} | Train Loss: {epoch_loss:.3f} | Train Acc: {epoch_acc*100:.2f}% | Val. Loss: {valid_loss:.3f} | Val. Acc: {valid_acc*100:.2f}% |Macro avg f1 {f1}% |')\n",
    "        model.eval()\n",
    "    \n",
    "        feature = test_sent_tensor\n",
    "        target = test_label_tensor\n",
    "\n",
    "    with torch.no_grad():\n",
    " \n",
    "        predictions = model(feature).squeeze(1)\n",
    "        loss = loss_fn(predictions, target)\n",
    "        acc = accuracy(predictions, target)\n",
    "        print(f'| Test Loss: {loss:.3f} | Test Acc: {acc*100:.2f}%')\n",
    "    return(valid_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialization\n",
      "-----------------------------------------------------------------\n",
      " Step |   Time |      Value |        dp |        lr |        wd | \n",
      "| Epoch: 01 | Train Loss: 0.685 | Train Acc: 56.88% | Val. Loss: 0.839 | Val. Acc: 66.87% |\n",
      "| Epoch: 02 | Train Loss: 0.834 | Train Acc: 66.75% | Val. Loss: 0.650 | Val. Acc: 67.00% |\n",
      "| Epoch: 03 | Train Loss: 0.644 | Train Acc: 67.07% | Val. Loss: 0.629 | Val. Acc: 69.87% |\n",
      "| Epoch: 04 | Train Loss: 0.625 | Train Acc: 71.58% | Val. Loss: 0.653 | Val. Acc: 63.20% |\n",
      "| Epoch: 05 | Train Loss: 0.650 | Train Acc: 64.18% | Val. Loss: 0.646 | Val. Acc: 65.47% |\n",
      "| Epoch: 06 | Train Loss: 0.644 | Train Acc: 65.94% | Val. Loss: 0.624 | Val. Acc: 71.47% |\n",
      "| Epoch: 07 | Train Loss: 0.621 | Train Acc: 71.97% | Val. Loss: 0.605 | Val. Acc: 73.93% |\n",
      "| Epoch: 08 | Train Loss: 0.601 | Train Acc: 74.36% | Val. Loss: 0.594 | Val. Acc: 72.80% |\n",
      "| Epoch: 09 | Train Loss: 0.589 | Train Acc: 72.87% | Val. Loss: 0.589 | Val. Acc: 71.40% |\n",
      "| Epoch: 10 | Train Loss: 0.584 | Train Acc: 71.47% | Val. Loss: 0.583 | Val. Acc: 71.53% |\n",
      "| Epoch: 11 | Train Loss: 0.578 | Train Acc: 71.07% | Val. Loss: 0.569 | Val. Acc: 72.73% |\n",
      "| Epoch: 12 | Train Loss: 0.565 | Train Acc: 72.37% | Val. Loss: 0.552 | Val. Acc: 73.47% |\n",
      "| Epoch: 13 | Train Loss: 0.551 | Train Acc: 74.19% | Val. Loss: 0.544 | Val. Acc: 75.13% |\n",
      "| Epoch: 14 | Train Loss: 0.544 | Train Acc: 75.25% | Val. Loss: 0.538 | Val. Acc: 75.27% |\n",
      "| Epoch: 15 | Train Loss: 0.539 | Train Acc: 75.11% | Val. Loss: 0.529 | Val. Acc: 75.27% |\n",
      "| Epoch: 16 | Train Loss: 0.529 | Train Acc: 75.42% | Val. Loss: 0.525 | Val. Acc: 74.60% |\n",
      "| Epoch: 17 | Train Loss: 0.525 | Train Acc: 75.37% | Val. Loss: 0.526 | Val. Acc: 74.33% |\n",
      "| Epoch: 18 | Train Loss: 0.525 | Train Acc: 75.11% | Val. Loss: 0.521 | Val. Acc: 74.87% |\n",
      "| Epoch: 19 | Train Loss: 0.519 | Train Acc: 75.63% | Val. Loss: 0.518 | Val. Acc: 76.80% |\n",
      "| Epoch: 20 | Train Loss: 0.517 | Train Acc: 75.74% | Val. Loss: 0.518 | Val. Acc: 76.33% |\n",
      "| Epoch: 21 | Train Loss: 0.514 | Train Acc: 75.76% | Val. Loss: 0.516 | Val. Acc: 76.33% |\n",
      "| Epoch: 22 | Train Loss: 0.510 | Train Acc: 76.38% | Val. Loss: 0.515 | Val. Acc: 75.73% |\n",
      "| Epoch: 23 | Train Loss: 0.510 | Train Acc: 76.05% | Val. Loss: 0.517 | Val. Acc: 75.80% |\n",
      "| Epoch: 24 | Train Loss: 0.509 | Train Acc: 76.27% | Val. Loss: 0.516 | Val. Acc: 75.87% |\n",
      "| Epoch: 25 | Train Loss: 0.509 | Train Acc: 76.25% | Val. Loss: 0.514 | Val. Acc: 76.87% |\n",
      "| Epoch: 26 | Train Loss: 0.505 | Train Acc: 76.86% | Val. Loss: 0.515 | Val. Acc: 76.80% |\n",
      "| Epoch: 27 | Train Loss: 0.504 | Train Acc: 77.02% | Val. Loss: 0.515 | Val. Acc: 76.60% |\n",
      "| Epoch: 28 | Train Loss: 0.500 | Train Acc: 77.43% | Val. Loss: 0.515 | Val. Acc: 76.80% |\n",
      "| Test Loss: 0.507 | Test Acc: 75.87%\n",
      "    1 | 02m22s |    0.76800 |    0.2378 |    0.0209 |    0.0040 | \n",
      "| Epoch: 01 | Train Loss: 0.715 | Train Acc: 42.60% | Val. Loss: 1.374 | Val. Acc: 66.87% |\n",
      "| Epoch: 02 | Train Loss: 1.370 | Train Acc: 66.75% | Val. Loss: 0.729 | Val. Acc: 66.53% |\n",
      "| Epoch: 03 | Train Loss: 0.723 | Train Acc: 66.71% | Val. Loss: 0.650 | Val. Acc: 66.13% |\n",
      "| Epoch: 04 | Train Loss: 0.649 | Train Acc: 66.25% | Val. Loss: 0.691 | Val. Acc: 56.20% |\n",
      "| Epoch: 05 | Train Loss: 0.690 | Train Acc: 56.22% | Val. Loss: 0.680 | Val. Acc: 56.47% |\n",
      "| Epoch: 06 | Train Loss: 0.679 | Train Acc: 57.36% | Val. Loss: 0.654 | Val. Acc: 70.80% |\n",
      "| Epoch: 07 | Train Loss: 0.654 | Train Acc: 71.35% | Val. Loss: 0.634 | Val. Acc: 71.40% |\n",
      "| Epoch: 08 | Train Loss: 0.634 | Train Acc: 71.30% | Val. Loss: 0.624 | Val. Acc: 68.87% |\n",
      "| Epoch: 09 | Train Loss: 0.623 | Train Acc: 68.70% | Val. Loss: 0.623 | Val. Acc: 67.93% |\n",
      "| Epoch: 10 | Train Loss: 0.621 | Train Acc: 68.07% | Val. Loss: 0.611 | Val. Acc: 68.60% |\n",
      "| Epoch: 11 | Train Loss: 0.614 | Train Acc: 68.77% | Val. Loss: 0.600 | Val. Acc: 71.60% |\n",
      "| Epoch: 12 | Train Loss: 0.600 | Train Acc: 71.29% | Val. Loss: 0.592 | Val. Acc: 73.20% |\n",
      "| Epoch: 13 | Train Loss: 0.594 | Train Acc: 72.67% | Val. Loss: 0.579 | Val. Acc: 72.87% |\n",
      "| Epoch: 14 | Train Loss: 0.582 | Train Acc: 72.51% | Val. Loss: 0.573 | Val. Acc: 72.60% |\n",
      "| Epoch: 15 | Train Loss: 0.574 | Train Acc: 72.43% | Val. Loss: 0.564 | Val. Acc: 73.47% |\n",
      "| Epoch: 16 | Train Loss: 0.566 | Train Acc: 72.89% | Val. Loss: 0.561 | Val. Acc: 73.47% |\n",
      "| Epoch: 17 | Train Loss: 0.561 | Train Acc: 73.64% | Val. Loss: 0.556 | Val. Acc: 73.13% |\n",
      "| Epoch: 18 | Train Loss: 0.559 | Train Acc: 73.55% | Val. Loss: 0.553 | Val. Acc: 73.53% |\n",
      "| Epoch: 19 | Train Loss: 0.555 | Train Acc: 73.54% | Val. Loss: 0.550 | Val. Acc: 73.47% |\n",
      "| Epoch: 20 | Train Loss: 0.552 | Train Acc: 73.71% | Val. Loss: 0.547 | Val. Acc: 73.73% |\n",
      "| Epoch: 21 | Train Loss: 0.548 | Train Acc: 74.24% | Val. Loss: 0.545 | Val. Acc: 74.27% |\n",
      "| Epoch: 22 | Train Loss: 0.547 | Train Acc: 74.21% | Val. Loss: 0.542 | Val. Acc: 74.40% |\n",
      "| Epoch: 23 | Train Loss: 0.541 | Train Acc: 74.49% | Val. Loss: 0.541 | Val. Acc: 74.27% |\n",
      "| Epoch: 24 | Train Loss: 0.540 | Train Acc: 74.44% | Val. Loss: 0.538 | Val. Acc: 74.47% |\n",
      "| Epoch: 25 | Train Loss: 0.532 | Train Acc: 74.65% | Val. Loss: 0.535 | Val. Acc: 75.00% |\n",
      "| Epoch: 26 | Train Loss: 0.527 | Train Acc: 75.44% | Val. Loss: 0.532 | Val. Acc: 75.47% |\n",
      "| Epoch: 27 | Train Loss: 0.521 | Train Acc: 76.23% | Val. Loss: 0.528 | Val. Acc: 75.60% |\n",
      "| Epoch: 28 | Train Loss: 0.517 | Train Acc: 75.92% | Val. Loss: 0.524 | Val. Acc: 75.73% |\n",
      "| Test Loss: 0.518 | Test Acc: 75.93%\n",
      "    2 | 02m13s |    0.75733 |    0.3014 |    0.0360 |    0.0054 | \n",
      "| Epoch: 01 | Train Loss: 0.722 | Train Acc: 37.56% | Val. Loss: 0.720 | Val. Acc: 35.40% |\n",
      "| Epoch: 02 | Train Loss: 0.721 | Train Acc: 37.60% | Val. Loss: 0.720 | Val. Acc: 35.27% |\n",
      "| Epoch: 03 | Train Loss: 0.721 | Train Acc: 37.86% | Val. Loss: 0.719 | Val. Acc: 35.33% |\n",
      "| Epoch: 04 | Train Loss: 0.721 | Train Acc: 38.05% | Val. Loss: 0.719 | Val. Acc: 35.47% |\n",
      "| Epoch: 05 | Train Loss: 0.720 | Train Acc: 38.11% | Val. Loss: 0.719 | Val. Acc: 35.53% |\n",
      "| Epoch: 06 | Train Loss: 0.720 | Train Acc: 37.96% | Val. Loss: 0.718 | Val. Acc: 35.40% |\n",
      "| Epoch: 07 | Train Loss: 0.720 | Train Acc: 37.86% | Val. Loss: 0.718 | Val. Acc: 35.33% |\n",
      "| Epoch: 08 | Train Loss: 0.719 | Train Acc: 37.78% | Val. Loss: 0.718 | Val. Acc: 35.67% |\n",
      "| Epoch: 09 | Train Loss: 0.719 | Train Acc: 38.46% | Val. Loss: 0.717 | Val. Acc: 35.67% |\n",
      "| Epoch: 10 | Train Loss: 0.718 | Train Acc: 38.22% | Val. Loss: 0.717 | Val. Acc: 35.73% |\n",
      "| Epoch: 11 | Train Loss: 0.718 | Train Acc: 38.55% | Val. Loss: 0.717 | Val. Acc: 35.80% |\n",
      "| Epoch: 12 | Train Loss: 0.718 | Train Acc: 38.33% | Val. Loss: 0.716 | Val. Acc: 35.87% |\n",
      "| Epoch: 13 | Train Loss: 0.718 | Train Acc: 38.41% | Val. Loss: 0.716 | Val. Acc: 35.93% |\n",
      "| Epoch: 14 | Train Loss: 0.718 | Train Acc: 38.55% | Val. Loss: 0.716 | Val. Acc: 36.00% |\n",
      "| Epoch: 15 | Train Loss: 0.718 | Train Acc: 38.45% | Val. Loss: 0.716 | Val. Acc: 36.13% |\n",
      "| Epoch: 16 | Train Loss: 0.717 | Train Acc: 39.29% | Val. Loss: 0.715 | Val. Acc: 36.33% |\n",
      "| Epoch: 17 | Train Loss: 0.716 | Train Acc: 39.10% | Val. Loss: 0.715 | Val. Acc: 36.47% |\n",
      "| Epoch: 18 | Train Loss: 0.716 | Train Acc: 39.37% | Val. Loss: 0.715 | Val. Acc: 36.60% |\n",
      "| Epoch: 19 | Train Loss: 0.715 | Train Acc: 39.26% | Val. Loss: 0.714 | Val. Acc: 36.80% |\n",
      "| Epoch: 20 | Train Loss: 0.714 | Train Acc: 39.72% | Val. Loss: 0.714 | Val. Acc: 37.00% |\n",
      "| Epoch: 21 | Train Loss: 0.715 | Train Acc: 38.78% | Val. Loss: 0.714 | Val. Acc: 37.00% |\n",
      "| Epoch: 22 | Train Loss: 0.715 | Train Acc: 39.81% | Val. Loss: 0.713 | Val. Acc: 36.93% |\n",
      "| Epoch: 23 | Train Loss: 0.714 | Train Acc: 39.71% | Val. Loss: 0.713 | Val. Acc: 37.00% |\n",
      "| Epoch: 24 | Train Loss: 0.714 | Train Acc: 40.31% | Val. Loss: 0.713 | Val. Acc: 37.13% |\n",
      "| Epoch: 25 | Train Loss: 0.714 | Train Acc: 39.96% | Val. Loss: 0.712 | Val. Acc: 37.07% |\n",
      "| Epoch: 26 | Train Loss: 0.712 | Train Acc: 40.51% | Val. Loss: 0.712 | Val. Acc: 37.20% |\n",
      "| Epoch: 27 | Train Loss: 0.713 | Train Acc: 40.26% | Val. Loss: 0.712 | Val. Acc: 37.20% |\n",
      "| Epoch: 28 | Train Loss: 0.713 | Train Acc: 40.15% | Val. Loss: 0.711 | Val. Acc: 37.40% |\n",
      "| Test Loss: 0.711 | Test Acc: 38.27%\n",
      "    3 | 02m23s |    0.37400 |    0.1132 |    0.0000 |    0.0042 | \n",
      "| Epoch: 01 | Train Loss: 0.696 | Train Acc: 49.98% | Val. Loss: 0.744 | Val. Acc: 66.87% |\n",
      "| Epoch: 02 | Train Loss: 0.738 | Train Acc: 66.73% | Val. Loss: 0.656 | Val. Acc: 66.73% |\n",
      "| Epoch: 03 | Train Loss: 0.651 | Train Acc: 66.88% | Val. Loss: 0.615 | Val. Acc: 69.60% |\n",
      "| Epoch: 04 | Train Loss: 0.612 | Train Acc: 69.61% | Val. Loss: 0.624 | Val. Acc: 71.40% |\n",
      "| Epoch: 05 | Train Loss: 0.622 | Train Acc: 71.39% | Val. Loss: 0.629 | Val. Acc: 69.53% |\n",
      "| Epoch: 06 | Train Loss: 0.627 | Train Acc: 69.45% | Val. Loss: 0.616 | Val. Acc: 71.40% |\n",
      "| Epoch: 07 | Train Loss: 0.614 | Train Acc: 72.15% | Val. Loss: 0.596 | Val. Acc: 73.67% |\n",
      "| Epoch: 08 | Train Loss: 0.593 | Train Acc: 74.38% | Val. Loss: 0.582 | Val. Acc: 73.47% |\n",
      "| Epoch: 09 | Train Loss: 0.579 | Train Acc: 73.91% | Val. Loss: 0.577 | Val. Acc: 72.87% |\n",
      "| Epoch: 10 | Train Loss: 0.572 | Train Acc: 72.75% | Val. Loss: 0.575 | Val. Acc: 72.60% |\n",
      "| Epoch: 11 | Train Loss: 0.570 | Train Acc: 72.29% | Val. Loss: 0.568 | Val. Acc: 72.93% |\n",
      "| Epoch: 12 | Train Loss: 0.563 | Train Acc: 72.85% | Val. Loss: 0.557 | Val. Acc: 73.47% |\n",
      "| Epoch: 13 | Train Loss: 0.552 | Train Acc: 74.18% | Val. Loss: 0.547 | Val. Acc: 74.27% |\n",
      "| Epoch: 14 | Train Loss: 0.543 | Train Acc: 75.00% | Val. Loss: 0.543 | Val. Acc: 75.73% |\n",
      "| Epoch: 15 | Train Loss: 0.540 | Train Acc: 75.18% | Val. Loss: 0.539 | Val. Acc: 76.27% |\n",
      "| Epoch: 16 | Train Loss: 0.536 | Train Acc: 75.34% | Val. Loss: 0.534 | Val. Acc: 75.87% |\n",
      "| Epoch: 17 | Train Loss: 0.532 | Train Acc: 75.30% | Val. Loss: 0.531 | Val. Acc: 75.27% |\n",
      "| Epoch: 18 | Train Loss: 0.528 | Train Acc: 75.37% | Val. Loss: 0.531 | Val. Acc: 74.40% |\n",
      "| Epoch: 19 | Train Loss: 0.528 | Train Acc: 75.20% | Val. Loss: 0.531 | Val. Acc: 74.33% |\n",
      "| Epoch: 20 | Train Loss: 0.526 | Train Acc: 75.41% | Val. Loss: 0.528 | Val. Acc: 74.33% |\n",
      "| Epoch: 21 | Train Loss: 0.523 | Train Acc: 75.26% | Val. Loss: 0.526 | Val. Acc: 74.87% |\n",
      "| Epoch: 22 | Train Loss: 0.519 | Train Acc: 75.79% | Val. Loss: 0.525 | Val. Acc: 76.00% |\n",
      "| Epoch: 23 | Train Loss: 0.520 | Train Acc: 75.62% | Val. Loss: 0.525 | Val. Acc: 76.00% |\n",
      "| Epoch: 24 | Train Loss: 0.521 | Train Acc: 75.74% | Val. Loss: 0.524 | Val. Acc: 76.00% |\n",
      "| Epoch: 25 | Train Loss: 0.519 | Train Acc: 75.70% | Val. Loss: 0.524 | Val. Acc: 74.93% |\n",
      "| Epoch: 26 | Train Loss: 0.518 | Train Acc: 75.58% | Val. Loss: 0.525 | Val. Acc: 74.80% |\n",
      "| Epoch: 27 | Train Loss: 0.518 | Train Acc: 75.70% | Val. Loss: 0.526 | Val. Acc: 74.80% |\n",
      "| Epoch: 28 | Train Loss: 0.519 | Train Acc: 75.42% | Val. Loss: 0.524 | Val. Acc: 75.07% |\n",
      "| Test Loss: 0.518 | Test Acc: 74.80%\n",
      "    4 | 02m23s |    0.75067 |    0.1391 |    0.0151 |    0.0069 | \n",
      "| Epoch: 01 | Train Loss: 0.703 | Train Acc: 49.05% | Val. Loss: 0.646 | Val. Acc: 66.87% |\n",
      "| Epoch: 02 | Train Loss: 0.649 | Train Acc: 66.57% | Val. Loss: 0.658 | Val. Acc: 66.80% |\n",
      "| Epoch: 03 | Train Loss: 0.658 | Train Acc: 66.74% | Val. Loss: 0.645 | Val. Acc: 66.87% |\n",
      "| Epoch: 04 | Train Loss: 0.644 | Train Acc: 66.88% | Val. Loss: 0.624 | Val. Acc: 67.27% |\n",
      "| Epoch: 05 | Train Loss: 0.624 | Train Acc: 67.43% | Val. Loss: 0.612 | Val. Acc: 68.60% |\n",
      "| Epoch: 06 | Train Loss: 0.614 | Train Acc: 68.81% | Val. Loss: 0.608 | Val. Acc: 71.40% |\n",
      "| Epoch: 07 | Train Loss: 0.609 | Train Acc: 71.17% | Val. Loss: 0.605 | Val. Acc: 72.00% |\n",
      "| Epoch: 08 | Train Loss: 0.607 | Train Acc: 71.50% | Val. Loss: 0.598 | Val. Acc: 72.53% |\n",
      "| Epoch: 09 | Train Loss: 0.600 | Train Acc: 72.16% | Val. Loss: 0.587 | Val. Acc: 73.33% |\n",
      "| Epoch: 10 | Train Loss: 0.590 | Train Acc: 72.70% | Val. Loss: 0.574 | Val. Acc: 73.40% |\n",
      "| Epoch: 11 | Train Loss: 0.576 | Train Acc: 73.52% | Val. Loss: 0.564 | Val. Acc: 73.07% |\n",
      "| Epoch: 12 | Train Loss: 0.564 | Train Acc: 73.52% | Val. Loss: 0.558 | Val. Acc: 73.00% |\n",
      "| Epoch: 13 | Train Loss: 0.562 | Train Acc: 73.20% | Val. Loss: 0.553 | Val. Acc: 73.27% |\n",
      "| Epoch: 14 | Train Loss: 0.555 | Train Acc: 73.45% | Val. Loss: 0.546 | Val. Acc: 73.67% |\n",
      "| Epoch: 15 | Train Loss: 0.547 | Train Acc: 74.31% | Val. Loss: 0.537 | Val. Acc: 74.47% |\n",
      "| Epoch: 16 | Train Loss: 0.538 | Train Acc: 74.50% | Val. Loss: 0.530 | Val. Acc: 75.13% |\n",
      "| Epoch: 17 | Train Loss: 0.532 | Train Acc: 75.35% | Val. Loss: 0.526 | Val. Acc: 75.53% |\n",
      "| Epoch: 18 | Train Loss: 0.529 | Train Acc: 75.12% | Val. Loss: 0.523 | Val. Acc: 76.13% |\n",
      "| Epoch: 19 | Train Loss: 0.530 | Train Acc: 75.00% | Val. Loss: 0.518 | Val. Acc: 76.07% |\n",
      "| Epoch: 20 | Train Loss: 0.525 | Train Acc: 75.00% | Val. Loss: 0.515 | Val. Acc: 76.00% |\n",
      "| Epoch: 21 | Train Loss: 0.520 | Train Acc: 75.53% | Val. Loss: 0.513 | Val. Acc: 76.13% |\n",
      "| Epoch: 22 | Train Loss: 0.517 | Train Acc: 75.58% | Val. Loss: 0.512 | Val. Acc: 75.93% |\n",
      "| Epoch: 23 | Train Loss: 0.515 | Train Acc: 75.79% | Val. Loss: 0.510 | Val. Acc: 75.73% |\n",
      "| Epoch: 24 | Train Loss: 0.511 | Train Acc: 75.82% | Val. Loss: 0.506 | Val. Acc: 76.60% |\n",
      "| Epoch: 25 | Train Loss: 0.509 | Train Acc: 75.74% | Val. Loss: 0.504 | Val. Acc: 76.73% |\n",
      "| Epoch: 26 | Train Loss: 0.508 | Train Acc: 76.06% | Val. Loss: 0.503 | Val. Acc: 76.60% |\n",
      "| Epoch: 27 | Train Loss: 0.503 | Train Acc: 76.25% | Val. Loss: 0.502 | Val. Acc: 76.60% |\n",
      "| Epoch: 28 | Train Loss: 0.505 | Train Acc: 76.10% | Val. Loss: 0.500 | Val. Acc: 76.67% |\n",
      "| Test Loss: 0.494 | Test Acc: 75.87%\n",
      "    5 | 02m15s |    0.76667 |    0.4103 |    0.0073 |    0.0020 | \n",
      "| Epoch: 01 | Train Loss: 0.664 | Train Acc: 62.67% | Val. Loss: 0.646 | Val. Acc: 66.60% |\n",
      "| Epoch: 02 | Train Loss: 0.649 | Train Acc: 66.23% | Val. Loss: 0.643 | Val. Acc: 66.87% |\n",
      "| Epoch: 03 | Train Loss: 0.648 | Train Acc: 66.70% | Val. Loss: 0.634 | Val. Acc: 67.13% |\n",
      "| Epoch: 04 | Train Loss: 0.636 | Train Acc: 67.07% | Val. Loss: 0.626 | Val. Acc: 67.33% |\n",
      "| Epoch: 05 | Train Loss: 0.628 | Train Acc: 67.63% | Val. Loss: 0.622 | Val. Acc: 68.07% |\n",
      "| Epoch: 06 | Train Loss: 0.624 | Train Acc: 68.31% | Val. Loss: 0.617 | Val. Acc: 68.73% |\n",
      "| Epoch: 07 | Train Loss: 0.619 | Train Acc: 69.13% | Val. Loss: 0.611 | Val. Acc: 69.60% |\n",
      "| Epoch: 08 | Train Loss: 0.613 | Train Acc: 70.05% | Val. Loss: 0.604 | Val. Acc: 69.73% |\n",
      "| Epoch: 09 | Train Loss: 0.605 | Train Acc: 70.60% | Val. Loss: 0.596 | Val. Acc: 69.87% |\n",
      "| Epoch: 10 | Train Loss: 0.600 | Train Acc: 70.78% | Val. Loss: 0.589 | Val. Acc: 70.20% |\n",
      "| Epoch: 11 | Train Loss: 0.592 | Train Acc: 70.89% | Val. Loss: 0.583 | Val. Acc: 71.13% |\n",
      "| Epoch: 12 | Train Loss: 0.585 | Train Acc: 70.97% | Val. Loss: 0.576 | Val. Acc: 71.40% |\n",
      "| Epoch: 13 | Train Loss: 0.578 | Train Acc: 71.88% | Val. Loss: 0.569 | Val. Acc: 72.13% |\n",
      "| Epoch: 14 | Train Loss: 0.572 | Train Acc: 72.24% | Val. Loss: 0.563 | Val. Acc: 72.93% |\n",
      "| Epoch: 15 | Train Loss: 0.567 | Train Acc: 72.80% | Val. Loss: 0.558 | Val. Acc: 73.47% |\n",
      "| Epoch: 16 | Train Loss: 0.562 | Train Acc: 73.35% | Val. Loss: 0.553 | Val. Acc: 73.60% |\n",
      "| Epoch: 17 | Train Loss: 0.559 | Train Acc: 73.69% | Val. Loss: 0.548 | Val. Acc: 73.73% |\n",
      "| Epoch: 18 | Train Loss: 0.552 | Train Acc: 74.02% | Val. Loss: 0.544 | Val. Acc: 74.13% |\n",
      "| Epoch: 19 | Train Loss: 0.547 | Train Acc: 74.57% | Val. Loss: 0.541 | Val. Acc: 74.07% |\n",
      "| Epoch: 20 | Train Loss: 0.545 | Train Acc: 74.40% | Val. Loss: 0.538 | Val. Acc: 74.40% |\n",
      "| Epoch: 21 | Train Loss: 0.541 | Train Acc: 74.68% | Val. Loss: 0.535 | Val. Acc: 74.67% |\n",
      "| Epoch: 22 | Train Loss: 0.539 | Train Acc: 74.55% | Val. Loss: 0.532 | Val. Acc: 75.00% |\n",
      "| Epoch: 23 | Train Loss: 0.537 | Train Acc: 74.72% | Val. Loss: 0.530 | Val. Acc: 75.20% |\n",
      "| Epoch: 24 | Train Loss: 0.533 | Train Acc: 75.02% | Val. Loss: 0.529 | Val. Acc: 75.27% |\n",
      "| Epoch: 25 | Train Loss: 0.531 | Train Acc: 75.32% | Val. Loss: 0.527 | Val. Acc: 75.13% |\n",
      "| Epoch: 26 | Train Loss: 0.531 | Train Acc: 75.35% | Val. Loss: 0.526 | Val. Acc: 75.20% |\n",
      "| Epoch: 27 | Train Loss: 0.530 | Train Acc: 75.25% | Val. Loss: 0.525 | Val. Acc: 74.93% |\n",
      "| Epoch: 28 | Train Loss: 0.528 | Train Acc: 74.93% | Val. Loss: 0.525 | Val. Acc: 74.93% |\n",
      "| Test Loss: 0.517 | Test Acc: 75.27%\n",
      "    6 | 02m22s |    0.74933 |    0.4857 |    0.0046 |    0.0088 | \n",
      "| Epoch: 01 | Train Loss: 0.714 | Train Acc: 42.45% | Val. Loss: 0.658 | Val. Acc: 66.40% |\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| Epoch: 02 | Train Loss: 0.653 | Train Acc: 66.55% | Val. Loss: 0.673 | Val. Acc: 66.53% |\n",
      "| Epoch: 03 | Train Loss: 0.667 | Train Acc: 66.76% | Val. Loss: 0.638 | Val. Acc: 67.00% |\n",
      "| Epoch: 04 | Train Loss: 0.631 | Train Acc: 67.16% | Val. Loss: 0.609 | Val. Acc: 68.00% |\n",
      "| Epoch: 05 | Train Loss: 0.601 | Train Acc: 68.65% | Val. Loss: 0.604 | Val. Acc: 71.33% |\n",
      "| Epoch: 06 | Train Loss: 0.597 | Train Acc: 72.87% | Val. Loss: 0.604 | Val. Acc: 71.47% |\n",
      "| Epoch: 07 | Train Loss: 0.599 | Train Acc: 72.60% | Val. Loss: 0.593 | Val. Acc: 72.00% |\n",
      "| Epoch: 08 | Train Loss: 0.586 | Train Acc: 73.81% | Val. Loss: 0.574 | Val. Acc: 73.07% |\n",
      "| Epoch: 09 | Train Loss: 0.565 | Train Acc: 74.56% | Val. Loss: 0.556 | Val. Acc: 73.87% |\n",
      "| Epoch: 10 | Train Loss: 0.546 | Train Acc: 75.05% | Val. Loss: 0.546 | Val. Acc: 73.40% |\n",
      "| Epoch: 11 | Train Loss: 0.538 | Train Acc: 74.89% | Val. Loss: 0.542 | Val. Acc: 73.60% |\n",
      "| Epoch: 12 | Train Loss: 0.531 | Train Acc: 74.66% | Val. Loss: 0.536 | Val. Acc: 74.07% |\n",
      "| Epoch: 13 | Train Loss: 0.524 | Train Acc: 75.12% | Val. Loss: 0.524 | Val. Acc: 75.33% |\n",
      "| Epoch: 14 | Train Loss: 0.510 | Train Acc: 75.68% | Val. Loss: 0.514 | Val. Acc: 76.40% |\n",
      "| Epoch: 15 | Train Loss: 0.500 | Train Acc: 76.67% | Val. Loss: 0.509 | Val. Acc: 76.93% |\n",
      "| Epoch: 16 | Train Loss: 0.491 | Train Acc: 77.17% | Val. Loss: 0.507 | Val. Acc: 76.87% |\n",
      "| Epoch: 17 | Train Loss: 0.489 | Train Acc: 77.29% | Val. Loss: 0.502 | Val. Acc: 77.00% |\n",
      "| Epoch: 18 | Train Loss: 0.479 | Train Acc: 78.04% | Val. Loss: 0.495 | Val. Acc: 77.20% |\n",
      "| Epoch: 19 | Train Loss: 0.471 | Train Acc: 78.23% | Val. Loss: 0.492 | Val. Acc: 77.73% |\n",
      "| Epoch: 20 | Train Loss: 0.460 | Train Acc: 78.16% | Val. Loss: 0.492 | Val. Acc: 77.67% |\n",
      "| Epoch: 21 | Train Loss: 0.454 | Train Acc: 78.50% | Val. Loss: 0.490 | Val. Acc: 77.53% |\n",
      "| Epoch: 22 | Train Loss: 0.447 | Train Acc: 78.76% | Val. Loss: 0.484 | Val. Acc: 77.33% |\n",
      "| Epoch: 23 | Train Loss: 0.438 | Train Acc: 78.93% | Val. Loss: 0.480 | Val. Acc: 77.00% |\n",
      "| Epoch: 24 | Train Loss: 0.430 | Train Acc: 79.88% | Val. Loss: 0.479 | Val. Acc: 77.80% |\n",
      "| Epoch: 25 | Train Loss: 0.427 | Train Acc: 80.06% | Val. Loss: 0.479 | Val. Acc: 77.53% |\n",
      "| Epoch: 26 | Train Loss: 0.422 | Train Acc: 80.43% | Val. Loss: 0.477 | Val. Acc: 77.93% |\n",
      "| Epoch: 27 | Train Loss: 0.414 | Train Acc: 80.68% | Val. Loss: 0.477 | Val. Acc: 78.20% |\n",
      "| Epoch: 28 | Train Loss: 0.408 | Train Acc: 80.86% | Val. Loss: 0.479 | Val. Acc: 78.27% |\n",
      "| Test Loss: 0.468 | Test Acc: 78.20%\n",
      "    7 | 02m30s |    0.78267 |    0.1910 |    0.0093 |    0.0003 | \n",
      "| Epoch: 01 | Train Loss: 0.685 | Train Acc: 57.33% | Val. Loss: 0.810 | Val. Acc: 66.87% |\n",
      "| Epoch: 02 | Train Loss: 0.807 | Train Acc: 66.76% | Val. Loss: 0.648 | Val. Acc: 67.07% |\n",
      "| Epoch: 03 | Train Loss: 0.644 | Train Acc: 67.26% | Val. Loss: 0.617 | Val. Acc: 70.60% |\n",
      "| Epoch: 04 | Train Loss: 0.617 | Train Acc: 70.76% | Val. Loss: 0.645 | Val. Acc: 64.73% |\n",
      "| Epoch: 05 | Train Loss: 0.645 | Train Acc: 64.33% | Val. Loss: 0.638 | Val. Acc: 66.27% |\n",
      "| Epoch: 06 | Train Loss: 0.637 | Train Acc: 66.42% | Val. Loss: 0.614 | Val. Acc: 71.20% |\n",
      "| Epoch: 07 | Train Loss: 0.613 | Train Acc: 71.59% | Val. Loss: 0.596 | Val. Acc: 73.40% |\n",
      "| Epoch: 08 | Train Loss: 0.594 | Train Acc: 73.70% | Val. Loss: 0.590 | Val. Acc: 72.53% |\n",
      "| Epoch: 09 | Train Loss: 0.587 | Train Acc: 72.70% | Val. Loss: 0.589 | Val. Acc: 71.33% |\n",
      "| Epoch: 10 | Train Loss: 0.588 | Train Acc: 71.20% | Val. Loss: 0.586 | Val. Acc: 71.27% |\n",
      "| Epoch: 11 | Train Loss: 0.581 | Train Acc: 71.23% | Val. Loss: 0.577 | Val. Acc: 71.93% |\n",
      "| Epoch: 12 | Train Loss: 0.574 | Train Acc: 71.87% | Val. Loss: 0.565 | Val. Acc: 73.33% |\n",
      "| Epoch: 13 | Train Loss: 0.563 | Train Acc: 73.30% | Val. Loss: 0.556 | Val. Acc: 74.13% |\n",
      "| Epoch: 14 | Train Loss: 0.556 | Train Acc: 74.47% | Val. Loss: 0.551 | Val. Acc: 75.13% |\n",
      "| Epoch: 15 | Train Loss: 0.552 | Train Acc: 75.35% | Val. Loss: 0.547 | Val. Acc: 75.47% |\n",
      "| Epoch: 16 | Train Loss: 0.549 | Train Acc: 74.88% | Val. Loss: 0.541 | Val. Acc: 75.60% |\n",
      "| Epoch: 17 | Train Loss: 0.542 | Train Acc: 75.40% | Val. Loss: 0.537 | Val. Acc: 74.73% |\n",
      "| Epoch: 18 | Train Loss: 0.538 | Train Acc: 75.19% | Val. Loss: 0.537 | Val. Acc: 74.20% |\n",
      "| Epoch: 19 | Train Loss: 0.539 | Train Acc: 74.98% | Val. Loss: 0.537 | Val. Acc: 73.93% |\n",
      "| Epoch: 20 | Train Loss: 0.536 | Train Acc: 75.15% | Val. Loss: 0.535 | Val. Acc: 74.20% |\n",
      "| Epoch: 21 | Train Loss: 0.537 | Train Acc: 74.79% | Val. Loss: 0.532 | Val. Acc: 74.87% |\n",
      "| Epoch: 22 | Train Loss: 0.532 | Train Acc: 75.47% | Val. Loss: 0.530 | Val. Acc: 74.93% |\n",
      "| Epoch: 23 | Train Loss: 0.531 | Train Acc: 75.33% | Val. Loss: 0.530 | Val. Acc: 75.20% |\n",
      "| Epoch: 24 | Train Loss: 0.529 | Train Acc: 75.69% | Val. Loss: 0.530 | Val. Acc: 75.27% |\n",
      "| Epoch: 25 | Train Loss: 0.530 | Train Acc: 75.43% | Val. Loss: 0.529 | Val. Acc: 75.07% |\n",
      "| Epoch: 26 | Train Loss: 0.527 | Train Acc: 75.63% | Val. Loss: 0.530 | Val. Acc: 74.73% |\n",
      "| Epoch: 27 | Train Loss: 0.527 | Train Acc: 75.62% | Val. Loss: 0.532 | Val. Acc: 74.93% |\n",
      "| Epoch: 28 | Train Loss: 0.530 | Train Acc: 75.48% | Val. Loss: 0.531 | Val. Acc: 74.87% |\n",
      "| Test Loss: 0.524 | Test Acc: 74.93%\n",
      "    8 | 02m36s |    0.74867 |    0.3615 |    0.0173 |    0.0067 | \n",
      "Bayesian Optimization\n",
      "-----------------------------------------------------------------\n",
      " Step |   Time |      Value |        dp |        lr |        wd | \n",
      "| Epoch: 01 | Train Loss: 0.732 | Train Acc: 35.56% | Val. Loss: 2.247 | Val. Acc: 66.87% |\n",
      "| Epoch: 02 | Train Loss: 2.239 | Train Acc: 66.75% | Val. Loss: 0.858 | Val. Acc: 66.87% |\n",
      "| Epoch: 03 | Train Loss: 0.840 | Train Acc: 66.74% | Val. Loss: 0.708 | Val. Acc: 56.73% |\n",
      "| Epoch: 04 | Train Loss: 0.690 | Train Acc: 60.02% | Val. Loss: 0.728 | Val. Acc: 45.47% |\n",
      "| Epoch: 05 | Train Loss: 0.707 | Train Acc: 50.98% | Val. Loss: 0.656 | Val. Acc: 64.87% |\n",
      "| Epoch: 06 | Train Loss: 0.643 | Train Acc: 68.08% | Val. Loss: 0.646 | Val. Acc: 66.47% |\n",
      "| Epoch: 07 | Train Loss: 0.631 | Train Acc: 66.70% | Val. Loss: 0.608 | Val. Acc: 69.20% |\n",
      "| Epoch: 08 | Train Loss: 0.528 | Train Acc: 75.80% | Val. Loss: 0.653 | Val. Acc: 69.47% |\n",
      "| Epoch: 09 | Train Loss: 0.472 | Train Acc: 80.53% | Val. Loss: 0.705 | Val. Acc: 73.93% |\n",
      "| Epoch: 10 | Train Loss: 0.416 | Train Acc: 84.01% | Val. Loss: 0.807 | Val. Acc: 74.53% |\n",
      "| Epoch: 11 | Train Loss: 0.388 | Train Acc: 85.14% | Val. Loss: 0.849 | Val. Acc: 74.93% |\n",
      "| Epoch: 12 | Train Loss: 0.339 | Train Acc: 86.95% | Val. Loss: 0.910 | Val. Acc: 73.47% |\n",
      "| Epoch: 13 | Train Loss: 0.333 | Train Acc: 86.78% | Val. Loss: 0.953 | Val. Acc: 73.80% |\n",
      "| Epoch: 14 | Train Loss: 0.293 | Train Acc: 88.10% | Val. Loss: 1.013 | Val. Acc: 72.53% |\n",
      "| Epoch: 15 | Train Loss: 0.271 | Train Acc: 88.75% | Val. Loss: 1.050 | Val. Acc: 72.87% |\n",
      "| Epoch: 16 | Train Loss: 0.253 | Train Acc: 89.33% | Val. Loss: 1.109 | Val. Acc: 72.80% |\n",
      "| Epoch: 17 | Train Loss: 0.232 | Train Acc: 90.67% | Val. Loss: 1.166 | Val. Acc: 72.00% |\n",
      "| Epoch: 18 | Train Loss: 0.217 | Train Acc: 91.14% | Val. Loss: 1.212 | Val. Acc: 71.67% |\n",
      "| Epoch: 19 | Train Loss: 0.209 | Train Acc: 91.72% | Val. Loss: 1.256 | Val. Acc: 71.73% |\n",
      "| Epoch: 20 | Train Loss: 0.195 | Train Acc: 92.31% | Val. Loss: 1.299 | Val. Acc: 72.07% |\n",
      "| Epoch: 21 | Train Loss: 0.182 | Train Acc: 92.43% | Val. Loss: 1.327 | Val. Acc: 72.20% |\n",
      "| Epoch: 22 | Train Loss: 0.170 | Train Acc: 93.09% | Val. Loss: 1.362 | Val. Acc: 71.93% |\n",
      "| Epoch: 23 | Train Loss: 0.173 | Train Acc: 93.05% | Val. Loss: 1.409 | Val. Acc: 71.93% |\n",
      "| Epoch: 24 | Train Loss: 0.165 | Train Acc: 93.14% | Val. Loss: 1.447 | Val. Acc: 72.07% |\n",
      "| Epoch: 25 | Train Loss: 0.157 | Train Acc: 93.83% | Val. Loss: 1.475 | Val. Acc: 72.00% |\n",
      "| Epoch: 26 | Train Loss: 0.150 | Train Acc: 94.01% | Val. Loss: 1.514 | Val. Acc: 71.87% |\n",
      "| Epoch: 27 | Train Loss: 0.148 | Train Acc: 94.17% | Val. Loss: 1.564 | Val. Acc: 71.87% |\n",
      "| Epoch: 28 | Train Loss: 0.144 | Train Acc: 94.28% | Val. Loss: 1.571 | Val. Acc: 72.40% |\n",
      "| Test Loss: 1.681 | Test Acc: 70.73%\n",
      "    9 | 02m27s |    0.72400 |    0.1742 |    0.0500 |    0.0000 | \n",
      "| Epoch: 01 | Train Loss: 0.702 | Train Acc: 49.25% | Val. Loss: 2.317 | Val. Acc: 66.87% |\n",
      "| Epoch: 02 | Train Loss: 2.311 | Train Acc: 66.75% | Val. Loss: 0.818 | Val. Acc: 66.60% |\n",
      "| Epoch: 03 | Train Loss: 0.805 | Train Acc: 66.94% | Val. Loss: 0.944 | Val. Acc: 33.33% |\n",
      "| Epoch: 04 | Train Loss: 0.939 | Train Acc: 36.41% | Val. Loss: 0.778 | Val. Acc: 36.47% |\n",
      "| Epoch: 05 | Train Loss: 0.759 | Train Acc: 42.19% | Val. Loss: 0.647 | Val. Acc: 67.07% |\n",
      "| Epoch: 06 | Train Loss: 0.633 | Train Acc: 69.50% | Val. Loss: 0.649 | Val. Acc: 69.40% |\n",
      "| Epoch: 07 | Train Loss: 0.644 | Train Acc: 69.43% | Val. Loss: 0.643 | Val. Acc: 70.07% |\n",
      "| Epoch: 08 | Train Loss: 0.620 | Train Acc: 70.99% | Val. Loss: 0.592 | Val. Acc: 73.67% |\n",
      "| Epoch: 09 | Train Loss: 0.537 | Train Acc: 78.94% | Val. Loss: 0.598 | Val. Acc: 73.87% |\n",
      "| Epoch: 10 | Train Loss: 0.493 | Train Acc: 81.96% | Val. Loss: 0.612 | Val. Acc: 75.20% |\n",
      "| Epoch: 11 | Train Loss: 0.424 | Train Acc: 84.51% | Val. Loss: 0.699 | Val. Acc: 75.53% |\n",
      "| Epoch: 12 | Train Loss: 0.406 | Train Acc: 84.89% | Val. Loss: 0.778 | Val. Acc: 75.47% |\n",
      "| Epoch: 13 | Train Loss: 0.386 | Train Acc: 85.56% | Val. Loss: 0.824 | Val. Acc: 74.73% |\n",
      "| Epoch: 14 | Train Loss: 0.356 | Train Acc: 86.93% | Val. Loss: 0.882 | Val. Acc: 73.47% |\n",
      "| Epoch: 15 | Train Loss: 0.329 | Train Acc: 87.90% | Val. Loss: 0.952 | Val. Acc: 73.27% |\n",
      "| Epoch: 16 | Train Loss: 0.306 | Train Acc: 88.00% | Val. Loss: 1.035 | Val. Acc: 72.40% |\n",
      "| Epoch: 17 | Train Loss: 0.297 | Train Acc: 88.06% | Val. Loss: 1.075 | Val. Acc: 71.87% |\n",
      "| Epoch: 18 | Train Loss: 0.283 | Train Acc: 89.32% | Val. Loss: 1.120 | Val. Acc: 71.47% |\n",
      "| Epoch: 19 | Train Loss: 0.253 | Train Acc: 90.38% | Val. Loss: 1.178 | Val. Acc: 72.00% |\n",
      "| Epoch: 20 | Train Loss: 0.240 | Train Acc: 90.83% | Val. Loss: 1.235 | Val. Acc: 72.33% |\n",
      "| Epoch: 21 | Train Loss: 0.233 | Train Acc: 90.96% | Val. Loss: 1.261 | Val. Acc: 72.67% |\n",
      "| Epoch: 22 | Train Loss: 0.217 | Train Acc: 91.72% | Val. Loss: 1.289 | Val. Acc: 72.60% |\n",
      "| Epoch: 23 | Train Loss: 0.212 | Train Acc: 91.95% | Val. Loss: 1.335 | Val. Acc: 72.53% |\n",
      "| Epoch: 24 | Train Loss: 0.202 | Train Acc: 92.30% | Val. Loss: 1.393 | Val. Acc: 72.73% |\n",
      "| Epoch: 25 | Train Loss: 0.190 | Train Acc: 92.62% | Val. Loss: 1.448 | Val. Acc: 72.00% |\n",
      "| Epoch: 26 | Train Loss: 0.184 | Train Acc: 92.74% | Val. Loss: 1.488 | Val. Acc: 71.27% |\n",
      "| Epoch: 27 | Train Loss: 0.180 | Train Acc: 93.17% | Val. Loss: 1.527 | Val. Acc: 71.47% |\n",
      "| Epoch: 28 | Train Loss: 0.170 | Train Acc: 93.44% | Val. Loss: 1.574 | Val. Acc: 71.40% |\n",
      "| Test Loss: 1.513 | Test Acc: 71.93%\n",
      "   10 | 02m29s |    0.71400 |    0.5000 |    0.0500 |    0.0000 | \n",
      "| Epoch: 01 | Train Loss: 0.683 | Train Acc: 56.30% | Val. Loss: 1.931 | Val. Acc: 66.87% |\n",
      "| Epoch: 02 | Train Loss: 1.934 | Train Acc: 66.75% | Val. Loss: 0.727 | Val. Acc: 67.00% |\n",
      "| Epoch: 03 | Train Loss: 0.726 | Train Acc: 66.91% | Val. Loss: 0.703 | Val. Acc: 51.00% |\n",
      "| Epoch: 04 | Train Loss: 0.705 | Train Acc: 50.46% | Val. Loss: 0.723 | Val. Acc: 37.93% |\n",
      "| Epoch: 05 | Train Loss: 0.723 | Train Acc: 42.86% | Val. Loss: 0.689 | Val. Acc: 53.00% |\n",
      "| Epoch: 06 | Train Loss: 0.689 | Train Acc: 52.14% | Val. Loss: 0.659 | Val. Acc: 69.60% |\n",
      "| Epoch: 07 | Train Loss: 0.659 | Train Acc: 69.06% | Val. Loss: 0.638 | Val. Acc: 67.67% |\n",
      "| Epoch: 08 | Train Loss: 0.639 | Train Acc: 67.42% | Val. Loss: 0.644 | Val. Acc: 67.27% |\n",
      "| Epoch: 09 | Train Loss: 0.652 | Train Acc: 66.81% | Val. Loss: 0.639 | Val. Acc: 67.13% |\n",
      "| Epoch: 10 | Train Loss: 0.646 | Train Acc: 66.74% | Val. Loss: 0.633 | Val. Acc: 67.33% |\n",
      "| Epoch: 11 | Train Loss: 0.637 | Train Acc: 66.93% | Val. Loss: 0.635 | Val. Acc: 67.13% |\n",
      "| Epoch: 12 | Train Loss: 0.637 | Train Acc: 67.04% | Val. Loss: 0.634 | Val. Acc: 67.33% |\n",
      "| Epoch: 13 | Train Loss: 0.634 | Train Acc: 67.51% | Val. Loss: 0.630 | Val. Acc: 67.07% |\n",
      "| Epoch: 14 | Train Loss: 0.631 | Train Acc: 67.23% | Val. Loss: 0.628 | Val. Acc: 66.93% |\n",
      "| Epoch: 15 | Train Loss: 0.628 | Train Acc: 67.18% | Val. Loss: 0.627 | Val. Acc: 67.00% |\n",
      "| Epoch: 16 | Train Loss: 0.629 | Train Acc: 67.12% | Val. Loss: 0.625 | Val. Acc: 67.00% |\n",
      "| Epoch: 17 | Train Loss: 0.626 | Train Acc: 67.11% | Val. Loss: 0.622 | Val. Acc: 67.00% |\n",
      "| Epoch: 18 | Train Loss: 0.623 | Train Acc: 67.22% | Val. Loss: 0.619 | Val. Acc: 67.53% |\n",
      "| Epoch: 19 | Train Loss: 0.620 | Train Acc: 67.83% | Val. Loss: 0.616 | Val. Acc: 68.40% |\n",
      "| Epoch: 20 | Train Loss: 0.618 | Train Acc: 68.55% | Val. Loss: 0.614 | Val. Acc: 70.20% |\n",
      "| Epoch: 21 | Train Loss: 0.616 | Train Acc: 68.97% | Val. Loss: 0.611 | Val. Acc: 70.27% |\n",
      "| Epoch: 22 | Train Loss: 0.612 | Train Acc: 69.27% | Val. Loss: 0.607 | Val. Acc: 70.60% |\n",
      "| Epoch: 23 | Train Loss: 0.610 | Train Acc: 69.26% | Val. Loss: 0.603 | Val. Acc: 70.47% |\n",
      "| Epoch: 24 | Train Loss: 0.605 | Train Acc: 69.47% | Val. Loss: 0.598 | Val. Acc: 71.40% |\n",
      "| Epoch: 25 | Train Loss: 0.601 | Train Acc: 69.79% | Val. Loss: 0.593 | Val. Acc: 71.47% |\n",
      "| Epoch: 26 | Train Loss: 0.596 | Train Acc: 70.56% | Val. Loss: 0.587 | Val. Acc: 71.53% |\n",
      "| Epoch: 27 | Train Loss: 0.589 | Train Acc: 71.10% | Val. Loss: 0.581 | Val. Acc: 71.87% |\n",
      "| Epoch: 28 | Train Loss: 0.584 | Train Acc: 71.62% | Val. Loss: 0.575 | Val. Acc: 72.40% |\n",
      "| Test Loss: 0.572 | Test Acc: 73.27%\n",
      "   11 | 02m31s |    0.72400 |    0.4175 |    0.0500 |    0.0100 | \n",
      "| Epoch: 01 | Train Loss: 0.655 | Train Acc: 65.91% | Val. Loss: 0.651 | Val. Acc: 67.07% |\n",
      "| Epoch: 02 | Train Loss: 0.654 | Train Acc: 65.93% | Val. Loss: 0.651 | Val. Acc: 67.07% |\n",
      "| Epoch: 03 | Train Loss: 0.656 | Train Acc: 65.96% | Val. Loss: 0.651 | Val. Acc: 67.07% |\n",
      "| Epoch: 04 | Train Loss: 0.655 | Train Acc: 66.33% | Val. Loss: 0.651 | Val. Acc: 67.07% |\n",
      "| Epoch: 05 | Train Loss: 0.654 | Train Acc: 66.12% | Val. Loss: 0.651 | Val. Acc: 67.07% |\n",
      "| Epoch: 06 | Train Loss: 0.655 | Train Acc: 66.17% | Val. Loss: 0.651 | Val. Acc: 67.07% |\n",
      "| Epoch: 07 | Train Loss: 0.655 | Train Acc: 66.46% | Val. Loss: 0.651 | Val. Acc: 67.07% |\n",
      "| Epoch: 08 | Train Loss: 0.654 | Train Acc: 66.34% | Val. Loss: 0.651 | Val. Acc: 67.07% |\n",
      "| Epoch: 09 | Train Loss: 0.654 | Train Acc: 66.53% | Val. Loss: 0.651 | Val. Acc: 67.07% |\n",
      "| Epoch: 10 | Train Loss: 0.654 | Train Acc: 65.91% | Val. Loss: 0.650 | Val. Acc: 67.07% |\n",
      "| Epoch: 11 | Train Loss: 0.653 | Train Acc: 66.46% | Val. Loss: 0.650 | Val. Acc: 67.07% |\n",
      "| Epoch: 12 | Train Loss: 0.654 | Train Acc: 66.19% | Val. Loss: 0.650 | Val. Acc: 67.07% |\n",
      "| Epoch: 13 | Train Loss: 0.655 | Train Acc: 66.09% | Val. Loss: 0.650 | Val. Acc: 67.07% |\n",
      "| Epoch: 14 | Train Loss: 0.654 | Train Acc: 66.51% | Val. Loss: 0.650 | Val. Acc: 67.07% |\n",
      "| Epoch: 15 | Train Loss: 0.654 | Train Acc: 66.21% | Val. Loss: 0.650 | Val. Acc: 67.07% |\n",
      "| Epoch: 16 | Train Loss: 0.653 | Train Acc: 66.11% | Val. Loss: 0.650 | Val. Acc: 67.07% |\n",
      "| Epoch: 17 | Train Loss: 0.653 | Train Acc: 66.46% | Val. Loss: 0.650 | Val. Acc: 67.07% |\n",
      "| Epoch: 18 | Train Loss: 0.653 | Train Acc: 66.46% | Val. Loss: 0.650 | Val. Acc: 67.07% |\n",
      "| Epoch: 19 | Train Loss: 0.652 | Train Acc: 66.17% | Val. Loss: 0.650 | Val. Acc: 67.07% |\n",
      "| Epoch: 20 | Train Loss: 0.653 | Train Acc: 66.26% | Val. Loss: 0.650 | Val. Acc: 67.07% |\n",
      "| Epoch: 21 | Train Loss: 0.654 | Train Acc: 66.10% | Val. Loss: 0.650 | Val. Acc: 67.07% |\n",
      "| Epoch: 22 | Train Loss: 0.653 | Train Acc: 65.98% | Val. Loss: 0.650 | Val. Acc: 67.07% |\n",
      "| Epoch: 23 | Train Loss: 0.653 | Train Acc: 66.34% | Val. Loss: 0.650 | Val. Acc: 67.07% |\n",
      "| Epoch: 24 | Train Loss: 0.652 | Train Acc: 66.45% | Val. Loss: 0.650 | Val. Acc: 67.07% |\n",
      "| Epoch: 25 | Train Loss: 0.653 | Train Acc: 66.36% | Val. Loss: 0.650 | Val. Acc: 67.07% |\n",
      "| Epoch: 26 | Train Loss: 0.652 | Train Acc: 66.22% | Val. Loss: 0.650 | Val. Acc: 67.07% |\n",
      "| Epoch: 27 | Train Loss: 0.653 | Train Acc: 66.44% | Val. Loss: 0.649 | Val. Acc: 67.07% |\n",
      "| Epoch: 28 | Train Loss: 0.655 | Train Acc: 66.28% | Val. Loss: 0.649 | Val. Acc: 67.07% |\n",
      "| Test Loss: 0.651 | Test Acc: 67.00%\n",
      "   12 | 02m47s |    0.67067 |    0.2921 |    0.0000 |    0.0100 | \n",
      "| Epoch: 01 | Train Loss: 0.670 | Train Acc: 62.44% | Val. Loss: 1.018 | Val. Acc: 66.87% |\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| Epoch: 02 | Train Loss: 1.017 | Train Acc: 66.75% | Val. Loss: 0.644 | Val. Acc: 67.33% |\n",
      "| Epoch: 03 | Train Loss: 0.638 | Train Acc: 67.27% | Val. Loss: 0.653 | Val. Acc: 64.33% |\n",
      "| Epoch: 04 | Train Loss: 0.651 | Train Acc: 64.01% | Val. Loss: 0.681 | Val. Acc: 54.60% |\n",
      "| Epoch: 05 | Train Loss: 0.680 | Train Acc: 55.88% | Val. Loss: 0.656 | Val. Acc: 64.80% |\n",
      "| Epoch: 06 | Train Loss: 0.654 | Train Acc: 64.98% | Val. Loss: 0.631 | Val. Acc: 72.60% |\n",
      "| Epoch: 07 | Train Loss: 0.627 | Train Acc: 73.20% | Val. Loss: 0.619 | Val. Acc: 73.07% |\n",
      "| Epoch: 08 | Train Loss: 0.614 | Train Acc: 72.97% | Val. Loss: 0.615 | Val. Acc: 69.60% |\n",
      "| Epoch: 09 | Train Loss: 0.610 | Train Acc: 69.16% | Val. Loss: 0.615 | Val. Acc: 67.47% |\n",
      "| Epoch: 10 | Train Loss: 0.611 | Train Acc: 67.51% | Val. Loss: 0.612 | Val. Acc: 67.07% |\n",
      "| Epoch: 11 | Train Loss: 0.610 | Train Acc: 67.35% | Val. Loss: 0.604 | Val. Acc: 67.93% |\n",
      "| Epoch: 12 | Train Loss: 0.603 | Train Acc: 67.83% | Val. Loss: 0.595 | Val. Acc: 70.13% |\n",
      "| Epoch: 13 | Train Loss: 0.594 | Train Acc: 70.00% | Val. Loss: 0.591 | Val. Acc: 72.67% |\n",
      "| Epoch: 14 | Train Loss: 0.590 | Train Acc: 72.61% | Val. Loss: 0.587 | Val. Acc: 73.27% |\n",
      "| Epoch: 15 | Train Loss: 0.586 | Train Acc: 73.83% | Val. Loss: 0.581 | Val. Acc: 73.20% |\n",
      "| Epoch: 16 | Train Loss: 0.581 | Train Acc: 74.08% | Val. Loss: 0.575 | Val. Acc: 73.40% |\n",
      "| Epoch: 17 | Train Loss: 0.574 | Train Acc: 73.86% | Val. Loss: 0.571 | Val. Acc: 73.47% |\n",
      "| Epoch: 18 | Train Loss: 0.568 | Train Acc: 73.73% | Val. Loss: 0.569 | Val. Acc: 73.47% |\n",
      "| Epoch: 19 | Train Loss: 0.568 | Train Acc: 73.40% | Val. Loss: 0.567 | Val. Acc: 73.33% |\n",
      "| Epoch: 20 | Train Loss: 0.564 | Train Acc: 73.57% | Val. Loss: 0.564 | Val. Acc: 73.60% |\n",
      "| Epoch: 21 | Train Loss: 0.561 | Train Acc: 73.84% | Val. Loss: 0.562 | Val. Acc: 73.40% |\n",
      "| Epoch: 22 | Train Loss: 0.558 | Train Acc: 74.01% | Val. Loss: 0.560 | Val. Acc: 73.73% |\n",
      "| Epoch: 23 | Train Loss: 0.556 | Train Acc: 74.08% | Val. Loss: 0.558 | Val. Acc: 73.87% |\n",
      "| Epoch: 24 | Train Loss: 0.555 | Train Acc: 74.16% | Val. Loss: 0.556 | Val. Acc: 73.67% |\n",
      "| Epoch: 25 | Train Loss: 0.553 | Train Acc: 74.05% | Val. Loss: 0.555 | Val. Acc: 73.67% |\n",
      "| Epoch: 26 | Train Loss: 0.552 | Train Acc: 74.11% | Val. Loss: 0.555 | Val. Acc: 73.40% |\n",
      "| Epoch: 27 | Train Loss: 0.552 | Train Acc: 74.07% | Val. Loss: 0.554 | Val. Acc: 73.40% |\n",
      "| Epoch: 28 | Train Loss: 0.550 | Train Acc: 74.22% | Val. Loss: 0.553 | Val. Acc: 73.47% |\n",
      "| Test Loss: 0.547 | Test Acc: 74.67%\n",
      "   13 | 03m01s |    0.73467 |    0.1854 |    0.0256 |    0.0100 | \n",
      "| Epoch: 01 | Train Loss: 0.670 | Train Acc: 62.47% | Val. Loss: 1.954 | Val. Acc: 66.87% |\n",
      "| Epoch: 02 | Train Loss: 1.953 | Train Acc: 66.75% | Val. Loss: 0.659 | Val. Acc: 67.27% |\n",
      "| Epoch: 03 | Train Loss: 0.651 | Train Acc: 67.59% | Val. Loss: 0.770 | Val. Acc: 36.00% |\n",
      "| Epoch: 04 | Train Loss: 0.771 | Train Acc: 37.71% | Val. Loss: 0.728 | Val. Acc: 34.40% |\n",
      "| Epoch: 05 | Train Loss: 0.727 | Train Acc: 35.31% | Val. Loss: 0.682 | Val. Acc: 51.87% |\n",
      "| Epoch: 06 | Train Loss: 0.680 | Train Acc: 52.97% | Val. Loss: 0.659 | Val. Acc: 69.33% |\n",
      "| Epoch: 07 | Train Loss: 0.657 | Train Acc: 70.74% | Val. Loss: 0.638 | Val. Acc: 67.67% |\n",
      "| Epoch: 08 | Train Loss: 0.636 | Train Acc: 67.31% | Val. Loss: 0.648 | Val. Acc: 66.87% |\n",
      "| Epoch: 09 | Train Loss: 0.646 | Train Acc: 66.75% | Val. Loss: 0.646 | Val. Acc: 66.87% |\n",
      "| Epoch: 10 | Train Loss: 0.645 | Train Acc: 66.75% | Val. Loss: 0.638 | Val. Acc: 66.80% |\n",
      "| Epoch: 11 | Train Loss: 0.638 | Train Acc: 66.75% | Val. Loss: 0.641 | Val. Acc: 66.87% |\n",
      "| Epoch: 12 | Train Loss: 0.640 | Train Acc: 66.79% | Val. Loss: 0.642 | Val. Acc: 66.87% |\n",
      "| Epoch: 13 | Train Loss: 0.641 | Train Acc: 66.73% | Val. Loss: 0.639 | Val. Acc: 66.87% |\n",
      "| Epoch: 14 | Train Loss: 0.638 | Train Acc: 66.75% | Val. Loss: 0.637 | Val. Acc: 66.87% |\n",
      "| Epoch: 15 | Train Loss: 0.635 | Train Acc: 66.75% | Val. Loss: 0.637 | Val. Acc: 66.87% |\n",
      "| Epoch: 16 | Train Loss: 0.636 | Train Acc: 66.75% | Val. Loss: 0.637 | Val. Acc: 66.87% |\n",
      "| Epoch: 17 | Train Loss: 0.636 | Train Acc: 66.75% | Val. Loss: 0.636 | Val. Acc: 66.87% |\n",
      "| Epoch: 18 | Train Loss: 0.635 | Train Acc: 66.75% | Val. Loss: 0.635 | Val. Acc: 66.87% |\n",
      "| Epoch: 19 | Train Loss: 0.635 | Train Acc: 66.75% | Val. Loss: 0.635 | Val. Acc: 66.87% |\n",
      "| Epoch: 20 | Train Loss: 0.635 | Train Acc: 66.75% | Val. Loss: 0.634 | Val. Acc: 66.87% |\n",
      "| Epoch: 21 | Train Loss: 0.634 | Train Acc: 66.75% | Val. Loss: 0.633 | Val. Acc: 66.87% |\n",
      "| Epoch: 22 | Train Loss: 0.633 | Train Acc: 66.75% | Val. Loss: 0.632 | Val. Acc: 66.87% |\n",
      "| Epoch: 23 | Train Loss: 0.632 | Train Acc: 66.75% | Val. Loss: 0.632 | Val. Acc: 66.87% |\n",
      "| Epoch: 24 | Train Loss: 0.632 | Train Acc: 66.75% | Val. Loss: 0.631 | Val. Acc: 66.87% |\n",
      "| Epoch: 25 | Train Loss: 0.631 | Train Acc: 66.75% | Val. Loss: 0.630 | Val. Acc: 66.87% |\n",
      "| Epoch: 26 | Train Loss: 0.630 | Train Acc: 66.75% | Val. Loss: 0.629 | Val. Acc: 66.87% |\n",
      "| Epoch: 27 | Train Loss: 0.629 | Train Acc: 66.75% | Val. Loss: 0.628 | Val. Acc: 66.87% |\n",
      "| Epoch: 28 | Train Loss: 0.629 | Train Acc: 66.75% | Val. Loss: 0.627 | Val. Acc: 66.87% |\n",
      "| Test Loss: 0.628 | Test Acc: 66.80%\n",
      "   14 | 02m52s |    0.66867 |    0.0500 |    0.0500 |    0.0100 | \n",
      "| Epoch: 01 | Train Loss: 0.710 | Train Acc: 42.63% | Val. Loss: 2.017 | Val. Acc: 66.87% |\n",
      "| Epoch: 02 | Train Loss: 2.015 | Train Acc: 66.75% | Val. Loss: 0.788 | Val. Acc: 66.87% |\n",
      "| Epoch: 03 | Train Loss: 0.781 | Train Acc: 66.76% | Val. Loss: 0.696 | Val. Acc: 54.87% |\n",
      "| Epoch: 04 | Train Loss: 0.693 | Train Acc: 53.87% | Val. Loss: 0.723 | Val. Acc: 36.20% |\n",
      "| Epoch: 05 | Train Loss: 0.721 | Train Acc: 38.01% | Val. Loss: 0.682 | Val. Acc: 58.20% |\n",
      "| Epoch: 06 | Train Loss: 0.681 | Train Acc: 59.58% | Val. Loss: 0.644 | Val. Acc: 67.27% |\n",
      "| Epoch: 07 | Train Loss: 0.642 | Train Acc: 67.49% | Val. Loss: 0.643 | Val. Acc: 67.47% |\n",
      "| Epoch: 08 | Train Loss: 0.642 | Train Acc: 67.28% | Val. Loss: 0.646 | Val. Acc: 67.27% |\n",
      "| Epoch: 09 | Train Loss: 0.644 | Train Acc: 66.56% | Val. Loss: 0.637 | Val. Acc: 64.00% |\n",
      "| Epoch: 10 | Train Loss: 0.634 | Train Acc: 65.16% | Val. Loss: 0.631 | Val. Acc: 67.53% |\n",
      "| Epoch: 11 | Train Loss: 0.630 | Train Acc: 67.59% | Val. Loss: 0.630 | Val. Acc: 67.80% |\n",
      "| Epoch: 12 | Train Loss: 0.628 | Train Acc: 67.59% | Val. Loss: 0.627 | Val. Acc: 68.40% |\n",
      "| Epoch: 13 | Train Loss: 0.624 | Train Acc: 67.77% | Val. Loss: 0.626 | Val. Acc: 68.40% |\n",
      "| Epoch: 14 | Train Loss: 0.624 | Train Acc: 67.94% | Val. Loss: 0.623 | Val. Acc: 68.40% |\n",
      "| Epoch: 15 | Train Loss: 0.621 | Train Acc: 67.84% | Val. Loss: 0.621 | Val. Acc: 68.27% |\n",
      "| Epoch: 16 | Train Loss: 0.620 | Train Acc: 67.83% | Val. Loss: 0.618 | Val. Acc: 68.60% |\n",
      "| Epoch: 17 | Train Loss: 0.616 | Train Acc: 68.02% | Val. Loss: 0.614 | Val. Acc: 69.27% |\n",
      "| Epoch: 18 | Train Loss: 0.612 | Train Acc: 68.83% | Val. Loss: 0.611 | Val. Acc: 70.87% |\n",
      "| Epoch: 19 | Train Loss: 0.609 | Train Acc: 69.75% | Val. Loss: 0.608 | Val. Acc: 71.13% |\n",
      "| Epoch: 20 | Train Loss: 0.607 | Train Acc: 70.18% | Val. Loss: 0.604 | Val. Acc: 71.20% |\n",
      "| Epoch: 21 | Train Loss: 0.603 | Train Acc: 70.46% | Val. Loss: 0.600 | Val. Acc: 71.20% |\n",
      "| Epoch: 22 | Train Loss: 0.599 | Train Acc: 70.53% | Val. Loss: 0.596 | Val. Acc: 71.53% |\n",
      "| Epoch: 23 | Train Loss: 0.596 | Train Acc: 70.66% | Val. Loss: 0.591 | Val. Acc: 71.93% |\n",
      "| Epoch: 24 | Train Loss: 0.590 | Train Acc: 71.09% | Val. Loss: 0.585 | Val. Acc: 72.07% |\n",
      "| Epoch: 25 | Train Loss: 0.583 | Train Acc: 71.87% | Val. Loss: 0.579 | Val. Acc: 72.53% |\n",
      "| Epoch: 26 | Train Loss: 0.578 | Train Acc: 72.39% | Val. Loss: 0.573 | Val. Acc: 72.73% |\n",
      "| Epoch: 27 | Train Loss: 0.569 | Train Acc: 73.11% | Val. Loss: 0.566 | Val. Acc: 73.13% |\n",
      "| Epoch: 28 | Train Loss: 0.563 | Train Acc: 73.52% | Val. Loss: 0.560 | Val. Acc: 73.27% |\n",
      "| Test Loss: 0.557 | Test Acc: 74.13%\n",
      "   15 | 02m47s |    0.73267 |    0.1113 |    0.0500 |    0.0100 | \n",
      "| Epoch: 01 | Train Loss: 0.687 | Train Acc: 53.99% | Val. Loss: 2.572 | Val. Acc: 66.87% |\n",
      "| Epoch: 02 | Train Loss: 2.548 | Train Acc: 66.75% | Val. Loss: 0.776 | Val. Acc: 67.07% |\n",
      "| Epoch: 03 | Train Loss: 0.753 | Train Acc: 67.61% | Val. Loss: 1.048 | Val. Acc: 35.20% |\n",
      "| Epoch: 04 | Train Loss: 1.028 | Train Acc: 36.96% | Val. Loss: 0.839 | Val. Acc: 37.33% |\n",
      "| Epoch: 05 | Train Loss: 0.809 | Train Acc: 40.86% | Val. Loss: 0.670 | Val. Acc: 57.80% |\n",
      "| Epoch: 06 | Train Loss: 0.643 | Train Acc: 65.94% | Val. Loss: 0.626 | Val. Acc: 73.80% |\n",
      "| Epoch: 07 | Train Loss: 0.613 | Train Acc: 76.17% | Val. Loss: 0.626 | Val. Acc: 70.07% |\n",
      "| Epoch: 08 | Train Loss: 0.609 | Train Acc: 70.40% | Val. Loss: 0.623 | Val. Acc: 70.67% |\n",
      "| Epoch: 09 | Train Loss: 0.574 | Train Acc: 71.63% | Val. Loss: 0.579 | Val. Acc: 74.00% |\n",
      "| Epoch: 10 | Train Loss: 0.478 | Train Acc: 80.42% | Val. Loss: 0.572 | Val. Acc: 75.20% |\n",
      "| Epoch: 11 | Train Loss: 0.405 | Train Acc: 85.21% | Val. Loss: 0.618 | Val. Acc: 75.93% |\n",
      "| Epoch: 12 | Train Loss: 0.357 | Train Acc: 86.32% | Val. Loss: 0.713 | Val. Acc: 76.07% |\n",
      "| Epoch: 13 | Train Loss: 0.341 | Train Acc: 86.85% | Val. Loss: 0.816 | Val. Acc: 75.67% |\n",
      "| Epoch: 14 | Train Loss: 0.324 | Train Acc: 87.98% | Val. Loss: 0.894 | Val. Acc: 74.67% |\n",
      "| Epoch: 15 | Train Loss: 0.297 | Train Acc: 89.03% | Val. Loss: 0.979 | Val. Acc: 73.67% |\n",
      "| Epoch: 16 | Train Loss: 0.271 | Train Acc: 89.71% | Val. Loss: 1.080 | Val. Acc: 73.33% |\n",
      "| Epoch: 17 | Train Loss: 0.255 | Train Acc: 90.18% | Val. Loss: 1.171 | Val. Acc: 73.27% |\n",
      "| Epoch: 18 | Train Loss: 0.245 | Train Acc: 90.27% | Val. Loss: 1.241 | Val. Acc: 72.67% |\n",
      "| Epoch: 19 | Train Loss: 0.229 | Train Acc: 90.69% | Val. Loss: 1.304 | Val. Acc: 72.87% |\n",
      "| Epoch: 20 | Train Loss: 0.218 | Train Acc: 90.95% | Val. Loss: 1.370 | Val. Acc: 73.33% |\n",
      "| Epoch: 21 | Train Loss: 0.208 | Train Acc: 91.37% | Val. Loss: 1.422 | Val. Acc: 73.87% |\n",
      "| Epoch: 22 | Train Loss: 0.189 | Train Acc: 92.51% | Val. Loss: 1.460 | Val. Acc: 74.20% |\n",
      "| Epoch: 23 | Train Loss: 0.187 | Train Acc: 92.39% | Val. Loss: 1.501 | Val. Acc: 73.93% |\n",
      "| Epoch: 24 | Train Loss: 0.182 | Train Acc: 92.78% | Val. Loss: 1.546 | Val. Acc: 74.13% |\n",
      "| Epoch: 25 | Train Loss: 0.177 | Train Acc: 93.09% | Val. Loss: 1.570 | Val. Acc: 74.13% |\n",
      "| Epoch: 26 | Train Loss: 0.164 | Train Acc: 93.49% | Val. Loss: 1.583 | Val. Acc: 73.87% |\n",
      "| Epoch: 27 | Train Loss: 0.166 | Train Acc: 93.35% | Val. Loss: 1.600 | Val. Acc: 73.73% |\n",
      "| Epoch: 28 | Train Loss: 0.161 | Train Acc: 93.67% | Val. Loss: 1.624 | Val. Acc: 73.80% |\n",
      "| Test Loss: 1.799 | Test Acc: 71.47%\n",
      "   16 | 02m45s |    0.73800 |    0.3605 |    0.0500 |    0.0000 | \n"
     ]
    }
   ],
   "source": [
    "# Bounded region of parameter space\n",
    "pbounds = {'lr': (0.00001, 0.05), 'wd': (0.000001, 0.01), 'dp': (0.05, 0.5)}\n",
    "\n",
    "optimizer = BayesianOptimization(\n",
    "    f=f,\n",
    "    pbounds=pbounds,\n",
    "    random_state=1,\n",
    ")\n",
    "\n",
    "optimizer.maximize(\n",
    "    init_points=8,\n",
    "    n_iter=8,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| Epoch: 01 | Train Loss: 0.681 | Train Acc: 58.86% | Val. Loss: 0.669 | Val. Acc: 66.87% |\n",
      "| Epoch: 02 | Train Loss: 0.664 | Train Acc: 66.75% | Val. Loss: 0.646 | Val. Acc: 67.00% |\n",
      "| Epoch: 03 | Train Loss: 0.639 | Train Acc: 67.18% | Val. Loss: 0.615 | Val. Acc: 68.80% |\n",
      "| Epoch: 04 | Train Loss: 0.610 | Train Acc: 68.63% | Val. Loss: 0.605 | Val. Acc: 71.40% |\n",
      "| Epoch: 05 | Train Loss: 0.599 | Train Acc: 72.34% | Val. Loss: 0.599 | Val. Acc: 72.73% |\n",
      "| Epoch: 06 | Train Loss: 0.595 | Train Acc: 73.22% | Val. Loss: 0.584 | Val. Acc: 73.13% |\n",
      "| Epoch: 07 | Train Loss: 0.579 | Train Acc: 73.93% | Val. Loss: 0.564 | Val. Acc: 73.27% |\n",
      "| Epoch: 08 | Train Loss: 0.557 | Train Acc: 74.49% | Val. Loss: 0.550 | Val. Acc: 73.40% |\n",
      "| Epoch: 09 | Train Loss: 0.542 | Train Acc: 74.52% | Val. Loss: 0.544 | Val. Acc: 73.93% |\n",
      "| Epoch: 10 | Train Loss: 0.534 | Train Acc: 74.70% | Val. Loss: 0.534 | Val. Acc: 74.53% |\n",
      "| Epoch: 11 | Train Loss: 0.524 | Train Acc: 75.34% | Val. Loss: 0.522 | Val. Acc: 75.47% |\n",
      "| Epoch: 12 | Train Loss: 0.512 | Train Acc: 76.19% | Val. Loss: 0.514 | Val. Acc: 76.13% |\n",
      "| Epoch: 13 | Train Loss: 0.504 | Train Acc: 76.48% | Val. Loss: 0.510 | Val. Acc: 76.33% |\n",
      "| Epoch: 14 | Train Loss: 0.499 | Train Acc: 76.70% | Val. Loss: 0.504 | Val. Acc: 76.60% |\n",
      "| Epoch: 15 | Train Loss: 0.492 | Train Acc: 76.89% | Val. Loss: 0.498 | Val. Acc: 76.80% |\n",
      "| Epoch: 16 | Train Loss: 0.479 | Train Acc: 77.25% | Val. Loss: 0.497 | Val. Acc: 76.80% |\n",
      "| Epoch: 17 | Train Loss: 0.473 | Train Acc: 77.70% | Val. Loss: 0.494 | Val. Acc: 76.80% |\n",
      "| Epoch: 18 | Train Loss: 0.466 | Train Acc: 77.61% | Val. Loss: 0.487 | Val. Acc: 76.67% |\n",
      "| Epoch: 19 | Train Loss: 0.455 | Train Acc: 78.50% | Val. Loss: 0.481 | Val. Acc: 77.47% |\n",
      "| Epoch: 20 | Train Loss: 0.447 | Train Acc: 79.04% | Val. Loss: 0.479 | Val. Acc: 77.53% |\n",
      "| Epoch: 21 | Train Loss: 0.441 | Train Acc: 79.51% | Val. Loss: 0.476 | Val. Acc: 77.80% |\n",
      "| Epoch: 22 | Train Loss: 0.433 | Train Acc: 79.63% | Val. Loss: 0.474 | Val. Acc: 77.40% |\n",
      "| Epoch: 23 | Train Loss: 0.426 | Train Acc: 79.97% | Val. Loss: 0.475 | Val. Acc: 77.27% |\n",
      "| Epoch: 24 | Train Loss: 0.423 | Train Acc: 80.01% | Val. Loss: 0.476 | Val. Acc: 77.20% |\n",
      "| Epoch: 25 | Train Loss: 0.417 | Train Acc: 80.15% | Val. Loss: 0.474 | Val. Acc: 77.40% |\n",
      "| Epoch: 26 | Train Loss: 0.411 | Train Acc: 80.70% | Val. Loss: 0.475 | Val. Acc: 78.07% |\n",
      "| Epoch: 27 | Train Loss: 0.406 | Train Acc: 81.55% | Val. Loss: 0.476 | Val. Acc: 77.93% |\n",
      "| Epoch: 28 | Train Loss: 0.402 | Train Acc: 81.66% | Val. Loss: 0.477 | Val. Acc: 78.00% |\n",
      "| Epoch: 29 | Train Loss: 0.397 | Train Acc: 81.93% | Val. Loss: 0.478 | Val. Acc: 78.13% |\n",
      "| Epoch: 30 | Train Loss: 0.393 | Train Acc: 81.96% | Val. Loss: 0.480 | Val. Acc: 78.60% |\n",
      "| Epoch: 31 | Train Loss: 0.387 | Train Acc: 82.42% | Val. Loss: 0.480 | Val. Acc: 78.87% |\n",
      "| Epoch: 32 | Train Loss: 0.386 | Train Acc: 82.16% | Val. Loss: 0.480 | Val. Acc: 78.60% |\n",
      "| Epoch: 33 | Train Loss: 0.382 | Train Acc: 82.75% | Val. Loss: 0.480 | Val. Acc: 78.40% |\n",
      "| Epoch: 34 | Train Loss: 0.377 | Train Acc: 83.08% | Val. Loss: 0.480 | Val. Acc: 78.13% |\n",
      "| Epoch: 35 | Train Loss: 0.371 | Train Acc: 83.40% | Val. Loss: 0.480 | Val. Acc: 78.73% |\n",
      "| Epoch: 36 | Train Loss: 0.367 | Train Acc: 83.48% | Val. Loss: 0.480 | Val. Acc: 78.60% |\n",
      "| Epoch: 37 | Train Loss: 0.360 | Train Acc: 83.86% | Val. Loss: 0.481 | Val. Acc: 78.87% |\n",
      "| Epoch: 38 | Train Loss: 0.358 | Train Acc: 83.42% | Val. Loss: 0.482 | Val. Acc: 78.73% |\n",
      "| Epoch: 39 | Train Loss: 0.353 | Train Acc: 84.15% | Val. Loss: 0.483 | Val. Acc: 78.33% |\n",
      "| Epoch: 40 | Train Loss: 0.349 | Train Acc: 84.54% | Val. Loss: 0.484 | Val. Acc: 78.00% |\n",
      "| Epoch: 41 | Train Loss: 0.345 | Train Acc: 84.95% | Val. Loss: 0.486 | Val. Acc: 78.40% |\n",
      "| Epoch: 42 | Train Loss: 0.341 | Train Acc: 85.00% | Val. Loss: 0.489 | Val. Acc: 78.67% |\n",
      "| Epoch: 43 | Train Loss: 0.337 | Train Acc: 85.20% | Val. Loss: 0.491 | Val. Acc: 78.67% |\n",
      "| Epoch: 44 | Train Loss: 0.333 | Train Acc: 85.30% | Val. Loss: 0.493 | Val. Acc: 78.47% |\n",
      "| Epoch: 45 | Train Loss: 0.330 | Train Acc: 85.50% | Val. Loss: 0.494 | Val. Acc: 78.67% |\n",
      "| Epoch: 46 | Train Loss: 0.327 | Train Acc: 85.72% | Val. Loss: 0.495 | Val. Acc: 78.67% |\n",
      "| Epoch: 47 | Train Loss: 0.323 | Train Acc: 86.02% | Val. Loss: 0.497 | Val. Acc: 78.27% |\n",
      "| Epoch: 48 | Train Loss: 0.320 | Train Acc: 85.89% | Val. Loss: 0.498 | Val. Acc: 78.27% |\n",
      "| Epoch: 49 | Train Loss: 0.319 | Train Acc: 86.10% | Val. Loss: 0.500 | Val. Acc: 78.40% |\n",
      "| Epoch: 50 | Train Loss: 0.313 | Train Acc: 86.52% | Val. Loss: 0.502 | Val. Acc: 78.33% |\n",
      "| Test Loss: 0.507 | Test Acc: 76.47%\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor(0.7833)"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f( 0.0093,  0.0003, 0.1910, epochs=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
